msgid ""
msgstr ""
"Project-Id-Version: qiskit-docs\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-11-10 16:30+0000\n"
"PO-Revision-Date: 2022-11-10 16:46\n"
"Last-Translator: \n"
"Language-Team: Korean\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.11.0\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"X-Crowdin-Project: qiskit-docs\n"
"X-Crowdin-Project-ID: 369271\n"
"X-Crowdin-Language: ko\n"
"X-Crowdin-File: /master/qiskit-ibm-runtime/docs/locale/en/LC_MESSAGES/tutorials/qka.po\n"
"X-Crowdin-File-ID: 9822\n"
"Language: ko_KR\n"

#: ../../tutorials/qka.ipynb:9
msgid "This page was generated from `docs/tutorials/qka.ipynb`__."
msgstr "이 페이지는 `docs/tutorials/qka.ipynb`__ 에서 생성되었다."

#: ../../tutorials/qka.ipynb:9
msgid "Quantum Kernel Alignment with Qiskit Runtime"
msgstr "Qiskit Runtime을 이용한 양자 커널 정렬"

#: ../../tutorials/qka.ipynb:11
msgid "**Classification with Support Vector Machines**\\  Classification problems are widespread in machine learning applications. Examples include credit card risk, handwriting recognition, and medical diagnosis. One approach to tackling classification problems is the support vector machine (SVM) [1,2]. This supervised learning algorithm uses labeled data samples to train a model that can predict to which class a test sample belongs. It does this by finding a separating hyperplane maximizing the margin between data classes. Often, data is not linearly separable in the original space. In these cases, the kernel trick is used to implicitly encode a transformation of the data into a higher-dimensional feature space, through the inner product between pairs of data points, where the data may become separable."
msgstr "**서포트 벡터 머신을 이용한 분류**\\  분류 문제는 신용 위험도 분류, 필기 인식, 의료 진단 등을 포함하는 기계 학습 분야에서 널리 사용된다. 서포트 벡터 머신(Support Vector Machine, SVM) [1,2]은 분류 문제를 위한 접근 방식 중 하나이다. 이 지도 학습 알고리즘은 라벨링된 데이터 샘플을 사용하여 테스트 샘플이 속해있는 클래스를 분류하기 위한 모델을 학습시킨다. 이 과정은 데이터 클래스 간의 차이를 극대화하는 초평면을 찾는 것으로 이루어진다. 많은 경우에 데이터가 속하는 원래의 공간에서는 데이터를 선형적으로 분리시킬 수 없다. 이를 해결하기 위한 커널은 각각의 데이터 포인트 쌍끼리의 내적을 취함으로써 데이터를 분리 가능한 더 높은 차원의 특성 공간으로 변환하는 트릭을 사용한다."

#: ../../tutorials/qka.ipynb:14
msgid "**Quantum Kernels**\\  Quantum computers can be used to encode classical data in a quantum-enhanced feature space. In 2019, IBM introduced an algorithm called the quantum kernel estimator (QKE) for computing quantum kernels [3]. This algorithm uses quantum circuits with data provided classically and offers an efficient way to evaluate inner products between data in a quantum feature space. For two data samples :math:`\\theta` and :math:`\\theta'`, the kernel matrix is given as"
msgstr "**양자 커널** 양자 컴퓨터는 양자적으로 효율화된 특성 공간에서 고전적 데이터를 인코딩 하는데 사용될 수 있다. 2019년, IBM은 양자 커널을 계산하기 위한 양자 커널 추정기 (Quantum Kernal Estimator, QKE) 알고리즘을 소개했다 [3]. 이 알고리즘은 양자 회로를 사용하여 양자 특성 공간에서 제시된 데이터간의 내적을 계산하기 위한 효율적인 방법을 제공한다. 두 개의 데이터 샘플 :math:`\\theta` 와 :math:`\\theta'` 에 대해서, 커널 행렬은 다음과 같이 주어진다."

#: ../../tutorials/qka.ipynb:16
msgid "K(\\theta, \\theta') = \\lvert\\langle 0^n \\rvert U^\\dagger(\\theta) U(\\theta') \\lvert 0^n \\rangle \\rvert^2,"
msgstr "K(\\theta, \\theta') = \\lvert\\langle 0^n \\rvert U^\\dagger(\\theta) U(\\theta') \\lvert 0^n \\rangle \\rvert^2,"

#: ../../tutorials/qka.ipynb:21
msgid "where :math:`U(\\theta)` prepares the quantum feature state. Quantum kernels used in a classification framework inherit the convex optimization program of the SVM and avoid common limitations of variational quantum classifiers. A key observation of this paper was that a necessary condition for a computational advantage requires quantum circuits for the kernel that are hard to simulate classically. More recently, IBM proved that quantum kernels can offer superpolynomial speedups over any classical learner on a learning problem based on the hardness of the discrete logarithm problem [4]. This means that quantum kernels can someday offer quantum advantage on suitable problems."
msgstr "이 때 :math:`U(\\theta)` 는 양자 특성 상태를 나타낸다. 분류 프레임워크에서 사용되는 양자 커널은 SVM의 컨벡스 최적화 기법을 이어받았고, 다양한 양자 분류기의 공통된 제한을 피할 수 있다. 이 논문의 핵심적인 관찰은 계산적 이점을 위한 필요 조건으로 고전적으로는 실험하기 어려운 커널에 대한 양자 회로가 요구된다는 것이었다. 보다 최근에, IBM은 어려운 이산 로그 문제 [4]를 기반으로 하는 학습 문제에 대해 양자 커널이 고전적인 학습 기법에 비해 초다항적인 속도 향상을 제공할 수 있다는 것을 입증하였다. 이는 언젠가 적절한 문제들에 있어서 양자 커널이 양자 이점을 제공할 수 있다는 것을 의미한다."

#: ../../tutorials/qka.ipynb:24
msgid "**Quantum Kernels that Exploit Structure in Data**\\  An important approach in the search for practical quantum advantage in machine learning is to identify quantum kernels for learning problems that have underlying structure in the data. We’ve taken a step in this direction in our recent paper [5], where we introduced a broad class of quantum kernels that exploit group structure in data. Examples of learning problems for data with group structure could include learning permutations or classifying translations. We call this new class of kernels *covariant quantum kernels* as they are related to covariant quantum measurements. The quantum feature map is defined by a unitary representation :math:`D(\\theta)` of a group :math:`G` for some element :math:`\\theta \\in G`, and a fiducial reference state :math:`\\lvert\\psi\\rangle = V\\lvert0^n\\rangle` prepared by a unitary circuit :math:`V`. The kernel matrix is given as"
msgstr "**데이터의 구조를 이용하는 양자 커널** 기계 학습 분야에서 실용적인 양자 이점을 찾아내기 위한 중요한 접근법은 내부적으로 구조를 갖는 데이터를 학습하는 문제에 대한 양자 커널을 찾아내는 것이다. 우리는 최근 논문에서 [5], 데이터의 군 구조를 찾아내는 양자 커널의 개괄적인 클래스를 소개함으로써 이 방향으로의 진전을 이룬 바 있다. 군 구조를 갖는 데이터 학습 문제의 예로서, 순열 학습 문제나 번역 분류 문제를 들 수 있다. 우리는 이러한 새 커널 클래스를 *공변 양자 커널 (covariant quantum kernels)* 이라고 부르는데, 이 클래스가 공변 양자 측정과 관계되어 있기 때문이다. 군 :math:`G` 의 임의의 원소  :math:`\\theta \\in G` 에 대해, 양자 특성맵은 단일 표현 :math:`D(\\theta)`으로 정의되며, 단일 회로 :math:`V` 로 기준 참조 상태 :math:`\\vert\\psi\\rangle = V\\vert0^n\\rangle` 를 준비할 수 있다. 커널 행렬은 다음과 같이 주어진다."

#: ../../tutorials/qka.ipynb:27
msgid "K(\\theta, \\theta') = \\vert\\langle 0^n \\rvert V^\\dagger D^\\dagger(\\theta) D(\\theta') V \\lvert 0^n \\rangle \\rvert^2. \\qquad (1)"
msgstr "K(\\theta, \\theta') = \\vert\\langle 0^n \\rvert V^\\dagger D^\\dagger(\\theta) D(\\theta') V \\lvert 0^n \\rangle \\rvert^2. \\qquad (1)"

#: ../../tutorials/qka.ipynb:32
msgid "In general, the choice of the fiducial state is not known *a priori* and can significantly impact the performance of the classifier. Here, we use a method called quantum kernel alignment (QKA) to find a good fiducial state for a given group."
msgstr "일반적으로, 기준 상태는 *연역적으로(priori)* 선택하는 방법이 알려져 있지 않으며, 선택에 따라 분류기의 성능에 큰 영향을 줄 수 있다. 여기서, 우리는 양자 커널 정렬 (Quantum Kernel Alignment, QKA) 이라 불리는 방법을 사용하여 주어진 군에 대한 좋은 기준 상태를 찾는다."

#: ../../tutorials/qka.ipynb:34
msgid "**Aligning Quantum Kernels on a Dataset**\\  In practice, SVMs require a choice of the kernel function. Sometimes, symmetries in the data can inform this selection, other times it is chosen in an ad hoc manner. Kernel alignment is one approach to learning a kernel on a given dataset by iteratively adapting it to have high similarity to a target kernel informed from the underlying data distribution [6]. As a result, the SVM with an aligned kernel will likely generalize better to new data than with an unaligned kernel. Using this concept, we introduced in [5] an algorithm for quantum kernel alignment, which provides a way to learn a quantum kernel from a family of kernels. Specifically, the algorithm optimizes the parameters in a quantum circuit to maximize the alignment of a kernel while converging to the maximum SVM margin. In the context of covariant quantum kernels, we extend Eq. :math:`(1)` to"
msgstr "**데이터셋에 양자 커널 맞추기** 실제 사용에서, SVM은 커널 함수를 요구한다. 때때로 데이터의 대칭으로부터 이 함수의 선택에 대한 정보를 얻을 수 있지만, 그렇지 않을 경우에는 특별 방식으로 선택하여야 한다. 커널 정렬은 주어진 데이터 셋에 대해서 내부적인 데이터 분포로 정해진 목표 커널과의 유사도를 높힐 수 있도록 순차적으로 적응하는 방식을 통해 커널을 선택하는 접근법이다 [6]. 그 결과로, 정렬된 커널을 사용하는 SVM은 정렬되지 않은 커널에 비해 새로운 데이터를 더 잘 예측할 수 있다. 이 개념을 사용하여 우리는 [5]에서, 하나의 커널 집합으로부터 양자 커널을 구하는 양자 커널 정렬 알고리즘을 소개하였다. 특히, 이 알고리즘은 양자 회로의 파라미터를 최적화함으로써 커널의 정렬을 극대화할 수 있고, 이는 곧 SVM에서 차이를 극대화하는 결과로 이어진다. 공변 양자 커널의 관점에서 우리는 식 :math:`(1)` 을 다음과 같이 확장한다."

#: ../../tutorials/qka.ipynb:37
msgid "K_\\lambda(\\theta,\\theta') = \\lvert\\langle 0^n \\rvert V^\\dagger_\\lambda D^\\dagger(\\theta) D(\\theta') V_\\lambda \\lvert 0^n \\rangle \\rvert^2, \\qquad (2)"
msgstr "K_\\lambda(\\theta,\\theta') = \\lvert\\langle 0^n \\rvert V^\\dagger_\\lambda D^\\dagger(\\theta) D(\\theta') V_\\lambda \\lvert 0^n \\rangle \\rvert^2, \\qquad (2)"

#: ../../tutorials/qka.ipynb:42
msgid "and use QKA to learn a good fiducial state parametrized by :math:`\\lambda` for a given group."
msgstr "그리고 주어진 군에 대하여 :math:`\\lambda` 매개변수로 표시되는 좋은 기준 상태를 학습하기 위해 QKA를 사용한다."

#: ../../tutorials/qka.ipynb:44
msgid "**Covariant Quantum Kernels on a Specific Learning Problem**\\  Let’s try out QKA on a learning problem. In the following, we’ll consider a binary classification problem we call *labeling cosets with error* [5]. In this problem, we will use a group and a subgroup to form two cosets, which will represent our data classes. We take the group :math:`G = SU(2)^{\\otimes n}` for :math:`n` qubits, which is the special unitary group of :math:`2\\times2` matrices and has wide applicability in nature, for example, the Standard Model of particle physics and in many condensed matter systems. We take the graph-stabilizer subgroup :math:`S_{\\mathrm{graph}} \\in G` with :math:`S_{\\mathrm{graph}} = \\langle \\{ X_i \\otimes_{k:(k,i) \\in \\mathcal{E}} Z_k \\}_{i \\in \\mathcal{V}} \\rangle` for a graph :math:`(\\mathcal{E},\\mathcal{V})` with edges :math:`\\mathcal{E}` and vertices :math:`\\mathcal{V}`. Note that the stabilizers fix a stabilizer state such that :math:`D_s \\lvert \\psi\\rangle = \\lvert \\psi\\rangle`. This observation will be useful a bit later."
msgstr "**특정 학습 문제에 대한 공변 양자 커널** 학습 문제에 QKA를 적용해보자. 앞으로의 문제에서, 우리는 *오류가 있는 잉여류 라벨링* [5] 이라고 불리는 이항 분류 문제를 다룰 것이다. 이 문제에서, 우리는 데이터 클래스를 나타내는 두 개의 잉여류를 만들기 위한 하나의 군과 하나의 부분군을 사용할 것이다. :math:`n` 큐비트을 나타내기 위해 군 :math:`G = SU(2)^{\\otimes n}` 을 사용하는데,  이 군은 :math:`2\\times2` 행렬의 특별한 유니터리 군이며, 입자 물리학의 표준모형이나 많은 응집 물질계와 같은 다양한 분야에 적용될 수 있다. 우리는  :math:`\\mathcal{E}` 를 간선으로, :math:`\\mathcal{V}` 를 정점으로 갖는 그래프 :math:`(\\mathcal{E},\\mathcal{V})` 에 대해, 그래프 스태빌라이저로 :math:`S_{\\mathrm{graph}} = \\langle \\{ X_i \\otimes_{k:(k,i) \\in \\mathcal{E}} Z_k \\}_{i \\in \\mathcal{V}} \\rangle` 인 부분군  :math:`S_{\\mathrm{graph}} \\in G` 을 사용한다. 스태빌라이저는 :math:`D_s \\vert\\psi\\rangle = \\vert\\psi\\rangle` 와 같은 안정 상태를 고정한다는 것을 기억하자. 이 사실은 조금 후에 유용하게 사용될 것이다."

#: ../../tutorials/qka.ipynb:48
msgid "To generate the dataset, we write the rotations of the group as :math:`D(\\theta_1, \\theta_2, 0)=\\exp(i \\theta_1 X) \\exp(i \\theta_2 Z) \\in SU(2)`, so that each qubit is parametrized by the first two Euler angles (the third we set to zero). Then, we draw randomly two sets of angles :math:`\\mathbf{\\theta}_\\pm \\in [-\\pi/4, \\pi/4]^{2n}` for the :math:`n`-qubit problem. From these two sets, we construct a binary classification problem by forming two left-cosets (representing the two classes) with those angles, :math:`C_\\pm = D(\\mathbf{\\theta}_\\pm) S_{\\mathrm{graph}}` where :math:`D(\\mathbf{\\theta}_\\pm) = \\otimes_{k=1}^n D(\\theta_\\pm^{2k-1}, \\theta_\\pm^{2k}, 0)`. Note that the elements of the cosets can again be written in terms of Euler angles. We build training and testing sets by randomly drawing elements from :math:`C_\\pm` such that the dataset has samples :math:`i=1,...,m` containing the first two Euler angles for each qubit :math:`\\mathbf{\\theta}_{y_i} = (\\theta_{y_i}^{1}, \\theta_{y_i}^{2}, \\theta_{y_i}^{3}, \\theta_{y_i}^{4}, ..., \\theta_{y_i}^{2n-1}, \\theta_{y_i}^{2n})` and labels :math:`y_i \\in \\{-1,1\\}` that indicate to which coset a sample belongs."
msgstr "데이터셋을 만들기 위해, 군의 회전을 :math:`D(\\theta_1, \\theta_2, 0)=\\exp(i \\theta_1 X) \\exp(i \\theta_2 Z) \\in SU(2)` 과 같이 표현하여 각각의 큐비트를 첫 두 개의 오일러 각(세번째는 0으로 둔다)으로 매개화할 것이다. 그리고 :math:`n`-큐비트 문제에 대하여 랜덤하게 두 개의 각 집합 :math:`\\mathbf{\\theta}_\\pm \\in [-\\pi/4, \\pi/4]^{2n}` 를 뽑는다. 이 두 집합으로부터, :math:`D(\\mathbf{\\theta}_\\pm) = \\otimes_{k=1}^n D(\\theta_\\pm^{2k-1}, \\theta_\\pm^{2k}, 0)` 인 (두 개의 클래스를 나타내는) 두 개의 좌잉여류 :math:`C_\\pm = D(\\mathbf{\\theta}_\\pm) S_{\\mathrm{graph}}` 를 만듦으로써 이항 분류 문제를 유도해낼 수 있다. 잉여류의 각 원소는 오일러 각으로 표현할 수 있음에 유의하라. 각각의 큐비트에 대해서 첫 두 개의 오일러 각을 포함하고, 각 샘플이 어떤 잉여류에 속하는지를 라벨 :math:`y_i \\in \\{-1,1\\}` 로 표시하는 :math:`i=1,...,m` 개의 샘플 데이터셋 :math:`\\mathbf{\\theta}_{y_i} = (\\theta_{y_i}^{1}, \\theta_{y_i}^{2}, \\theta_{y_i}^{3}, \\theta_{y_i}^{4}, ..., \\theta_{y_i}^{2n-1}, \\theta_{y_i}^{2n})` 을 :math:`C_\\pm` 으로부터 랜덤하게 추출함으로써 훈련용 데이터셋과 테스트용 데이터셋을 만들 수 있다."

#: ../../tutorials/qka.ipynb:52
msgid "Next, we select a fiducial state. A natural candidate is the stabilizer state we encountered above. Why? Because this is a subgroup invariant state, :math:`D_s\\lvert\\psi\\rangle = \\lvert\\psi\\rangle`, which causes the data for a given coset to be mapped to a unique state: :math:`D(\\mathbf{\\theta}_\\pm)D_s \\lvert\\psi\\rangle = D(\\mathbf{\\theta}_\\pm) \\lvert\\psi\\rangle`. This means the classifier only needs to distinguish the *two* states :math:`D(\\mathbf{\\theta}_\\pm) \\lvert\\psi\\rangle \\langle \\psi\\rvert D^\\dagger(\\mathbf{\\theta}_\\pm)` for every element of the coset. In this tutorial, we will add a small Gaussian error with variance :math:`0.01` to the Euler angles of the dataset. This noise will perturb these two states, but if the variance is sufficiently small, we expect the states will still be classified correctly. Let’s consider a parametrized version of the stabilizer state, associated with the coupling graph :math:`(\\mathcal{E},\\mathcal{V})` given by the device connectivity, as our fiducial state and then use kernel alignment to find its optimal parameters. Specifically, we’ll replace the initial layers of Hadamards in the graph state with :math:`y`-rotations by an angle :math:`\\lambda`,"
msgstr "다음으로 기준상태를 선택한다. 일반적인 후보는 앞에서 본 스태빌라이저 상태이다. 왜냐하면 이것이 주어진 잉여류의 데이터를 단일 상태 :math:`D(\\mathbf{\\theta}_\\pm)D_s \\vert\\psi\\rangle = D(\\mathbf{\\theta}_\\pm) \\vert\\psi\\rangle` 로 대응시킬 수 있는 불변 상태 :math:`D_s\\vert\\psi\\rangle = \\vert\\psi\\rangle` 의 부분군이기 때문이다. 이는 분류기가 잉여류의 모든 원소에 대하여 *두 개* 의 상태 :math:`D(\\mathbf{\\theta}_\\pm) \\vert\\psi\\rangle \\langle \\psi\\vert D^\\dagger(\\mathbf{\\theta}_\\pm)` 만을 구분하면 된다는 것을 뜻한다. 이 튜토리얼에서, 우리는 데이터셋의 오일러 각에 변량이 :math:`0.01` 인 작은 가우시안 오차를 더할 것이다. 이 노이즈는 두 개의 상태를 교란시키겠지만, 변량이 충분히 작다면 여전히 각각의 상태를 올바로 분류할 수 있을 것으로 기대할 수 있다. 디바이스의 연결로부터 주어진 결합 그래프 :math:`(\\mathcal{E},\\mathcal{V})` 에 연관된 스태빌라이저 상태의 매개화된 버전을 기준 상태로 두고, 최적화된 매개변수를 찾기 위해 커널 정렬을 수행하자. 특히, 그래프의 첫 하다마르 레이어를 각 :math:`\\lambda` 만큼 :math:`y` 회전하는 것으로 변경한다."

#: ../../tutorials/qka.ipynb:56
msgid "\\lvert \\psi_\\lambda\\rangle = V_\\lambda \\lvert 0^n\\rangle = \\prod_{(k,t) \\in \\mathcal{E}} CZ_{k,t} \\prod_{k \\in \\mathcal{V}} \\exp\\left(i \\frac{\\lambda}{2} Y_k\\right)\\lvert 0^n\\rangle,"
msgstr "\\lvert \\psi_\\lambda\\rangle = V_\\lambda \\lvert 0^n\\rangle = \\prod_{(k,t) \\in \\mathcal{E}} CZ_{k,t} \\prod_{k \\in \\mathcal{V}} \\exp\\left(i \\frac{\\lambda}{2} Y_k\\right)\\lvert 0^n\\rangle,"

#: ../../tutorials/qka.ipynb:61
msgid "where :math:`CZ=\\mathrm{diag}(1,1,1,-1)`. Then, given two samples from our dataset, :math:`\\mathbf{\\theta}` and :math:`\\mathbf{\\theta}'`, the kernel matrix is evaluated as in Eq. :math:`(2)`. If we initialize the kernel with :math:`\\lambda \\approx 0`, we expect the quantum kernel alignment algorithm to converge towards the optimal :math:`\\lambda = \\pi/2` and the classifier to yield 100% test accuracy."
msgstr "이 때, :math:`CZ=\\mathrm{diag}(1,1,1,-1)` 이다. 그러면 데이터셋의 두 개의 샘플 :math:`\\mathbf{\\theta}` 과 :math:`\\mathbf{\\theta}'` 이 주어졌을 때, 커널 행렬은 식 :math:`(2)` 와 같이 계산된다. 커널을 :math:`\\lambda \\approx 0` 로 초기화한다면, 우리는 양자 커널 정렬 알고리즘이 최적해인 :math:`\\lambda = \\pi/2` 으로 수렴하고 분류기가 100%의 테스트 정확성을 얻을 것으로 예상한다."

#: ../../tutorials/qka.ipynb:63
msgid "Let’s define two specific problem instances to test these ideas out. We’ll be using the quantum device ``ibmq_montreal``, with coupling map shown below:"
msgstr "이 아이디어를 테스트할 수 있는 두 개의 특별한 문제를 정의해보자. 우리는 결합 그래프과 아래와 같이 주어진 양자 장치 ``ibmq_montreal`` 를 사용할 것이다."

#: ../../tutorials/qka.ipynb:65
msgid "|090cf64fc85b4dc99a24a68babdd4126|"
msgstr ""

#: ../../tutorials/qka.ipynb:78
msgid "090cf64fc85b4dc99a24a68babdd4126"
msgstr ""

#: ../../tutorials/qka.ipynb:67
msgid "We’ll pick two different subgraphs, one for 7 qubits and one for 10, to define our problem instances. Using these subgraphs, we’ll generate the corresponding datasets as described above, and then align the quantum kernel with QKA to learn a good fiducial state."
msgstr "우리는 문제를 정의하기 위해 하나는 7 큐비트, 다른 하나는 10 큐비트인 두 개의 서브그래프를 고를 것이다. 이 서브그래프를 사용하여, 위에서 논의한대로 대응하는 데이터셋을 생성하고 좋은 기준 상태를 얻기 위해 QKA로 양자 커널을 정렬한다."

#: ../../tutorials/qka.ipynb:69
msgid "|67b5892f46a94f8280f74f67331cbe37|"
msgstr ""

#: ../../tutorials/qka.ipynb:81
msgid "67b5892f46a94f8280f74f67331cbe37"
msgstr ""

#: ../../tutorials/qka.ipynb:71
msgid "**Speeding up Algorithms with Qiskit Runtime**\\  QKA is an iterative quantum-classical algorithm, in which quantum hardware is used to execute parametrized quantum circuits for evaluating the quantum kernel matrices with QKE, while a classical optimizer tunes the parameters of those circuits to maximize the alignment. Iterative algorithms of this type can be slow due to latency between the quantum and classical calculations. Qiskit Runtime is a new architecture that can speed up iterative algorithms like QKA by co-locating classical computations with the quantum hardware executions. In this tutorial, we’ll use QKA with Qiskit Runtime to learn a good quantum kernel for the *labeling cosets with error* problem defined above."
msgstr "**Qiskit Runtime으로 알고리즘 속도 향상하기** QKA는 순차적인 양자-고전 알고리즘으로써, QKE로 양자 커널 행렬을 계산하는 매개화된 양자 회로를 사용하고, 정렬을 극대화하기 위해 이러한 회로의 매개변수를 조율하는 고전적인 최적화기를 사용한다. 이러한 타입의 순차 알고리즘은 양자적 계산과 고전적 계산 사이의 대기 시간으로 인해 느릴 수 있다. Qiskit Runime은 고전적인 계산을 양자 하드웨어 동작으로 재배치함으로써 QKA와 같은 순차 알고리즘의 속도를 향상시킬 수 있다. 이 튜토리얼에서, 우리는 위에서 정의된 *오류가 있는 잉여류 라벨링* 문제에 대한 양자 커널을 학습시키기 위해 Qiskit Runtime을 활용한 QKA 알고리즘을 사용할 것이다."

#: ../../tutorials/qka.ipynb:74
msgid "**References**\\  [1] B. E. Boser, I. M. Guyon, and V. N. Vapnik, Proceedings of the Fifth Annual Workshop on Computational Learning Theory, COLT ’92 (Association for Computing Machinery, New York, NY, USA, 1992) pp. 144-152 `link <https://doi.org/10.1145/130385.130401>`__ [2] V. Vapnik, The Nature of Statistical Learning Theory, Information Science and Statistics (Springer New York, 2013) `link <https://books.google.com/books?id=EqgACAAAQBAJ>`__ [3] V. Havlíček, A. D. Córcoles, K. Temme, A. W. Harrow, A. Kandala, J. M. Chow, and J. M. Gambetta, Nature 567, 209-212 (2019) `link <https://doi.org/10.1038/s41586-019-0980-2>`__ [4] Y. Liu, S. Arunachalam, and K. Temme, arXiv:2010.02174 (2020) `link <https://arxiv.org/abs/2010.02174>`__ [5] J. R. Glick, T. P. Gujarati, A. D. Córcoles, Y. Kim, A. Kandala, J. M. Gambetta, K. Temme, arXiv:2105.03406 (2021) `link <https://arxiv.org/abs/2105.03406>`__\\  [6] N. Cristianini, J. Shawe-taylor, A. Elisseeff, and J. Kandola, Advances in Neural Information Processing Systems 14 (2001) `link <https://proceedings.neurips.cc/paper/2001/file/1f71e393b3809197ed66df836fe833e5-Paper.pdf>`__"
msgstr "**참고문헌**  [1] B. E. Boser, I. M. Guyon, and V. N. Vapnik, Proceedings of the Fifth Annual Workshop on Computational Learning Theory, COLT ’92 (Association for Computing Machinery, New York, NY, USA, 1992) pp. 144-152 `link <https://doi.org/10.1145/130385.130401>`__ [2] V. Vapnik, The Nature of Statistical Learning Theory, Information Science and Statistics (Springer New York, 2013) `link <https://books.google.com/books?id=EqgACAAAQBAJ>`__ [3] V. Havlíček, A. D. Córcoles, K. Temme, A. W. Harrow, A. Kandala, J. M. Chow, and J. M. Gambetta, Nature 567, 209-212 (2019) `link <https://doi.org/10.1038/s41586-019-0980-2>`__ [4] Y. Liu, S. Arunachalam, and K. Temme, arXiv:2010.02174 (2020) `link <https://arxiv.org/abs/2010.02174>`__ [5] J. R. Glick, T. P. Gujarati, A. D. Córcoles, Y. Kim, A. Kandala, J. M. Gambetta, K. Temme, arXiv:2105.03406 (2021) `link <https://arxiv.org/abs/2105.03406>`__\\  [6] N. Cristianini, J. Shawe-taylor, A. Elisseeff, and J. Kandola, Advances in Neural Information Processing Systems 14 (2001) `link <https://proceedings.neurips.cc/paper/2001/file/1f71e393b3809197ed66df836fe833e5-Paper.pdf>`__"

#: ../../tutorials/qka.ipynb:93
msgid "Load your account and get the quantum backend"
msgstr "계정 로드 후 양자 백엔드 얻기"

#: ../../tutorials/qka.ipynb:95
msgid "We’ll be using the 27-qubit device ``ibmq_montreal`` for this tutorial in Qiskit Runtime on IBM Quantum."
msgstr "이 튜토리얼에서는 IBM Quantum의 Qiskit Runtime에서 27 큐비트 장치 ``ibmq_montreal`` 을 사용한다."

#: ../../tutorials/qka.ipynb:124
msgid "Invoke the Quantum Kernel Alignment program"
msgstr "양자 커널 정렬 프로그램 실행"

#: ../../tutorials/qka.ipynb:126
msgid "Before executing the runtime program for QKA, we need to prepare the dataset and configure the input parameters for the algorithm."
msgstr "QKA를 위한 런타임 프로그램을 실행하기 전에, 데이터셋을 준비하고 알고리즘의 입력 파라미터를 설정하여야 한다."

#: ../../tutorials/qka.ipynb:129
msgid "1. Prepare the dataset"
msgstr "1. 데이터셋 준비"

#: ../../tutorials/qka.ipynb:131
msgid "First, we load the dataset from the ``csv`` file and then extract the labeled training and test samples. Here, we’ll look at the 7-qubit problem, shown above in subfigure a). A second dataset is also available for the 10-qubit problem in b)."
msgstr "먼저, ``csv`` 파일로부터 데이터셋을 로드하고 라벨링된 훈련용 샘플과 테스트 샘플을 추출한다. 여기서, 우리는 서브그래프 a) 에서 보여진 7 큐비트 문제를 살펴볼 것이다. b) 의 10 큐비트 문제에 대해서는 두 번째 데이터셋을 사용할 수 있다."

#: ../../tutorials/qka.ipynb:157
msgid "Let’s take a look at the data to see how it’s formatted. Each row of the dataset contains a list of Euler angles, followed by the class label :math:`\\pm1` in the last column. For an :math:`n`-qubit problem, there are :math:`2n` features corresponding to the first two Euler angles for each qubit (recall discussion above). The rows alternate between class labels."
msgstr "데이터가 어떻게 형식화되어 있는 지를 살펴보자.데이터셋의 각 행에는 오일러 각의 리스트가 있고, 마지막 열에는 클래스 라벨 :math:`\\pm1` 이 있다. :math:`n` 큐비트 문제에 대해 각각의 큐비트에 대응하는 첫 두 개의 오일러 각 :math:`2n` 개의 항목이 있다 (앞에서의 내용을 상기하라). 행에서 클래스 라벨은 번갈아가며 나타난다."

#: ../../tutorials/qka.ipynb:232
msgid "Now, let’s explicitly construct the training and test samples (denoted ``x``) and their labels (denoted ``y``)."
msgstr "이제, (``x`` 로 표시되는) 훈련용 샘플과 테스트용 샘플, 그리고 (``y`` 로 표시되는) 라벨을 명시적으로 구성해보자."

#: ../../tutorials/qka.ipynb:270
msgid "2. Configure the QKA algorithm"
msgstr "2. QKA 알고리즘 구성"

#: ../../tutorials/qka.ipynb:272
msgid "The first task is to set up the feature map and its entangler map, which specifies the arrangement of :math:`CZ` gates in the fiducial state. We will choose this to match the connectivity of the problem subgraph, pictured above. We also initialize the fiducial state parameter :math:`\\lambda` with ``initial_point``."
msgstr "첫 번째 과제는 기준 상태에서 :math:`CZ` 게이트의 배열을 지정하는 기능 맵과 얽힘 맵을 설정하는 것이다. 위에서 그려진 문제의 서브그래프의 연결과 일치할 수 있도록 설정한다. 또한 기준 상태 매개변수 :math:`\\lambda` 를 ``initial_point`` 로 초기화한다."

#: ../../tutorials/qka.ipynb:308
msgid "Let’s print out the circuit for the feature map (the circuit for the kernel will be a feature map for one data sample composed with an inverse feature map for a second sample). The first part of the feature map is the fiducial state, which is prepared with a layer of :math:`y` rotations followed by :math:`CZ`\\ s. Then, the last two layers of :math:`z` and :math:`x` rotations in the circuit denote the group representation :math:`D(\\theta)` for a data sample :math:`\\theta`. Note that a single-qubit rotation is defined as :math:`RP(\\phi) = \\exp(- i [\\phi/2] P)` for :math:`P \\in {X, Y, Z}`."
msgstr "기능 맵의 회로를 출력해보자 (커널의 회로는 하나의 데이터 샘플에 대해서는 기능 맵, 두 번째 샘플에 대해서는 역기능 맵이 될 것이다). 기능 맵의 첫 부분은 기준 상태이며, :math:`y` 회전과 :math:`CZ`\\ s로 이루어진 레이어로 구성되어 있다. 그리고나서, 회로의 마지막 두 개의 레이어 :math:`y`, :math:`x` 회전은 데이터 샘플 :math:`\\theta` 에 대한 군 표현 :math:`D(\\theta)` 을 나타낸다. 단일 큐비트 회전은 :math:`P \\in {X, Y, Z}` 에 대해 :math:`RP(\\phi) = \\exp(- i [\\phi/2] P)` 으로 정의된다는 것에 유의하라."

#: ../../tutorials/qka.ipynb:430
msgid "Next, we set the values for the SVM soft-margin penalty ``C`` and the number of SPSA iterations ``maxiters`` we use to align the quantum kernel."
msgstr "다음으로 SVM의 소프트 마진 패널티 ``C`` 와 양자 커널을 정렬하기 위한 SPSA 반복 횟수 ``maxiters`` 를 설정한다."

#: ../../tutorials/qka.ipynb:452
msgid "Finally, we decide how to map the virtual qubits of our problem graph to the physical qubits of the hardware. For example, in the 7-qubit problem, we can directly map the virtual qubits ``[0, 1, 2, 3, 4, 5, 6]`` to the physical qubits ``[10, 11, 12, 13, 14, 15, 16]`` of the device. This allows us to avoid introducing SWAP gates for qubits that are not connected, which can increase the circuit depth."
msgstr "마지막으로, 문제 그래프의 가상 큐비트를 하드웨어의 물리적 큐비트에 맵핑하는 방법을 결정한다. 예를 들어, 7 큐비트 문제에서는 가상 큐비트 ``[0, 1, 2, 3, 4, 5, 6]`` 을 장치의 물리적 큐비트 ``[10, 11, 12, 13, 14, 15, 16]`` 으로 직접 맵핑할 수 있다. 이 방법은 회로의 깊이를 증가시킬 수 있는 미접속 큐비트의 SWAP 게이트 사용을 피할 수 있도록 해준다."

#: ../../tutorials/qka.ipynb:475
msgid "3. Set up and run the program"
msgstr "3. 설정 후 프로그램 실행"

#: ../../tutorials/qka.ipynb:477
msgid "We’re almost ready to run the program. First, let’s take a look at the program metadata, which includes a description of the input parameters and their default values."
msgstr "프로그램을 실행할 준비가 거의 다 되었다. 먼저, 입력 매개변수와 기본값에 대한 설명이 포함된 프로그램의 메타데이터를 살펴보자."

#: ../../tutorials/qka.ipynb:672
msgid "We see that this program has several input parameters, which we’ll configure below. To run the program, we’ll set up its two main components: ``inputs`` (the input parameters from the program metadata) and ``options`` (the quantum backend). We’ll also define a callback function so that the intermediate results of the algorithm will be printed as the program runs. Note that each step of the algorithm for the settings we’ve selected here takes approximately 11 minutes."
msgstr "이 프로그램에는 우리가 아래에서 설정할 여러개의 입력 매개변수가 있음을 확인할 수 있다. 프로그램을 실행하기 위해서는 두 개의 주요 요소인 ``inputs`` (프로그램 메타데이터의 입력 매개변수) 와 ``options`` (양자 백엔드) 을 설정하여야 한다. 또한 콜백 함수를 지정하여 프로그램이 실행될 때 알고리즘의 중간 결과가 출력될 수 있도록 할 것이다. 여기서 선택된 설정을 사용하는 알고리즘의 각 단계는 약 11분이 소요된다."

#: ../../tutorials/qka.ipynb:809
msgid "4. Retrieve the results of the program"
msgstr "4. 프로그램의 결과 살펴보기"

#: ../../tutorials/qka.ipynb:811
msgid "Now that we’ve run the program, we can retrieve the output, which is the aligned kernel parameter and the aligned kernel matrix. Let’s also plot this kernel matrix (we’ll subtract off the diagonal to show the contrast between the remaining entries). The kernel matrix is expected to have a block-diagonal structure. This reflects the fact that the kernel maps the input data effectively to just two states (modulo the small noise we added to the data; recall the discussion above). That is, data in the same coset (same class label) have a larger overlap than do data from different cosets."
msgstr "이제 프로그램을 실행하였으니, 정렬된 커널 매개변수와 정렬된 커널 매트릭스로 구성되는 결과를 살펴볼 수 있다. 커널 행렬을 그려보자 (남은 항목간의 대비를 표시하기 위해 대각 성분을 뺄 것이다). 커널 행렬은 블록 대각 형태를 가질 것으로 예상할 수 있다. 이는 커널이 입력 데이터를 효과적으로 두 개의 상태로 맵핑했음을 보여준다 (우리가 더한 작은 노이즈를 기반으로; 위의 논의를 다시 생각해보라). 즉, 같은 잉여류(coset)(같은 클래스 라벨) 에 있는 데이터는 다른 잉여류에 속한 데이터보다 더 큰 오버랩을 갖는다."

