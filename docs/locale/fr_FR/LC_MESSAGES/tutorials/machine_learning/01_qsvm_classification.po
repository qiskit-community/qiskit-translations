msgid ""
msgstr ""
"Project-Id-Version: qiskit-docs\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-04-02 00:19+0000\n"
"PO-Revision-Date: 2021-04-06 12:39\n"
"Last-Translator: \n"
"Language-Team: French\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"
"Plural-Forms: nplurals=2; plural=(n > 1);\n"
"X-Crowdin-Project: qiskit-docs\n"
"X-Crowdin-Project-ID: 369271\n"
"X-Crowdin-Language: fr\n"
"X-Crowdin-File: /master/docs/locale/en/LC_MESSAGES/tutorials/machine_learning/01_qsvm_classification.po\n"
"X-Crowdin-File-ID: 9464\n"
"Language: fr_FR\n"

#: ../../tutorials/machine_learning/01_qsvm_classification.ipynb:10
msgid "This page was generated from `tutorials/machine_learning/01_qsvm_classification.ipynb`__."
msgstr "Cette page a été générée depuis `tutorials/machine_learning/01_qsvm_classification.ipynb`__."

#: ../../tutorials/machine_learning/01_qsvm_classification.ipynb:12
msgid "Run interactively in the `IBM Quantum lab <https://quantum-computing.ibm.com/jupyter/tutorial/machine_learning/01_qsvm_classification.ipynb>`_."
msgstr "Exécuter en mode interactif dans le `IBM Quantum lab <https://quantum-computing.ibm.com/jupyter/tutorial/machine_learning/01_qsvm_classification.ipynb>`_."

#: ../../tutorials/machine_learning/01_qsvm_classification.ipynb:9
msgid "Quantum-enhanced Support Vector Machine (QSVM)"
msgstr "QSVM (Quantum-enhanced Support Vector Machine)"

#: ../../tutorials/machine_learning/01_qsvm_classification.ipynb:11
msgid "Classification algorithms and methods for machine learning are essential for pattern recognition and data mining applications. Well known techniques such as support vector machines and neural networks have blossomed over the last two decades as a result of the spectacular advances in classical hardware computational capabilities and speed. This progress in computer power made it possible to apply techniques, that were theoretically developed towards the middle of the 20th century, on classification problems that were becoming increasingly challenging."
msgstr "Les algorithmes de classification et les méthodes d'apprentissage automatique sont essentiels pour la classification et l'exploration de données. Des techniques bien connues telles que les machines vectorielles de support et les réseaux neuronaux se sont multipliées au cours des deux dernières décennies à la suite des avancées spectaculaires des capacités et de la vitesse de calcul du matériel classique. Ce progrès de la puissance informatique a permis d'appliquer des techniques, \n"
" dont les développements théoriques datent du milieu du XXe siècle, sur des problèmes de classification qui deviennent de plus en plus difficiles."

#: ../../tutorials/machine_learning/01_qsvm_classification.ipynb:14
msgid "A key concept in classification methods is that of a kernel. Data cannot typically be separated by a hyperplane in its original space. A common technique used to find such a hyperplane consists of applying a non-linear transformation function to the data. This function is called a feature map, as it transforms the raw features, or measurable properties, of the phenomenon or subject under study. Classifying in this new feature space -and, as a matter of fact, also in any other space, including the raw original one- is nothing more than seeing how close data points are to each other. This is the same as computing the inner product for each pair of data points in the set. So, in fact we do not need to compute the non-linear feature map for each datum, but only the inner product of each pair of data points in the new feature space. This collection of inner products is called the kernel and it is perfectly possible to have feature maps that are hard to compute but whose kernels are not."
msgstr "Un concept clé des méthodes de classification est celui de noyau. Les données ne peuvent généralement pas être séparées par un hyperplan dans son espace d'origine. Une technique commune utilisée pour trouver un tel hyperplan consiste à appliquer une fonction de transformation non linéaire aux données. Cette fonction est appelée une carte des caractéristiques, car elle transforme les caractéristiques brutes ou les propriétés mesurables, du phénomène ou du sujet à l'étude. Classification dans ce nouvel espace de fonctionnalités - et, en fait, aussi dans tout autre espace, y compris l'original - n'est rien de plus que de voir à quel point les points de données sont proches les uns des autres. C'est la même chose que de calculer le produit intérieur pour chaque paire de données dans l'ensemble. Donc, en fait, nous n'avons pas besoin de calculer la carte des fonctionnalités non linéaires pour chaque datum, mais seulement le produit intérieur de chaque paire de données dans le nouvel espace de fonctionnalité. Cette collection de produits intérieurs s'appelle le noyau et il est parfaitement possible d'avoir des cartes de fonctionnalités difficiles à calculer mais dont les noyaux ne le sont pas."

#: ../../tutorials/machine_learning/01_qsvm_classification.ipynb:17
msgid "In this notebook we provide an example of a classification problem that requires a feature map for which computing the kernel is not efficient classically -this means that the required computational resources are expected to scale exponentially with the size of the problem. We show how this can be solved in a quantum processor by a direct estimation of the kernel in the feature space. The method we used falls in the category of what is called supervised learning, consisting of a training phase (where the kernel is calculated and the support vectors obtained) and a test or classification phase (where new unlabeled data is classified according to the solution found in the training phase)."
msgstr "Dans ce bloc-notes nous fournissons un exemple de problème de classification qui nécessite une carte de fonctionnalités pour laquelle le calcul du noyau n'est pas efficace classiquement - ce qui signifie que les ressources de calcul requises sont censées s'adapter exponentiellement à la taille du problème. Nous montrons comment cela peut être résolu dans un processeur quantique par une estimation directe du noyau dans l'espace fonctionnel. La méthode que nous utilisons s'inscrit dans la catégorie de ce que l'on appelle l'apprentissage supervisé, consistant en une phase de formation (où le noyau est calculé et les vecteurs de support obtenus) et une phase de test ou de classification (où de nouvelles données non étiquetées sont classées en fonction de la solution trouvée dans la phase d'apprentissage)."

#: ../../tutorials/machine_learning/01_qsvm_classification.ipynb:20
msgid "References and additional details:"
msgstr "Références et informations supplémentaires:"

#: ../../tutorials/machine_learning/01_qsvm_classification.ipynb:22
msgid "[1] Vojtech Havlicek, Antonio D. C´orcoles, Kristan Temme, Aram W. Harrow, Abhinav Kandala, Jerry M. Chow, and Jay M. Gambetta1, “Supervised learning with quantum enhanced feature spaces,” `arXiv: 1804.11326 <https://arxiv.org/pdf/1804.11326.pdf>`__"
msgstr "[1] Vojtech Havlicek, Antonio D. C´orcoles, Kristan Temme, Aram W. Harrow, Abhinav Kandala, Jerry M. Chow, and Jay M. Gambetta1, “Supervised learning with quantum enhanced feature spaces,” `arXiv: 1804.11326 <https://arxiv.org/pdf/1804.11326.pdf>`__"

#: ../../tutorials/machine_learning/01_qsvm_classification.ipynb:54
msgid "The ad hoc data set"
msgstr "Le jeu de données ad hoc"

#: ../../tutorials/machine_learning/01_qsvm_classification.ipynb:56
msgid "As a first example we will use the ad hoc dataset as described in the above referenced paper. From the dataset we take samples for use as training, testing and the final prediction (datapoints)."
msgstr ""

#: ../../tutorials/machine_learning/01_qsvm_classification.ipynb:117
msgid "With the dataset ready we can setup the `QSVM <https://qiskit.org/documentation/stubs/qiskit.aqua.algorithms.QSVM.html>`__ algorithm to do a classification. Here we use the `ZZFeatureMap <https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZZFeatureMap.html>`__ data encoding circuit from the Qiskit circuit library."
msgstr ""

#: ../../tutorials/machine_learning/01_qsvm_classification.ipynb:119
msgid "Here the BasicAer ``qasm_simulator`` is used with 1024 shots."
msgstr ""

#: ../../tutorials/machine_learning/01_qsvm_classification.ipynb:121
msgid "For the testing, the result includes the details and the success ratio. For the prediction, the result includes the predicted labels."
msgstr ""

#: ../../tutorials/machine_learning/01_qsvm_classification.ipynb:185
msgid "The following shows the kernel matrix that was built from the training sample of the dataset."
msgstr ""

#: ../../tutorials/machine_learning/01_qsvm_classification.ipynb:215
msgid "Qiskit also has a classical SVM implementation that takes the same input data for classification. Let’s run this and do a comparison. Now the ad hoc data set was created to show that there can be datasets where quantum could give an advantage."
msgstr ""

#: ../../tutorials/machine_learning/01_qsvm_classification.ipynb:288
msgid "The breast cancer dataset"
msgstr ""

#: ../../tutorials/machine_learning/01_qsvm_classification.ipynb:290
msgid "Now we run our algorithm with a real-world dataset: the breast cancer dataset, we use the first two principal components as features."
msgstr ""

#: ../../tutorials/machine_learning/01_qsvm_classification.ipynb:364
msgid "The kernel matrix that was built from the training sample of the dataset."
msgstr ""

#: ../../tutorials/machine_learning/01_qsvm_classification.ipynb:394
msgid "Again we are able to compare the result to a classical approach."
msgstr ""

