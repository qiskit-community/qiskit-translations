msgid ""
msgstr ""
"Project-Id-Version: qiskit-docs\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-06-29 14:17+0000\n"
"PO-Revision-Date: 2021-07-07 12:54\n"
"Last-Translator: \n"
"Language-Team: Italian\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: qiskit-docs\n"
"X-Crowdin-Project-ID: 369271\n"
"X-Crowdin-Language: it\n"
"X-Crowdin-File: /master/machine-learning/docs/locale/en/LC_MESSAGES/tutorials.po\n"
"X-Crowdin-File-ID: 9528\n"
"Language: it_IT\n"

#: ../../tutorials/01_neural_networks.ipynb:13
#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:13
#: ../../tutorials/03_quantum_kernel.ipynb:13
#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:13
#: ../../tutorials/05_torch_connector.ipynb:13
msgid "Run interactively in jupyter notebook."
msgstr "Esegui in maniera interattiva su un jupyter notebook."

#: ../../tutorials/01_neural_networks.ipynb:9
msgid "Quantum Neural Networks"
msgstr "Quantum Neural Network (Reti Neurali Quantistiche)"

#: ../../tutorials/01_neural_networks.ipynb:11
msgid "This notebook demonstrates the different generic quantum neural network (QNN) implementations provided in Qiskit Machine Learning. The networks are meant as application-agnostic computational units that can be used for many different use cases. Depending on the application, a particular type of network might more or less suitable and might require to be set up in a particular way. The following different available neural networks will now be discussed in more detail:"
msgstr "Questo notebook mostra le differenti implementazioni di una generica quantum neural network (QNN) presenti in Qiskit Machine Learning. Le reti sono intese come unità computazionali agnostiche rispetto alla loro applicazione e possono quindi essere utilizzate in diversi casi d'uso. In base all'applicazione che si sta considerando, una specifica rete può essere più o meno adatta e può richiedere di essere impostata in un modo specifico. Discuretemo più nel dettaglio le seguenti diverse reti neurali:"

#: ../../tutorials/01_neural_networks.ipynb:13
msgid "``NeuralNetwork``: The interface for neural networks."
msgstr "``NeuralNetwork``: L'interfaccia per le reti neurali."

#: ../../tutorials/01_neural_networks.ipynb:14
msgid "``OpflowQNN``: A network based on the evaluation of quantum mechanical observables."
msgstr "``OpflowQNN``: Una rete basata sulla valutazione degli osservabili di meccanica quantistica."

#: ../../tutorials/01_neural_networks.ipynb:15
msgid "``TwoLayerQNN``: A special ``OpflowQNN`` implementation for convenience."
msgstr "``TwoLayerQNN``: Una speciale implementazione di ``OpflowQNN``, implementata perchè spesso utilizzata."

#: ../../tutorials/01_neural_networks.ipynb:16
msgid "``CircuitQNN``: A network based on the samples resulting from measuring a quantum circuit."
msgstr "``CircuitQNN``: Una rete basata sui campioni risultanti dalla misurazione di un circuito quantistico."

#: ../../tutorials/01_neural_networks.ipynb:64
msgid "1. ``NeuralNetwork``"
msgstr "1. ``NeuralNetwork``"

#: ../../tutorials/01_neural_networks.ipynb:66
msgid "The ``NeuralNetwork`` represents the interface for all neural networks available in Qiskit Machine Learning. It exposes a forward and a backward pass taking the data samples and trainable weights as input. A ``NeuralNetwork`` does not contain any training capabilities, these are pushed to the actual algorithms / applications. Thus, a ``NeuralNetwork`` also does not store the values for trainable weights. In the following, different implementations of this interfaces are introduced."
msgstr "``NeuralNetwork`` rappresenta l'interfaccia per tutte le reti neurali disponibili in Qiskit Machine Learning. Espone\n"
"un forward pass ed un backward pass, prendendo come input campioni di dati e pesi addestrabili. Una ``NeuralNetwork`` non contiene alcuna capacità di allenamento, questa viene effettuata dagli algoritmi / applicazioni effettivamente utilizzati. Quindi, una ``NeuralNetwork`` non memorizza automaticamente i valori dei pesi addestrabili. Qui di seguito sono introdotte differenti implementazioni di questa interfaccia."

#: ../../tutorials/01_neural_networks.ipynb:68
msgid "Suppose a ``NeuralNetwork`` called ``nn``. Then, the ``nn.forward(input, weights)`` pass takes either flat inputs for the data and weights of size ``nn.num_inputs`` and ``nn.num_weights``, respectively. ``NeuralNetwork`` supports batching of inputs and returns batches of output of the corresponding shape."
msgstr "Supponiamo di avere una ``NeuralNetwork`` chiamata ``nn``. Allora, il comando pass ``nn.forward(input, weights)`` accetta sia un input flat per i dati che i pesi, di dimensione rispettivamente ``nn.num_inputs`` e ``nn.num_weights``. ``NeuralNetwork`` supporta processi batch per gli input, restituendo batch di output delle dimensioni corrispondenti."

#: ../../tutorials/01_neural_networks.ipynb:80
msgid "2. ``OpflowQNN``"
msgstr "2. ``OpflowQNN``"

#: ../../tutorials/01_neural_networks.ipynb:82
msgid "The ``OpflowQNN`` takes a (parametrized) operator from Qiskit and leverages Qiskit’s gradient framework to provide the backward pass. Such an operator can for instance be an expected value of a quantum mechanical observable with respect to a parametrized quantum state. The Parameters can be used to load classical data as well as represent trainable weights. The ``OpflowQNN`` also allows lists of operators and more complex structures to construct more complex QNNs."
msgstr "``OpflowQNN`` accetta in input un operatore (parametrizzato) da Qiskit e sfrutta il framework di gradienti di Qiskit per eseguire il backward pass. Per esempio, tale operatore può essere un valore di aspettazione di un osservabile quantistico in relazione ad uno stato quantistico parametrizzato. I Parameters possono essere usati per caricare dei dati classici, come anche possono essere usati per rappresentare i pesi addestrabili. ``OpflowQNN`` accetta anche liste di operatori e strutture più complesse per costruire delle QNN più complesse."

#: ../../tutorials/01_neural_networks.ipynb:321
msgid "Combining multiple observables in a ``ListOp`` also allows to create more complex QNNs"
msgstr "La combinazione di più osservabili in una ``ListOp`` permette anche di creare QNN più complesse"

#: ../../tutorials/01_neural_networks.ipynb:412
msgid "3. ``TwoLayerQNN``"
msgstr "3. ``TwoLayerQNN``"

#: ../../tutorials/01_neural_networks.ipynb:414
msgid "The ``TwoLayerQNN`` is a special ``OpflowQNN`` on :math:`n` qubits that consists of first a feature map to insert data and second an ansatz that is trained. The default observable is :math:`Z^{\\otimes n}`, i.e., parity."
msgstr "La ``TwoLayerQNN`` è un caso particolare di ``OpflowQNN`` che agisce su :math:`n` qubit e che è composta prima da una feature map per inserire i dati, poi da un ansatz che viene addestrato. L'osservabile di default è :math:`Z^{\\otimes n}`, ovvero la parità."

#: ../../tutorials/01_neural_networks.ipynb:612
msgid "4. ``CircuitQNN``"
msgstr "4. ``CircuitQNN``"

#: ../../tutorials/01_neural_networks.ipynb:614
msgid "The ``CircuitQNN`` is based on a (parametrized) ``QuantumCircuit``. This can take input as well as weight parameters and produces samples from the measurement. The samples can either be interpreted as probabilities of measuring the integer index corresponding to a bitstring or directly as a batch of binary output. In the case of probabilities, gradients can be estimated efficiently and the ``CircuitQNN`` provides a backward pass as well. In case of samples, differentiation is not possible and the backward pass returns ``(None, None)``."
msgstr "``CircuitQNN`` si basa su un ``QuantumCircuit`` (parametrizzato). Questo accetta parametri di input, come anche parametri dei pesi, per poi produrre campioni dalla misurazione. I campioni possono essere interpretati sia come la probabilità di misurare l'indice intero corrispondente ad una bitstring, sia direttamente come un batch di output binario. Nel caso delle probabilità, i gradienti possono essere stimati in modo efficiente ed il ``CircuitQNN`` fornisce anche un backward pass. In caso di campioni, la derivazione non è possibile, ed il backward pass restituisce ``(None, None)``."

#: ../../tutorials/01_neural_networks.ipynb:617
msgid "Further, the ``CircuitQNN`` allows to specify an ``interpret`` function to post-process the samples. This is expected to take a measured integer (from a bitstring) and map it to a new index, i.e. non-negative integer. In this case, the output shape needs to be provided and the probabilities are agregated accordingly."
msgstr "Inoltre, il ``CircuitQNN`` permette di specificare una funzione ``interpret`` per effettuare il post processamento dei campioni. Ci si aspetta che questo modulo mappi un intero misurato (da una bitstring) in un nuovo indice, ad esempio un indice non negativo. In questo caso, è necessario specificare la dimensione dell'output e le probabilità sono aggregate di conseguenza."

#: ../../tutorials/01_neural_networks.ipynb:619
msgid "A ``CircuitQNN`` can be configured to return sparse as well as dense probability vectors. If no ``interpret`` function is used, the dimension of the probability vector scales exponentially with the number of qubits and a sparse recommendation is usually recommended. In case of an ``interpret`` function it depends on the expected outcome. If, for instance, an index is mapped to the parity of the corresponding bitstring, i.e., to 0 or 1, a dense output makes sense and the result will be a probability vector of length 2."
msgstr "Un ``CircuitQNN`` può essere configurato per restituire vettori di probabilità sia sparsi che densi. Se non viene utilizzata alcuna funzione ``interpret``, la dimensione del vettore di probabilità cresce in modo esponenziale con il numero di qubit, ed è spesso suggerito l'utilizzo di una configurazione sparsa. Nel caso in cui venga specificata una funzione ``interpret``, la situazione dipende dall'esito previsto. Se, per esempio, un indice viene mappato alla parità della corrispondente bitstring, e.g. a 0 o 1, ha senso un output denso ed il risultato sarà un vettore di probabilità di lunghezza 2."

#: ../../tutorials/01_neural_networks.ipynb:662
msgid "4.1 Output: sparse integer probabilities"
msgstr "4.1 Output: probabilita sparse intere"

#: ../../tutorials/01_neural_networks.ipynb:761
msgid "4.2 Output: dense parity probabilities"
msgstr "4.2 Output: probabilità di parità dense"

#: ../../tutorials/01_neural_networks.ipynb:869
msgid "4.3 Output: Samples"
msgstr "4.3 Output: Campioni"

#: ../../tutorials/01_neural_networks.ipynb:985
msgid "4.4 Output: Parity Samples"
msgstr "4.4 Output: Campioni di parità"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:9
msgid "Neural Network Classifier & Regressor"
msgstr "Neural Network Classifier & Regressor"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:11
msgid "In this tutorial we show how the ``NeuralNetworkClassifier`` and ``NeuralNetworkRegressor`` are used. Both take as an input a (Quantum) ``NeuralNetwork`` and leverage it in a specific context. In both cases we also provide a pre-configured variant for convenience, the Variational Quantum Classifier (``VQC``) and Variational Quantum Regressor (``VQR``). The tutorial is structured as follows:"
msgstr "In questo tutorial mostriamo come vengono utilizzati ``NeuralNetworkClassifier`` e ``NeuralNetworkRegressor```. Entrambi accettano come input una ``NeuralNetwork`` (quantistica), e la sfruttano in un contesto specifico. In entrambi i casi forniamo, per comodità, anche una variante pre-configurata, il Variational Quantum Classifier (``VQC``) ed il Variational Quantum Regressor (``VQR```). Il tutorial è strutturato come segue:"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:13
msgid "`Classification <#Classification>`__"
msgstr "`Classificazione <#Classification>`__"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:15
msgid "Classification with an ``OpflowQNN``"
msgstr "Classificazione con un ``OpflowQNN``"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:16
#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:249
msgid "Classification with a ``CircuitQNN``"
msgstr "Classificazione con ``CircuitQNN``"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:17
#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:398
msgid "Variational Quantum Classifier (``VQC``)"
msgstr "Variational Quantum Classifier (``VQC``)"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:19
msgid "`Regression <#Regression>`__"
msgstr "`Regressione <#Regression>`__"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:21
#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:539
msgid "Regression with an ``OpflowQNN``"
msgstr "Regressione con un ``OpflowQNN``"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:22
msgid "Variational Quantum Regressor (``VQR``)"
msgstr "Variational Quantum Regressor (``VQR``)"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:70
#: ../../tutorials/03_quantum_kernel.ipynb:53
#: ../../tutorials/05_torch_connector.ipynb:69
msgid "Classification"
msgstr "Classificazione"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:72
msgid "We prepare a simple classification dataset to illustrate the following algorithms."
msgstr "Per illustrare i seguenti algoritmi, prepariamo un semplice dataset per la classificazione."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:117
msgid "Classification with the an ``OpflowQNN``"
msgstr "Classificazione con un ``OpflowQNN``"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:119
msgid "First we show how an ``OpflowQNN`` can be used for classification within a ``NeuralNetworkClassifier``. In this context, the ``OpflowQNN`` is expected to return one-dimensional output in :math:`[-1, +1]`. This only works for binary classification and we assign the two classes to :math:`\\{-1, +1\\}`. For convenience, we use the ``TwoLayerQNN``, which is a special type of ``OpflowQNN`` defined via a feature map and an ansatz."
msgstr "Per prima cosa mostriamo come un ``OpflowQNN`` può essere utilizzato per la classificazione con ``NeuralNetworkClassifier``. In questo contesto, ``OpflowQNN`` dovrebbe restituire un output unidimensionale in :math:`[-1, +1]`. Questo funziona solo per la classificazione binaria, e quindi assegniamo le due classi a :math:`\\{-1, +1\\}`. Per comodità, usiamo la ``TwoLayerQNN``, la quale è un tipo speciale di ``OpflowQNN`` definita tramite una feature map ed un ansatz."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:251
msgid "Next we show how a ``CircuitQNN`` can be used for classification within a ``NeuralNetworkClassifier``. In this context, the ``CircuitQNN`` is expected to return :math:`d`-dimensional probability vector as output, where :math:`d` denotes the number of classes. Sampling from a ``QuantumCircuit`` automatically results in a probability distribution and we just need to define a mapping from the measured bitstrings to the different classes. For binary classification we use the parity mapping."
msgstr "Successivamente mostriamo come si può utilizzare ``CircuitQNN`` per una classificazione con ``NeuralNetworkClassifier``. In questo contesto, ``CircuitQNN`` deve restituire come output un vettore di probabilità :math:`d`-dimensionale, dove :math:`d` indica il numero di classi. Campionare da un ``QuantumCircuit`` risulta automaticamente in una distribuzione di probabilità, e dobbiamo solo definire una mappatura delle bitstring misurate nelle diverse classi. Per la classificazione binaria usiamo la mappatura di parità."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:400
msgid "The ``VQC`` is a special variant of the ``NeuralNetworkClassifier`` with a ``CircuitQNN``. It applies a parity mapping (or extensions to multiple classes) to map from the bitstring to the classification, which results in a probability vector, which is interpreted as a one-hot encoded result. By default, it applies this the ``CrossEntropyLoss`` function that expects labels given in one-hot encoded format and will return predictions in that format too."
msgstr "``VQC`` è una variante particolare del ``NeuralNetworkClassifier`` con un ``CircuitQNN``. Applica una mappa di parità (o estensioni a più classi) per mappare la bitstring alla classificazione, cosa che si traduce in un vettore di probabilità interpretato come un risultato codificato con il metodo one-hot encoding. Di default, il ``VQC`` applica la funzione ``CrossEntropyLoss`` che si aspetta i label in formato one-hot encoded e restituirà anche le previsioni in quel formato."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:496
#: ../../tutorials/05_torch_connector.ipynb:524
msgid "Regression"
msgstr "Regressione"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:498
msgid "We prepare a simple regression dataset to illustrate the following algorithms."
msgstr "Per illustrare i seguenti algoritmi prepariamo un semplice dataset su cui eseguire un metodo di regressione."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:541
msgid "Here we restrict to regression with an ``OpflowQNN`` that returns values in :math:`[-1, +1]`. More complex and also multi-dimensional models could be constructed, also based on ``CircuitQNN`` but that exceeds the scope of this tutorial."
msgstr "Qui ci limitiamo alla regressione con un ``OpflowQNN`` che restituisce valori in :math:`[-1, +1]`. Si potrebbero costruire modelli più complessi e anche multidimensionali, sempre basati su ``CircuitQNN``, ma questo va oltre l'ambito di questo tutorial."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:648
msgid "Regression with the Variational Quantum Regressor (``VQR``)"
msgstr "Regressione con il Variational Quantum Regressor (``VQR``)"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:650
msgid "Similar to the ``VQC`` for classification, the ``VQR`` is a special variant of the ``NeuralNetworkRegressor`` with a ``OpflowQNN``. By default it considers the ``L2Loss`` function to minimize the mean squared error between predictions and targets."
msgstr "Come nel caso del ``VQC`` per la classificazione, il ``VQR`` è una variante speciale del ``NeuralNetworkRegressor`` con un ``OpflowQNN``. Di default, il ``VQR`` utilizza la funzione ``L2Loss`` per minimizzare l'errore quadratico medio tra predizioni e target."

#: ../../tutorials/03_quantum_kernel.ipynb:9
msgid "Quantum Kernel Machine Learning"
msgstr "Quantum Kernel Machine Learning"

#: ../../tutorials/03_quantum_kernel.ipynb:11
msgid "The general task of machine learning is to find and study patterns in data. For many datasets, the datapoints are better understood in a higher dimensional feature space, through the use of a kernel function: :math:`k(\\vec{x}_i, \\vec{x}_j) = \\langle f(\\vec{x}_i), f(\\vec{x}_j) \\rangle` where :math:`k` is the kernel function, :math:`\\vec{x}_i, \\vec{x}_j` are :math:`n` dimensional inputs, :math:`f` is a map from :math:`n`-dimension to :math:`m`-dimension space and :math:`\\langle a,b \\rangle` denotes the dot product. When considering finite data, a kernel function can be represented as a matrix: :math:`K_{ij} = k(\\vec{x}_i,\\vec{x}_j)`."
msgstr "L'obiettivo generale del machine learning è quello di trovare e studiare pattern nei dati. Per molti dataset, i dati sono meglio interpretati in uno spazio delle feature di dimensione più alta rispetto a quello di partenza, raggiunto attraverso l'uso di una funzione kernel: :math:`k(\\vec{x}_i, \\vec{x}_j) = \\langle f(\\vec{x}_i), f(\\vec{x}_j) \\rangle`, dove :math:`k` è la funzione kernel, :math:`\\vec{x}_i, \\vec{x}_j` sono input a :math:`n` dimensioni, :math:`f` è una mappa da uno spazio :math:`n`-dimensionale ad uno spazio :math:`m`-dimensionale, e :math:`\\langle a, \\rangle` indica il prodotto scalare. Quando si considerano dati finiti, una funzione kernel può essere rappresentata come una matrice: :math:`K_{ij} = k(\\vec{x}_i,\\vec{x}_j)`."

#: ../../tutorials/03_quantum_kernel.ipynb:14
msgid "In quantum kernel machine learning, a quantum feature map :math:`\\phi(\\vec{x})` is used to map a classical feature vector :math:`\\vec{x}` to a quantum Hilbert space, :math:`| \\phi(\\vec{x})\\rangle \\langle \\phi(\\vec{x})|`, such that :math:`K_{ij} = \\left| \\langle \\phi^\\dagger(\\vec{x}_j)| \\phi(\\vec{x}_i) \\rangle \\right|^{2}`. See `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__ for more details."
msgstr "Nel quantum kernel machine learning, si utilizza una feature map quantistica :math:`\\phi(\\vec{x})` per mappare un vettore classico :math:`\\vec{x}` in uno spazio di Hilbert quantistico, :math:`| \\phi(\\vec{x})\\rangle \\langle \\phi(\\vec{x})|`, tale che :math:`K_{ij} = \\left| \\langle \\phi^\\dagger(\\vec{x}_j)| \\phi(\\vec{x}_i) \\rangle \\right|^{2}`. Per maggiori dettagli, si può fare riferimento a `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__ ."

#: ../../tutorials/03_quantum_kernel.ipynb:16
msgid "In this notebook, we use ``qiskit`` to calculate a kernel matrix using a quantum feature map, then use this kernel matrix in ``scikit-learn`` classification and clustering algorithms."
msgstr "In questo notebook usiamo ``qiskit`` per calcolare una matrice di kernel usando una feature map quantistica, poi utilizziamo questa matrice di kernel negli algoritmi di classificazione e di clustering in ``scikit-learn``."

#: ../../tutorials/03_quantum_kernel.ipynb:55
msgid "For our classification example, we will use the *ad hoc dataset* as described in `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__, and the ``scikit-learn`` `support vector machine <https://scikit-learn.org/stable/modules/svm.html>`__ classification (``svc``) algorithm."
msgstr "Seguendo `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__, nel nostro esempio di classificazione, utilizzeremo un *dataset ad hoc* e, come algoritmo di classificazione, useremo la `support vector machine <https://scikit-learn.org/stable/modules/svm.html>`__ (``svc``) di ``scikit-learn``."

#: ../../tutorials/03_quantum_kernel.ipynb:111
msgid "With our training and testing datasets ready, we set up the ``QuantumKernel`` class to calculate a kernel matrix using the `ZZFeatureMap <https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZZFeatureMap.html>`__, and the ``BasicAer`` ``qasm_simulator`` using 1024 shots."
msgstr "Con i nostri dataset di train e test pronti, creiamo la classe``QuantumKernel`` per calcolare la matrice di kernel usando la `ZZFeatureMap <https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZZFeatureMap.html>`__, ed il ``qasm_simulator`` di ``BasicAer``, utilizzando 1024 shots."

#: ../../tutorials/03_quantum_kernel.ipynb:138
msgid "The ``scikit-learn`` ``svc`` algorithm allows us to define a `custom kernel <https://scikit-learn.org/stable/modules/svm.html#custom-kernels>`__ in two ways: by providing the kernel as a callable function or by precomputing the kernel matrix. We can do either of these using the ``QuantumKernel`` class in ``qiskit``."
msgstr "L'algoritmo ``svc`` di ``scikit-learn`` ci permette di definire un `custom kernel <https://scikit-learn.org/stable/modules/svm.html#custom-kernels>`__ in due modi: fornendo il kernel come una funzione richiamabile o calcolando in anticipo la matrice di kernel. Possiamo utilizzare indistintamente uno dei due metodi anche quando usiamo la classe ``QuantumKernel`` di ``qiskit``."

#: ../../tutorials/03_quantum_kernel.ipynb:140
msgid "The following code gives the kernel as a callable function:"
msgstr "Il seguente codice fornisce il kernel come funzione richiamabile:"

#: ../../tutorials/03_quantum_kernel.ipynb:184
msgid "The following code precomputes and plots the training and testing kernel matrices before providing them to the ``scikit-learn`` ``svc`` algorithm:"
msgstr "Il seguente codice precalcola e mostra le matrici kernel realative ai dati di training e di test prima di fornirle all'algoritmo ``svc`` di ``scikit-learn``:"

#: ../../tutorials/03_quantum_kernel.ipynb:250
msgid "``qiskit`` also contains the ``qsvc`` class that extends the ``sklearn svc`` class, that can be used as follows:"
msgstr "``qiskit`` contiene anche la classe ``qsvc`` che estende la classe ``sklearn svc`` e che può essere utilizzata come segue:"

#: ../../tutorials/03_quantum_kernel.ipynb:295
msgid "Clustering"
msgstr "Clustering"

#: ../../tutorials/03_quantum_kernel.ipynb:297
msgid "For our clustering example, we will again use the *ad hoc dataset* as described in `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__, and the ``scikit-learn`` ``spectral`` clustering algorithm."
msgstr "Seguendo `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__, nel nostro esempio di clustering, utilizzeremo un *dataset ad hoc* e, l'algoritmo di clustering ``spectral`` di ``scikit-learn``."

#: ../../tutorials/03_quantum_kernel.ipynb:299
msgid "We will regenerate the dataset with a larger gap between the two classes, and as clustering is an unsupervised machine learning task, we don’t need a test sample."
msgstr "Creeremo il dataset con una separazione maggiore tra le due classi, e non avremo bisogno di un campione di test, in un quanto il clustering è un algoritmo di machine learning non supervisionato."

#: ../../tutorials/03_quantum_kernel.ipynb:350
msgid "We again set up the ``QuantumKernel`` class to calculate a kernel matrix using the `ZZFeatureMap <https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZZFeatureMap.html>`__, and the BasicAer ``qasm_simulator`` using 1024 shots."
msgstr "Ancora una volta inizializziamo la classe ``QuantumKernel`` per calcolare una matrice di kernel usando una `ZZFeatureMap <https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZZFeatureMap.html>`__, ed il ``qasm_simulator`` di BasicAer con 1024 shots."

#: ../../tutorials/03_quantum_kernel.ipynb:377
msgid "The scikit-learn spectral clustering algorithm allows us to define a [custom kernel] in two ways: by providing the kernel as a callable function or by precomputing the kernel matrix. Using the QuantumKernel class in qiskit, we can only use the latter."
msgstr "L'algoritmo di clustering spectral di scikit-learn ci permette di definire un [custom kernel] in due modi: fornendo il kernel come una funzione richiamabile, o calcolando in anticipo la matrice di kernel. Quando utilizziamo la classe QuantumKernel in Qiskit, possiamo utilizzare solo il secondo."

#: ../../tutorials/03_quantum_kernel.ipynb:379
msgid "The following code precomputes and plots the kernel matrices before providing it to the scikit-learn spectral clustering algorithm, and scoring the labels using normalized mutual information, since we apriori know the class labels."
msgstr "Il seguente codice precalcola e mostra le matrici di kernel prima di fornirle all'algoritmo di clustering spectral di scikit-learning, e prima di valutare le labels utilizzando l'informazione reciproca normalizzata, dato che conosciamo a priori i label delle classi."

#: ../../tutorials/03_quantum_kernel.ipynb:439
msgid "``scikit-learn`` has other algorithms that can use a precomputed kernel matrix, here are a few:"
msgstr "``scikit-learn`` fornisce anche altri algoritmi che possono utilizzare una matrice di kernel pre-calcolata, eccone alcuni:"

#: ../../tutorials/03_quantum_kernel.ipynb:441
msgid "`Agglomerative clustering <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html>`__"
msgstr "`Agglomerative clustering <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html>`__"

#: ../../tutorials/03_quantum_kernel.ipynb:442
msgid "`Support vector regression <https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html>`__"
msgstr "`Support vector regression <https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html>`__"

#: ../../tutorials/03_quantum_kernel.ipynb:443
msgid "`Ridge regression <https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html>`__"
msgstr "`Ridge regression <https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html>`__"

#: ../../tutorials/03_quantum_kernel.ipynb:444
msgid "`Guassian process regression <https://scikit-learn.org/stable/modules/gaussian_process.html>`__"
msgstr "`Guassian process regression <https://scikit-learn.org/stable/modules/gaussian_process.html>`__"

#: ../../tutorials/03_quantum_kernel.ipynb:445
msgid "`Principal component analysis <https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html>`__"
msgstr "`Principal component analysis <https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html>`__"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:9
msgid "qGANs for Loading Random Distributions"
msgstr "qGANs per Caricare Distribuzioni Casuali"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:11
msgid "Given :math:`k`-dimensional data samples, we employ a quantum Generative Adversarial Network (qGAN) to learn the data’s underlying random distribution and to load it directly into a quantum state:"
msgstr "Partendo da un campione di dati :math:`k`-dimensionale, utilizziamo una quantum Generative Adversarial Network (qGAN) per ricavare la distribuzione casuale dei dati e per caricarla direttamente in uno stato quantistico:"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:13
msgid "\\big| g_{\\theta}\\rangle = \\sum_{j=0}^{2^n-1} \\sqrt{p_{\\theta}^{j}}\\big| j \\rangle\n\n"
msgstr "\\big| g_{\\theta}\\rangle = \\sum_{j=0}^{2^n-1} \\sqrt{p_{\\theta}^{j}}\\big| j \\rangle\n\n"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:15
msgid "where :math:`p_{\\theta}^{j}` describe the occurrence probabilities of the basis states :math:`\\big| j\\rangle`."
msgstr "dove i :math:`p_{\\theta}^{j}` descrivono le probabilità di occorrenza degli stati di base :math:`\\big| j\\rangle`."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:17
msgid "The aim of the qGAN training is to generate a state :math:`\\big| g_{\\theta}\\rangle` where :math:`p_{\\theta}^{j}`, for :math:`j\\in \\left\\{0, \\ldots, {2^n-1} \\right\\}`, describe a probability distribution that is close to the distribution underlying the training data :math:`X=\\left\\{x^0, \\ldots, x^{k-1} \\right\\}`."
msgstr "L'obiettivo dell'allenamento di una qGAN è quello di generare uno stato :math:`\\big| g_{\\theta}\\rangle` dove i :math:`p_{\\theta}^{j}`, per :math:`j\\in \\left\\{0, \\ldots, {2^n-1} \\right\\}`, descrivono una distribuzione di probabilità che è vicina alla distribuzione dei dati di training :math:`X=\\left\\{x^0, \\ldots, x^{k-1} \\right\\}`."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:19
msgid "For further details please refer to `Quantum Generative Adversarial Networks for Learning and Loading Random Distributions <https://arxiv.org/abs/1904.00043>`__ *Zoufal, Lucchi, Woerner* [2019]."
msgstr "Per ulteriori dettagli puoi far riferimento a `Quantum Generative Adversarial Networks for Learning and Loading Random Distributions <https://arxiv.org/abs/1904.00043>`__ *Zoufal, Lucchi, Woerner* [2019]."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:21
msgid "For an example of how to use a trained qGAN in an application, the pricing of financial derivatives, please see the `Option Pricing with qGANs <https://github.com/Qiskit/qiskit-finance/tree/main/docs/tutorials/10_qgan_option_pricing.ipynb>`__ tutorial."
msgstr "Per un esempio su come utilizzare una qGAN allenata in una applicazione, in questo caso il prezzo dei derivati finanziari, far riferimento al tutorial `Option Pricing with qGANs <https://github.com/Qiskit/qiskit-finance/tree/main/docs/tutorials/10_qgan_option_pricing.ipynb>`__."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:56
msgid "Load the Training Data"
msgstr "Caricare i Dati di Training"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:58
msgid "First, we need to load the :math:`k`-dimensional training data samples (here k=1)."
msgstr "Per prima cosa, dobbiamo caricare i campioni di dati di training :math:`k`-dimensionali (qui usiamo k = 1)."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:60
msgid "Next, the data resolution is set, i.e. the min/max data values and the number of qubits used to represent each data dimension."
msgstr "Successivamente, si imposta la la risoluzione dei dati, ovvero i valori min/max dei dati ed il numero di qubit utilizzati per rappresentare ogni dimensione dei dati."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:95
msgid "Initialize the qGAN"
msgstr "Inizializzare la qGAN"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:97
msgid "The qGAN consists of a quantum generator :math:`G_{\\theta}`, i.e., an ansatz, and a classical discriminator :math:`D_{\\phi}`, a neural network."
msgstr "La qGAN è composta da un generatore quantistico :math:`G_{\\theta}`, e.g. un ansatz, e da un discriminatore classico :math:`D_{\\phi}`, che è una neural network."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:99
msgid "To implement the quantum generator, we choose a depth-\\ :math:`1` ansatz that implements :math:`R_Y` rotations and :math:`CZ` gates which takes a uniform distribution as an input state. Notably, for :math:`k>1` the generator’s parameters must be chosen carefully. For example, the circuit depth should be :math:`>1` because higher circuit depths enable the representation of more complex structures."
msgstr "Per implementare un generatore quantistico, scegliamo un ansatz di profondità :math:`1`, che implementa rotazioni :math:`R_Y` e gate :math:`CZ` e che prende una distribuzione uniforme come stato di input. Da notare che per :math:`k>1` i parametri del generatore devono essere scelti con cura. Per esempio, la profondità del circuito dovrebbe essere :math:`>1` poiché maggiore profondità nei circuiti permette una rappresentazione di strutture più complesse."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:101
msgid "The classical discriminator used here is based on a neural network implementation using NumPy. There is also a discriminator based on PyTorch which is not installed by default when installing Qiskit - see `Optional Install <https://github.com/Qiskit/qiskit-machine-learning#optional-installs>`__ for more information."
msgstr "Il discriminatore classico qui usato è basato su una implementazione di una rete neurale usando NumPy. Si può anche usare un discriminatore basato su PyTorch ma non installato di default con Qiskit - per maggiori informazioni leggi `Optional Install <https://github.com/Qiskit/qiskit-machine-learning#optional-installs>`__."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:103
msgid "Here, both networks are updated with the ADAM optimization algorithm (ADAM is qGAN optimizer default)."
msgstr "Qui entrambe le reti sono aggiornate con l'algoritmo di ottimizzazione ADAM (ADAM è l'ottimizzatore predefinito per le qGAN)."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:164
msgid "Run the qGAN Training"
msgstr "Eseguire l'addestramento della qGAN"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:166
msgid "During the training the discriminator’s and the generator’s parameters are updated alternately w.r.t the following loss functions:"
msgstr "Durante l'addestramento, i parametri del discriminatore e del generatore vengono aggiornati in modo alternato, in rispetto alle seguenti funzioni di loss:"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:168
msgid "L_G\\left(\\phi, \\theta\\right) = -\\frac{1}{m}\\sum\\limits_{l=1}^{m}\\left[\\log\\left(D_{\\phi}\\left(g^{l}\\right)\\right)\\right]\n\n"
msgstr "$L_G\\left(\\phi, \\theta\\right) = -\\frac{1}{m}\\sum\\limits_{l=1}^{m}\\left[\\log\\left(D_{\\phi}\\left(g^{l}\\right)\\right)\\right]$\n\n"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:170
msgid "and"
msgstr "e"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:172
msgid "L_D\\left(\\phi, \\theta\\right) =\n"
"  \\frac{1}{m}\\sum\\limits_{l=1}^{m}\\left[\\log D_{\\phi}\\left(x^{l}\\right) + \\log\\left(1-D_{\\phi}\\left(g^{l}\\right)\\right)\\right],"
msgstr "$L_D\\left(\\phi, \\theta\\right) =\n"
"  \\frac{1}{m}\\sum\\limits_{l=1}^{m}\\left[\\log D_{\\phi}\\left(x^{l}\\right) + \\log\\left(1-D_{\\phi}\\left(g^{l}\\right)\\right)\\right]$,"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:177
msgid "with :math:`m` denoting the batch size and :math:`g^l` describing the data samples generated by the quantum generator."
msgstr "con :math:`m` che indica la dimensione del batch e :math:`g^l` che descrivono i campioni di dati generati dal generatore quantistico."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:179
msgid "Please note that the training, for the purpose of this notebook, has been kept briefer by the selection of a known initial point (``init_params``). Without such prior knowledge be aware training may take some while."
msgstr "Da notare che, per lo scopo di questo notebook, si è ottenuto un addestramento più breve grazie alla selezione di un punto iniziale conosciuto (``init_params``). Senza tale conoscenza preliminare, il training sarebbe probabilmente stato più lungo."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:245
msgid "Training Progress & Outcome"
msgstr "Progresso e Risultato dell'Addestramento"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:247
msgid "Now, we plot the evolution of the generator’s and the discriminator’s loss functions during the training, as well as the progress in the relative entropy between the trained and the target distribution."
msgstr "Ora, creiamo un grafico dell’evoluzione delle funzioni di loss del generatore e del discriminatore durante l'addestramento, nonché i progressi nell’entropia relativa tra la distribuzione addestrata e quella target."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:249
msgid "Finally, we also compare the cumulative distribution function (CDF) of the trained distribution to the CDF of the target distribution."
msgstr "Infine, confrontiamo anche la funzione di ripartizione (CDF) della distribuzione addestrata con quella della distribuzione target."

#: ../../tutorials/05_torch_connector.ipynb:9
msgid "Torch Connector"
msgstr "Torch Connector"

#: ../../tutorials/05_torch_connector.ipynb:11
msgid "This tutorial shows how the ``TorchConnector`` allows to use any ``NeuralNetwork`` from Qiskit Machine Learning and integrate it in a PyTorch workflow. The ``TorchConnector`` takes any ``NeuralNetwork`` and makes it available as a PyTorch ``Module``."
msgstr "Questo tutorial mostra come il ``TorchConnector`` permetta di usare qualsiasi ``NeuralNetwork`` da Qiskit Machine Learning e di integrarla in un flusso di lavoro in PyTorch. Il ``TorchConnector`` rende disponibile qualsiasi ``NeuralNetwork`` come un ``Module`` di PyTorch."

#: ../../tutorials/05_torch_connector.ipynb:14
msgid "Content:"
msgstr "Contenuto:"

#: ../../tutorials/05_torch_connector.ipynb:16
msgid "`Part 1: Simple Classification & Regression <#Part-1:-Simple-Classification-&-Regression>`__ - Classification - Classification with PyTorch and the ``OpflowQNN`` - Classification with PyTorch and the ``CircuitQNN`` - Regression - Regression with PyTorch and the ``OpflowQNN``"
msgstr "`Parte 1: Semplice Classificazione & Regressione <#Part-1:-Simple-Classification-&-Regression>`__ - Classificazione - Classificazione con PyTorch e ``OpflowQNN`` - Classificazione con PyTorch e il ``CircuitQNN`` - Regressione - Regressione con PyTorch e ``OpflowQNN``"

#: ../../tutorials/05_torch_connector.ipynb:18
msgid "`Part 2: MNIST Classification <#Part-2:-MNIST-Classification>`__"
msgstr "`Parte 2: Classificazione sul dataset MNIST <#Part-2:-MNIST-Classification>`__"

#: ../../tutorials/05_torch_connector.ipynb:20
msgid "Illustrates how to embed a (Quantum) ``NeuralNetwork`` into a target PyTorch workflow to classify MNIST data."
msgstr "Si illustra come incorporare una ``NeuralNetwork`` (quantistica) in uno specifico flusso di lavoro in PyTorch per classificare i dati di MNIST."

#: ../../tutorials/05_torch_connector.ipynb:57
msgid "Part 1: Simple Classification & Regression"
msgstr "Parte 1: Semplice Classificazione & Regressione"

#: ../../tutorials/05_torch_connector.ipynb:71
msgid "First, we show how the ``TorchConnector`` can be used to use a Quantum ``NeuralNetwork`` to solve a classification tasks. Therefore, we generate a simple random data set."
msgstr "Per prima cosa mostriamo come si può usare ``TorchConnector`` per utilizzare una ``NeuralNetwork`` quantistica per risolvere delle classificazioni. Per questo generiamo un semplice dataset casuale."

#: ../../tutorials/05_torch_connector.ipynb:117
msgid "Classification with PyTorch and the ``OpflowQNN``"
msgstr "Classificazione con PyTorch e ``OpflowQNN``"

#: ../../tutorials/05_torch_connector.ipynb:119
msgid "Linking an ``OpflowQNN`` to PyTorch is relatively straight-forward. Here we illustrate this using the ``TwoLayerQNN``."
msgstr "Collegare un ``OpflowQNN`` a PyTorch è relativamente semplice. Qui lo mostriamo come fare utilizzando ``TwoLayerQNN``."

#: ../../tutorials/05_torch_connector.ipynb:330
msgid "The red circles indicate wrongly classified data points."
msgstr "I cerchi rossi indicano dei dati che sono stati classificati in modo errato."

#: ../../tutorials/05_torch_connector.ipynb:342
msgid "Classification with PyTorch and the ``CircuitQNN``"
msgstr "Classificazione con PyTorch e ``CircuitQNN``"

#: ../../tutorials/05_torch_connector.ipynb:344
msgid "Linking an ``CircuitQNN`` to PyTorch requires the correct setup, otherwise backpropagation is not possible."
msgstr "Per collegare un ``CircuitQNN`` a PyTorch è richiesto un corretto setup, altrimenti non è possibile la backpropagation."

#: ../../tutorials/05_torch_connector.ipynb:526
msgid "We use a model based on the ``TwoLayerQNN`` to also illustrate an regression task."
msgstr "Per illustrare un esempio di regressione usiamo un modello basato sulla ``TwoLayerQNN``."

#: ../../tutorials/05_torch_connector.ipynb:758
msgid "Part 2: MNIST Classification"
msgstr "Parte 2: Classificazione sul datasest MNIST"

#: ../../tutorials/05_torch_connector.ipynb:760
msgid "Also see Qiskit Textbook: https://qiskit.org/textbook/ch-machine-learning/machine-learning-qiskit-pytorch.html"
msgstr "Vedi anche il Qiskit Textbook: https://qiskit.org/textbook/ch-machine-learning/machine-learning-qiskit-pytorch.html"

#: ../../tutorials/index.rst:3
msgid "Machine Learning Tutorials"
msgstr "Tutorial di Machine Learning"

