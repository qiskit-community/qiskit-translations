msgid ""
msgstr ""
"Project-Id-Version: qiskit-docs\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-07-08 17:14-0400\n"
"PO-Revision-Date: 2021-07-12 10:21\n"
"Last-Translator: \n"
"Language-Team: Italian\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: qiskit-docs\n"
"X-Crowdin-Project-ID: 369271\n"
"X-Crowdin-Language: it\n"
"X-Crowdin-File: /master/machine-learning/docs/locale/en/LC_MESSAGES/tutorials/01_neural_networks.po\n"
"X-Crowdin-File-ID: 9628\n"
"Language: it_IT\n"

#: ../../tutorials/01_neural_networks.ipynb:13
msgid "Run interactively in jupyter notebook."
msgstr "Esegui in maniera interattiva su un jupyter notebook."

#: ../../tutorials/01_neural_networks.ipynb:9
msgid "Quantum Neural Networks"
msgstr "Quantum Neural Network (Reti Neurali Quantistiche)"

#: ../../tutorials/01_neural_networks.ipynb:11
msgid "This notebook demonstrates the different generic quantum neural network (QNN) implementations provided in Qiskit Machine Learning. The networks are meant as application-agnostic computational units that can be used for many different use cases. Depending on the application, a particular type of network might more or less suitable and might require to be set up in a particular way. The following different available neural networks will now be discussed in more detail:"
msgstr "Questo notebook mostra le differenti implementazioni di una generica quantum neural network (QNN) presenti in Qiskit Machine Learning. Le reti sono intese come unità computazionali agnostiche rispetto alla loro applicazione e possono quindi essere utilizzate in diversi casi d'uso. In base all'applicazione che si sta considerando, una specifica rete può essere più o meno adatta e può richiedere di essere impostata in un modo specifico. Discuretemo più nel dettaglio le seguenti diverse reti neurali:"

#: ../../tutorials/01_neural_networks.ipynb:13
msgid "``NeuralNetwork``: The interface for neural networks."
msgstr "``NeuralNetwork``: L'interfaccia per le reti neurali."

#: ../../tutorials/01_neural_networks.ipynb:14
msgid "``OpflowQNN``: A network based on the evaluation of quantum mechanical observables."
msgstr "``OpflowQNN``: Una rete basata sulla valutazione degli osservabili di meccanica quantistica."

#: ../../tutorials/01_neural_networks.ipynb:15
msgid "``TwoLayerQNN``: A special ``OpflowQNN`` implementation for convenience."
msgstr "``TwoLayerQNN``: Una speciale implementazione di ``OpflowQNN``, implementata perchè spesso utilizzata."

#: ../../tutorials/01_neural_networks.ipynb:16
msgid "``CircuitQNN``: A network based on the samples resulting from measuring a quantum circuit."
msgstr "``CircuitQNN``: Una rete basata sui campioni risultanti dalla misurazione di un circuito quantistico."

#: ../../tutorials/01_neural_networks.ipynb:64
msgid "1. ``NeuralNetwork``"
msgstr "1. ``NeuralNetwork``"

#: ../../tutorials/01_neural_networks.ipynb:66
msgid "The ``NeuralNetwork`` represents the interface for all neural networks available in Qiskit Machine Learning. It exposes a forward and a backward pass taking the data samples and trainable weights as input. A ``NeuralNetwork`` does not contain any training capabilities, these are pushed to the actual algorithms / applications. Thus, a ``NeuralNetwork`` also does not store the values for trainable weights. In the following, different implementations of this interfaces are introduced."
msgstr "``NeuralNetwork`` rappresenta l'interfaccia per tutte le reti neurali disponibili in Qiskit Machine Learning. Espone\n"
"un forward pass ed un backward pass, prendendo come input campioni di dati e pesi addestrabili. Una ``NeuralNetwork`` non contiene alcuna capacità di allenamento, questa viene effettuata dagli algoritmi / applicazioni effettivamente utilizzati. Quindi, una ``NeuralNetwork`` non memorizza automaticamente i valori dei pesi addestrabili. Qui di seguito sono introdotte differenti implementazioni di questa interfaccia."

#: ../../tutorials/01_neural_networks.ipynb:68
msgid "Suppose a ``NeuralNetwork`` called ``nn``. Then, the ``nn.forward(input, weights)`` pass takes either flat inputs for the data and weights of size ``nn.num_inputs`` and ``nn.num_weights``, respectively. ``NeuralNetwork`` supports batching of inputs and returns batches of output of the corresponding shape."
msgstr "Supponiamo di avere una ``NeuralNetwork`` chiamata ``nn``. Allora, il comando pass ``nn.forward(input, weights)`` accetta sia un input flat per i dati che i pesi, di dimensione rispettivamente ``nn.num_inputs`` e ``nn.num_weights``. ``NeuralNetwork`` supporta processi batch per gli input, restituendo batch di output delle dimensioni corrispondenti."

#: ../../tutorials/01_neural_networks.ipynb:80
msgid "2. ``OpflowQNN``"
msgstr "2. ``OpflowQNN``"

#: ../../tutorials/01_neural_networks.ipynb:82
msgid "The ``OpflowQNN`` takes a (parametrized) operator from Qiskit and leverages Qiskit’s gradient framework to provide the backward pass. Such an operator can for instance be an expected value of a quantum mechanical observable with respect to a parametrized quantum state. The Parameters can be used to load classical data as well as represent trainable weights. The ``OpflowQNN`` also allows lists of operators and more complex structures to construct more complex QNNs."
msgstr "``OpflowQNN`` accetta in input un operatore (parametrizzato) da Qiskit e sfrutta il framework di gradienti di Qiskit per eseguire il backward pass. Per esempio, tale operatore può essere un valore di aspettazione di un osservabile quantistico in relazione ad uno stato quantistico parametrizzato. I Parameters possono essere usati per caricare dei dati classici, come anche possono essere usati per rappresentare i pesi addestrabili. ``OpflowQNN`` accetta anche liste di operatori e strutture più complesse per costruire delle QNN più complesse."

#: ../../tutorials/01_neural_networks.ipynb:321
msgid "Combining multiple observables in a ``ListOp`` also allows to create more complex QNNs"
msgstr "La combinazione di più osservabili in una ``ListOp`` permette anche di creare QNN più complesse"

#: ../../tutorials/01_neural_networks.ipynb:412
msgid "3. ``TwoLayerQNN``"
msgstr "3. ``TwoLayerQNN``"

#: ../../tutorials/01_neural_networks.ipynb:414
msgid "The ``TwoLayerQNN`` is a special ``OpflowQNN`` on :math:`n` qubits that consists of first a feature map to insert data and second an ansatz that is trained. The default observable is :math:`Z^{\\otimes n}`, i.e., parity."
msgstr "La ``TwoLayerQNN`` è un caso particolare di ``OpflowQNN`` che agisce su :math:`n` qubit e che è composta prima da una feature map per inserire i dati, poi da un ansatz che viene addestrato. L'osservabile di default è :math:`Z^{\\otimes n}`, ovvero la parità."

#: ../../tutorials/01_neural_networks.ipynb:612
msgid "4. ``CircuitQNN``"
msgstr "4. ``CircuitQNN``"

#: ../../tutorials/01_neural_networks.ipynb:614
msgid "The ``CircuitQNN`` is based on a (parametrized) ``QuantumCircuit``. This can take input as well as weight parameters and produces samples from the measurement. The samples can either be interpreted as probabilities of measuring the integer index corresponding to a bitstring or directly as a batch of binary output. In the case of probabilities, gradients can be estimated efficiently and the ``CircuitQNN`` provides a backward pass as well. In case of samples, differentiation is not possible and the backward pass returns ``(None, None)``."
msgstr "``CircuitQNN`` si basa su un ``QuantumCircuit`` (parametrizzato). Questo accetta parametri di input, come anche parametri dei pesi, per poi produrre campioni dalla misurazione. I campioni possono essere interpretati sia come la probabilità di misurare l'indice intero corrispondente ad una bitstring, sia direttamente come un batch di output binario. Nel caso delle probabilità, i gradienti possono essere stimati in modo efficiente ed il ``CircuitQNN`` fornisce anche un backward pass. In caso di campioni, la derivazione non è possibile, ed il backward pass restituisce ``(None, None)``."

#: ../../tutorials/01_neural_networks.ipynb:617
msgid "Further, the ``CircuitQNN`` allows to specify an ``interpret`` function to post-process the samples. This is expected to take a measured integer (from a bitstring) and map it to a new index, i.e. non-negative integer. In this case, the output shape needs to be provided and the probabilities are aggregated accordingly."
msgstr "Inoltre, il ``CircuitQNN`` permette di specificare una funzione ``interpret`` per effettuare il post processamento dei campioni. Ci si aspetta che questo modulo mappi un intero misurato (da una bitstring) in un nuovo indice, ad esempio un indice non negativo. In questo caso, è necessario specificare la dimensione dell'output e le probabilità sono aggregate di conseguenza."

#: ../../tutorials/01_neural_networks.ipynb:619
msgid "A ``CircuitQNN`` can be configured to return sparse as well as dense probability vectors. If no ``interpret`` function is used, the dimension of the probability vector scales exponentially with the number of qubits and a sparse recommendation is usually recommended. In case of an ``interpret`` function it depends on the expected outcome. If, for instance, an index is mapped to the parity of the corresponding bitstring, i.e., to 0 or 1, a dense output makes sense and the result will be a probability vector of length 2."
msgstr "Un ``CircuitQNN`` può essere configurato per restituire vettori di probabilità sia sparsi che densi. Se non viene utilizzata alcuna funzione ``interpret``, la dimensione del vettore di probabilità cresce in modo esponenziale con il numero di qubit, ed è spesso suggerito l'utilizzo di una configurazione sparsa. Nel caso in cui venga specificata una funzione ``interpret``, la situazione dipende dall'esito previsto. Se, per esempio, un indice viene mappato alla parità della corrispondente bitstring, e.g. a 0 o 1, ha senso un output denso ed il risultato sarà un vettore di probabilità di lunghezza 2."

#: ../../tutorials/01_neural_networks.ipynb:662
msgid "4.1 Output: sparse integer probabilities"
msgstr "4.1 Output: probabilita sparse intere"

#: ../../tutorials/01_neural_networks.ipynb:761
msgid "4.2 Output: dense parity probabilities"
msgstr "4.2 Output: probabilità di parità dense"

#: ../../tutorials/01_neural_networks.ipynb:869
msgid "4.3 Output: Samples"
msgstr "4.3 Output: Campioni"

#: ../../tutorials/01_neural_networks.ipynb:985
msgid "4.4 Output: Parity Samples"
msgstr "4.4 Output: Campioni di parità"

