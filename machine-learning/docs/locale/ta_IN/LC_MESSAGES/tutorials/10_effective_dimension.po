msgid ""
msgstr ""
"Project-Id-Version: qiskit-docs\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-11-10 17:18+0000\n"
"PO-Revision-Date: 2023-11-10 18:25\n"
"Last-Translator: \n"
"Language: ta\n"
"Language-Team: Tamil\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.13.1\n"
"X-Crowdin-Project: qiskit-docs\n"
"X-Crowdin-Project-ID: 369271\n"
"X-Crowdin-Language: ta\n"
"X-Crowdin-File: /main/machine-learning/docs/locale/en/LC_MESSAGES/tutorials/10_effective_dimension.po\n"
"X-Crowdin-File-ID: 9730\n"

#: ../../tutorials/10_effective_dimension.ipynb:9
msgid "This page was generated from `docs/tutorials/10_effective_dimension.ipynb`__."
msgstr "இந்தப் பக்கம் `docs/tutorials/10_effective_dimension.ipynb`__. லிருந்து உருவாக்கப்பட்டது."

#: ../../tutorials/10_effective_dimension.ipynb:9
msgid "Effective Dimension of Qiskit Neural Networks"
msgstr "கிஸ்கிட் நரம்பியல் நெட்வொர்க்குகளின் பயனுள்ள பரிமாணம்"

#: ../../tutorials/10_effective_dimension.ipynb:11
msgid "In this tutorial, we will take advantage of the ``EffectiveDimension`` and ``LocalEffectiveDimension`` classes to evaluate the power of Quantum Neural Network models. These are metrics based on information geometry that connect to notions such as trainability, expressibility or ability to generalize."
msgstr "இந்த டுடோரியலில், நாம் பயன்படுத்திக்கொள்வோம் ``EffectiveDimension`` மற்றும்``LocalEffectiveDimension`` வகுப்பு குவாண்டம் நியூரல் நெட்வொர்க் மாதிரிகளின் சக்தியை மதிப்பிடுவதற்கு. இவை தகவல் வடிவவியலின் அடிப்படையிலான அளவீடுகள் ஆகும், அவை பயிற்சி, வெளிப்படுத்துதல் அல்லது பொதுமைப்படுத்தும் திறன் போன்ற கருத்துகளுடன் இணைக்கப்படுகின்றன."

#: ../../tutorials/10_effective_dimension.ipynb:13
msgid "Before diving into the code example, we will briefly explain what is the difference between these two metrics, and why are they relevant to the study of Quantum Neural Networks. More information about global effective dimension can be found in `this paper <https://arxiv.org/pdf/2011.00027.pdf>`__, while the local effective dimension was introduced in a `later work <https://arxiv.org/abs/2112.04807>`__."
msgstr "குறியீட்டு எடுத்துக்காட்டில் நுழைவதற்கு முன், இந்த இரண்டு அளவீடுகளுக்கும் இடையே உள்ள வித்தியாசம் என்ன என்பதையும், குவாண்டம் நியூரல் நெட்வொர்க்குகளின் ஆய்வுக்கு அவை ஏன் பொருத்தமானவை என்பதையும் சுருக்கமாக விளக்குவோம். உலகளாவிய பயனுள்ள பரிமாணத்தைப் பற்றிய கூடுதல் தகவல்களைக் காணலாம் `this paper <https://arxiv.org/pdf/2011.00027.pdf>`__, உள்ளூர் பயனுள்ள பரிமாணம் அறிமுகப்படுத்தப்பட்டது `later work <https://arxiv.org/abs/2112.04807>`__."

#: ../../tutorials/10_effective_dimension.ipynb:25
msgid "1. Global vs. Local Effective Dimension"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:27
msgid "Both classical and quantum machine learning models share a common goal: being good at **generalizing**, i.e. learning insights from data and applying them on unseen data."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:29
msgid "Finding a good metric to assess this ability is a non-trivial matter. In `The Power of Quantum Neural Networks <https://arxiv.org/pdf/2011.00027.pdf>`__, the authors introduce the **global** effective dimension as a useful indicator of how well a particular model will be able to perform on new data. In `Effective Dimension of Machine Learning Models <https://arxiv.org/pdf/2112.04807.pdf>`__, the **local** effective dimension is proposed as a new capacity measure that bounds the generalization error of machine learning models."
msgstr "இந்தத் திறனை மதிப்பிடுவதற்கு ஒரு நல்ல அளவீட்டைக் கண்டறிவது என்பது அற்பமான விஷயம் அல்ல. `தி பவர் ஆஃப் குவாண்டம் நியூரல் நெட்வொர்க்குகளில் <https://arxiv.org/pdf/2011.00027.pdf>`__, ஒரு குறிப்பிட்ட மாதிரியானது புதிய தரவுகளில் எவ்வளவு சிறப்பாகச் செயல்பட முடியும் என்பதற்கான பயனுள்ள குறிகாட்டியாக ஆசிரியர்கள் **உலகளாவிய** பயனுள்ள பரிமாணத்தை அறிமுகப்படுத்துகின்றனர். `இயந்திர கற்றல் மாதிரிகளின் பயனுள்ள பரிமாணத்தில் <https://arxiv.org/pdf/2112.04807.pdf>`__, இயந்திர கற்றல் மாதிரிகளின் பொதுமைப்படுத்தல் பிழையை கட்டுப்படுத்தும் புதிய திறன் அளவீடாக **உள்ளூர்** பயனுள்ள பரிமாணம் முன்மொழியப்பட்டது."

#: ../../tutorials/10_effective_dimension.ipynb:32
msgid "The key difference between global (``EffectiveDimension`` class) and **local** effective dimension (``LocalEffectiveDimension`` class) is actually not in the way they are computed, but in the nature of the parameter space that is analyzed. The global effective dimension incorporates the **full parameter space** of the model, and is calculated from a **large number of parameter (weight) sets**. On the other hand, the local effective dimension focuses on how well the **trained** model can generalize to new data, and how **expressive** it can be. Therefore, the local effective dimension is calculated from **a single** set of weight samples (training result). This difference is small in terms of practical implementation, but quite relevant at a conceptual level."
msgstr "உலகளாவிய (``எஃபெக்டிவ் டைமன்ஷன்`` வகுப்பு) மற்றும் **உள்ளூர்** பயனுள்ள பரிமாணம் (``லோக்கல் எஃபெக்டிவ் டைமன்ஷன்`` வகுப்பு) ஆகியவற்றுக்கு இடையேயான முக்கிய வேறுபாடு உண்மையில் அவை கணக்கிடப்படும் விதத்தில் இல்லை, ஆனால் பகுப்பாய்வு செய்யப்படும் அளவுரு இடத்தின் தன்மையில் உள்ளது.. உலகளாவிய பயனுள்ள பரிமாணம் மாதிரியின் **முழு அளவுரு இடத்தை** ஒருங்கிணைக்கிறது, மேலும் **பெரிய எண்ணிக்கையிலான அளவுரு (எடை) தொகுப்புகளிலிருந்து** கணக்கிடப்படுகிறது. மறுபுறம், உள்ளூர் பயனுள்ள பரிமாணம், **பயிற்சி பெற்ற** மாதிரியானது புதிய தரவுகளுக்கு எவ்வளவு நன்றாகப் பொதுமைப்படுத்த முடியும், மேலும் அது எவ்வாறு **வெளிப்படையாக** இருக்கும் என்பதில் கவனம் செலுத்துகிறது. எனவே, உள்ளூர் பயனுள்ள பரிமாணம் **ஒரே** எடை மாதிரிகளின் தொகுப்பிலிருந்து கணக்கிடப்படுகிறது (பயிற்சி முடிவு). நடைமுறைச் செயலாக்கத்தின் அடிப்படையில் இந்த வேறுபாடு சிறியது, ஆனால் கருத்தியல் மட்டத்தில் மிகவும் பொருத்தமானது."

#: ../../tutorials/10_effective_dimension.ipynb:45
msgid "2. The Effective Dimension Algorithm"
msgstr "2. தி எஃபெக்டிவ் டைமன்ஷன் அல்காரிதம்"

#: ../../tutorials/10_effective_dimension.ipynb:47
msgid "Both the global and local effective dimension algorithms use the Fisher Information matrix to provide a measure of complexity. The details on how this matrix is calculated are provided in the `reference paper <https://arxiv.org/pdf/2011.00027.pdf>`__, but in general terms, this matrix captures how sensitive a neural network's output is to changes in the network's parameter space."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:49
msgid "In particular, this algorithm follows 4 main steps:"
msgstr "குறிப்பாக, இந்த அல்காரிதம் 4 முக்கிய படிகளைப் பின்பற்றுகிறது:"

#: ../../tutorials/10_effective_dimension.ipynb:51
msgid "**Monte Carlo simulation:** the forward and backward passes (gradients) of the neural network are computed for each pair of input and weight samples."
msgstr "**மான்டே கார்லோ உருவகப்படுத்துதல்:** ஒவ்வொரு ஜோடி உள்ளீடு மற்றும் எடை மாதிரிகளுக்கும் நரம்பியல் வலையமைப்பின் முன்னோக்கி மற்றும் பின்தங்கிய பாஸ்கள் (சாய்வுகள்) கணக்கிடப்படுகின்றன."

#: ../../tutorials/10_effective_dimension.ipynb:52
msgid "**Fisher Matrix Computation:** these outputs and gradients are used to compute the Fisher Information Matrix."
msgstr "**ஃபிஷர் மேட்ரிக்ஸ் கணக்கீடு:** இந்த வெளியீடுகளும் சாய்வுகளும் ஃபிஷர் தகவல் மேட்ரிக்ஸைக் கணக்கிடப் பயன்படுத்தப்படுகின்றன."

#: ../../tutorials/10_effective_dimension.ipynb:53
msgid "**Fisher Matrix Normalization:** averaging over all input samples and dividing by the matrix trace"
msgstr "**ஃபிஷர் மேட்ரிக்ஸ் இயல்பாக்கம்:** அனைத்து உள்ளீட்டு மாதிரிகளின் சராசரி மற்றும் மேட்ரிக்ஸ் ட்ரேஸ் மூலம் வகுத்தல்"

#: ../../tutorials/10_effective_dimension.ipynb:54
msgid "**Effective Dimension Calculation:** according to the formula from `Abbas et al. <https://arxiv.org/pdf/2011.00027.pdf>`__"
msgstr "**பயனுள்ள பரிமாணக் கணக்கீடு:** `அப்பாஸ் மற்றும் பலர் வழங்கிய சூத்திரத்தின்படி. <https://arxiv.org/pdf/2011.00027.pdf>`__"

#: ../../tutorials/10_effective_dimension.ipynb:66
msgid "3. Basic Example (SamplerQNN)"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:68
msgid "This example shows how to set up a QNN model problem and run the global effective dimension algorithm. Both Qiskit ``SamplerQNN`` (shown in this example) and ``EstimatorQNN`` (shown in a later example) can be used with the ``EffectiveDimension`` class."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:70
msgid "We start off from the required imports and a fixed seed for the random number generator for reproducibility purposes."
msgstr "தேவையான இறக்குமதிகள் மற்றும் மறுஉற்பத்தி நோக்கங்களுக்காக சீரற்ற எண் ஜெனரேட்டருக்கான நிலையான விதையிலிருந்து தொடங்குகிறோம்."

#: ../../tutorials/10_effective_dimension.ipynb:109
msgid "3.1 Define QNN"
msgstr "3.1 QNN ஐ வரையறுக்கவும்"

#: ../../tutorials/10_effective_dimension.ipynb:111
msgid "The first step to create a ``SamplerQNN`` is to define a parametrized feature map and ansatz. In this toy example, we will use 3 qubits and the ``QNNCircuit`` class to simplify the composition of a feature map and an ansatz circuit. The resulting circuit is then used in the ``SamplerQNN`` class."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:176
msgid "The parametrized circuit can then be sent together with an optional interpret map (parity in this case) to the ``SamplerQNN`` constructor."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:219
msgid "3.2 Set up Effective Dimension calculation"
msgstr "3.2 பயனுள்ள பரிமாண கணக்கீட்டை அமைக்கவும்"

#: ../../tutorials/10_effective_dimension.ipynb:221
msgid "In order to compute the effective dimension of our QNN using the ``EffectiveDimension`` class, we need a series of sets of input samples and weights, as well as the total number of data samples available in a dataset. The ``input_samples`` and ``weight_samples`` are set in the class constructor, while the number of data samples is given during the call to the effective dimension computation, to be able to test and compare how this measure changes with different dataset sizes."
msgstr "``EffectiveDimension`` வகுப்பைப் பயன்படுத்தி எங்கள் QNN இன் பயனுள்ள பரிமாணத்தைக் கணக்கிடுவதற்கு, உள்ளீட்டு மாதிரிகள் மற்றும் எடைகளின் தொடர் தொகுப்புகள் மற்றும் தரவுத்தொகுப்பில் கிடைக்கும் தரவு மாதிரிகளின் மொத்த எண்ணிக்கையும் தேவை. ``input_samples`` மற்றும் ``weight_samples`` ஆகியவை வகுப்பு கட்டமைப்பாளரில் அமைக்கப்பட்டுள்ளன, அதே நேரத்தில் பயனுள்ள பரிமாணக் கணக்கீட்டிற்கான அழைப்பின் போது தரவு மாதிரிகளின் எண்ணிக்கை வழங்கப்படுகிறது, இந்த அளவீடு வெவ்வேறு தரவுத்தொகுப்புடன் எவ்வாறு மாறுகிறது என்பதைச் சோதித்து ஒப்பிட்டுப் பார்க்க முடியும். அளவுகள்."

#: ../../tutorials/10_effective_dimension.ipynb:232
msgid "We can define the number of input samples and weight samples and the class will randomly sample a corresponding array from a normal (for ``input_samples``) or a uniform (for ``weight_samples``) distribution. Instead of passing a number of samples we can pass an array, sampled manually."
msgstr "உள்ளீட்டு மாதிரிகள் மற்றும் எடை மாதிரிகளின் எண்ணிக்கையை நாம் வரையறுக்கலாம் மற்றும் வகுப்பு தோராயமாக ஒரு சாதாரண (``input_samples``) அல்லது ஒரு சீரான (``weight_samples``) விநியோகத்திலிருந்து தொடர்புடைய வரிசையை மாதிரி செய்யும். பல மாதிரிகளை அனுப்புவதற்குப் பதிலாக, ஒரு வரிசையை கைமுறையாக மாதிரியாக அனுப்பலாம்."

#: ../../tutorials/10_effective_dimension.ipynb:259
msgid "If we want to test a specific set of input samples and weight samples, we can provide it directly to the ``EffectiveDimension`` class as shown in the following snippet:"
msgstr "உள்ளீட்டு மாதிரிகள் மற்றும் எடை மாதிரிகளின் குறிப்பிட்ட தொகுப்பைச் சோதிக்க விரும்பினால், பின்வரும் துணுக்கில் காட்டப்பட்டுள்ளபடி நேரடியாக ``EffectiveDimension`` வகுப்பிற்கு வழங்கலாம்:"

#: ../../tutorials/10_effective_dimension.ipynb:284
msgid "The effective dimension algorithm also requires a dataset size. In this example, we will define an array of sizes to later see how this input affects the result."
msgstr "பயனுள்ள பரிமாண அல்காரிதத்திற்கு தரவுத்தொகுப்பு அளவும் தேவைப்படுகிறது. இந்த எடுத்துக்காட்டில், இந்த உள்ளீடு முடிவை எவ்வாறு பாதிக்கிறது என்பதைப் பார்க்க, அளவுகளின் வரிசையை வரையறுப்போம்."

#: ../../tutorials/10_effective_dimension.ipynb:307
msgid "3.3 Compute Global Effective Dimension"
msgstr "3.3 உலகளாவிய பயனுள்ள பரிமாணத்தைக் கணக்கிடுங்கள்"

#: ../../tutorials/10_effective_dimension.ipynb:309
msgid "Let's now calculate the effective dimension of our network for the previously defined set of input samples, weights, and a dataset size of 5000."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:330
msgid "The effective dimension values will range between 0 and ``d``, where ``d`` represents the dimension of the model, and it's practically obtained from the number of weights of the QNN. By dividing the result by ``d``, we can obtain the normalized effective dimension, which correlates directly with the capacity of the model."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:383
msgid "By calling the ``EffectiveDimension`` class with an array if input sizes ``n``, we can monitor how the effective dimension changes with the dataset size."
msgstr "உள்ளீட்டு அளவுகள் ``n`` எனில் அணிவரிசையுடன் ``EffectiveDimension`` வகுப்பை அழைப்பதன் மூலம், தரவுத்தொகுப்பின் அளவுடன் பயனுள்ள பரிமாணம் எவ்வாறு மாறுகிறது என்பதைக் கண்காணிக்கலாம்."

#: ../../tutorials/10_effective_dimension.ipynb:468
msgid "4. Local Effective Dimension Example"
msgstr "4. லோக்கல் எஃபெக்டிவ் டைமன்ஷன் உதாரணம்"

#: ../../tutorials/10_effective_dimension.ipynb:470
msgid "As explained in the introduction, the local effective dimension algorithm only uses **one** set of weights, and it can be used to monitor how training affects the expressiveness of a neural network. The ``LocalEffectiveDimension`` class enforces this constraint to ensure that these calculations are conceptually separate, but the rest of the implementation is shared with ``EffectiveDimension``."
msgstr "அறிமுகத்தில் விளக்கப்பட்டுள்ளபடி, உள்ளூர் பயனுள்ள பரிமாண வழிமுறையானது **ஒன்று** எடைகளின் தொகுப்பை மட்டுமே பயன்படுத்துகிறது, மேலும் பயிற்சியானது நரம்பியல் வலையமைப்பின் வெளிப்பாட்டை எவ்வாறு பாதிக்கிறது என்பதைக் கண்காணிக்கப் பயன்படுகிறது. இந்தக் கணக்கீடுகள் கருத்தியல் ரீதியாக தனித்தனியாக இருப்பதை உறுதிசெய்ய ``LocalEffective Dimension`` வகுப்பு இந்தக் கட்டுப்பாட்டைச் செயல்படுத்துகிறது, ஆனால் மீதமுள்ள செயல்படுத்தல் ``Effective Dimension`` உடன் பகிரப்படுகிறது."

#: ../../tutorials/10_effective_dimension.ipynb:472
msgid "This example shows how to leverage the ``LocalEffectiveDimension`` class to analyze the effect of training on QNN expressiveness."
msgstr "QNN வெளிப்பாட்டின் மீதான பயிற்சியின் விளைவை பகுப்பாய்வு செய்ய ``LocalEffective Dimension`` வகுப்பை எவ்வாறு பயன்படுத்துவது என்பதை இந்த எடுத்துக்காட்டு காட்டுகிறது."

#: ../../tutorials/10_effective_dimension.ipynb:484
msgid "4.1 Define Dataset and QNN"
msgstr "4.1 தரவுத்தொகுப்பு மற்றும் QNN ஐ வரையறுக்கவும்"

#: ../../tutorials/10_effective_dimension.ipynb:486
msgid "We start by creating a 3D binary classification dataset using ``make_classification`` function from scikit-learn."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:519
msgid "The next step is to create a QNN, an instance of ``EstimatorQNN`` in our case in the same fashion we created an instance of ``SamplerQNN``."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:541
msgid "4.2 Train QNN"
msgstr "4.2 ரயில் QNN"

#: ../../tutorials/10_effective_dimension.ipynb:543
msgid "We can now proceed to train the QNN. The training step may take some time, be patient. You can pass a callback to the classifier to observe how the training process is going on. We fix ``initial_point`` for reproducibility purposes as usual."
msgstr "நாம் இப்போது QNN க்கு பயிற்சி அளிக்கத் தொடரலாம். பயிற்சி படி சிறிது நேரம் ஆகலாம், பொறுமையாக இருங்கள். பயிற்சி செயல்முறை எவ்வாறு நடக்கிறது என்பதைக் கவனிக்க, வகைப்படுத்திக்கு நீங்கள் திரும்ப அழைப்பை அனுப்பலாம். வழக்கம்போல் மறுஉருவாக்கம் நோக்கங்களுக்காக ``initial_point`` ஐ சரிசெய்கிறோம்."

#: ../../tutorials/10_effective_dimension.ipynb:616
msgid "The classifier can now differentiate between classes with an accuracy of:"
msgstr "வகைப்படுத்தி இப்போது வகுப்புகளை துல்லியமாக வேறுபடுத்த முடியும்:"

#: ../../tutorials/10_effective_dimension.ipynb:664
msgid "4.3 Compute Local Effective Dimension of trained QNN"
msgstr "4.3 பயிற்சியளிக்கப்பட்ட QNN இன் உள்ளூர் பயனுள்ள பரிமாணத்தைக் கணக்கிடுங்கள்"

#: ../../tutorials/10_effective_dimension.ipynb:666
msgid "Now that we have trained our network, let's evaluate the local effective dimension based on the trained weights. To do that we access the trained weights directly from the classifier."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:727
msgid "4.4 Compute Local Effective Dimension of untrained QNN"
msgstr "4.4 பயிற்சி பெறாத QNN இன் உள்ளூர் பயனுள்ள பரிமாணத்தைக் கணக்கிடுங்கள்"

#: ../../tutorials/10_effective_dimension.ipynb:729
msgid "We can compare this result with the effective dimension of the untrained network, using the ``initial_point`` as our weight sample:"
msgstr "எங்கள் எடை மாதிரியாக ``initial_point`` ஐப் பயன்படுத்தி, பயிற்சி பெறாத நெட்வொர்க்கின் பயனுள்ள பரிமாணத்துடன் இந்த முடிவை ஒப்பிடலாம்:"

#: ../../tutorials/10_effective_dimension.ipynb:788
msgid "4.5 Plot and analyze results"
msgstr "4.5 முடிவுகளை வரைந்து பகுப்பாய்வு செய்யுங்கள்"

#: ../../tutorials/10_effective_dimension.ipynb:790
msgid "If we plot the effective dimension values before and after training, we can see the following result:"
msgstr "பயிற்சிக்கு முன்னும் பின்னும் பயனுள்ள பரிமாண மதிப்புகளை நாம் திட்டமிட்டால், பின்வரும் முடிவைக் காணலாம்:"

#: ../../tutorials/10_effective_dimension.ipynb:828
msgid "In general, we should expect the value of the local effective dimension to decrease after training. This can be understood by looking back into the main goal of machine learning, which is to pick a model that is expressive enough to fit your data, but not too expressive that it overfits and performs badly on new data samples."
msgstr "பொதுவாக, பயிற்சிக்குப் பிறகு உள்ளூர் பயனுள்ள பரிமாணத்தின் மதிப்பு குறையும் என்று எதிர்பார்க்க வேண்டும். மெஷின் லேர்னிங்கின் முக்கிய இலக்கைத் திரும்பிப் பார்ப்பதன் மூலம் இதைப் புரிந்து கொள்ள முடியும், இது உங்கள் தரவுகளுக்குப் பொருந்தக்கூடிய அளவுக்கு வெளிப்படுத்தக்கூடிய மாதிரியைத் தேர்ந்தெடுப்பது, ஆனால் புதிய தரவு மாதிரிகளில் மிகையாகப் பொருந்தி மோசமாகச் செயல்படும் மாதிரியைத் தேர்ந்தெடுப்பது அல்ல."

#: ../../tutorials/10_effective_dimension.ipynb:830
msgid "Certain optimizers help regularize the overfitting of a model by learning parameters, and this action of learning inherently reduces a model’s expressiveness, as measured by the local effective dimension. Following this logic, a randomly initialized parameter set will most likely produce a higher effective dimension that the final set of trained weights, because that model with that particular parameterization is “using more parameters” unnecessarily to fit the data. After training (with the implicit regularization), a trained model will not need to use so many parameters and thus have more “inactive parameters” and a lower effective dimension."
msgstr "கற்றல் அளவுருக்கள் மூலம் ஒரு மாதிரியின் அதிகப்படியான பொருத்தத்தை முறைப்படுத்த சில உகப்பாக்கிகள் உதவுகின்றன, மேலும் இந்த கற்றல் செயல், உள்ளூர் பயனுள்ள பரிமாணத்தால் அளவிடப்படும் மாதிரியின் வெளிப்பாட்டுத்தன்மையை இயல்பாகவே குறைக்கிறது. இந்த தர்க்கத்தைப் பின்பற்றி, தோராயமாக துவக்கப்பட்ட அளவுரு தொகுப்பு, பயிற்சி பெற்ற எடைகளின் இறுதித் தொகுப்பை விட அதிக செயல்திறன் கொண்ட பரிமாணத்தை உருவாக்கும், ஏனெனில் அந்த குறிப்பிட்ட அளவுருவுடன் கூடிய அந்த மாதிரியானது தரவை பொருத்துவதற்கு தேவையில்லாமல் \"அதிக அளவுருக்களைப் பயன்படுத்துகிறது\". பயிற்சிக்குப் பிறகு (மறைமுகமான முறைப்படுத்தலுடன்), பயிற்சியளிக்கப்பட்ட மாதிரி பல அளவுருக்களைப் பயன்படுத்த வேண்டிய அவசியமில்லை, இதனால் அதிக \"செயலற்ற அளவுருக்கள்\" மற்றும் குறைந்த செயல்திறன் பரிமாணம் இருக்கும்."

#: ../../tutorials/10_effective_dimension.ipynb:833
msgid "We must keep in mind though that this is the general intuition, and there might be cases where a randomly selected set of weights happens to provide a lower effective dimension than the trained weights for a specific model."
msgstr ""

