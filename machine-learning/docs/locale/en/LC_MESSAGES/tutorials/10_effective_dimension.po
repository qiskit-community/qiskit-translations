# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2018, 2023, Qiskit Machine Learning Development Team
# This file is distributed under the same license as the Qiskit Machine
# Learning package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Qiskit Machine Learning 0.6.1\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-05-22 20:19+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: en\n"
"Language-Team: en <LL@li.org>\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../tutorials/10_effective_dimension.ipynb:9
msgid ""
"This page was generated from "
"`docs/tutorials/10_effective_dimension.ipynb`__."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:9
msgid "Effective Dimension of Qiskit Neural Networks"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:11
msgid ""
"In this tutorial, we will take advantage of the ``EffectiveDimension`` "
"and ``LocalEffectiveDimension`` classes to evaluate the power of Quantum "
"Neural Network models. These are metrics based on information geometry "
"that connect to notions such as trainability, expressibility or ability "
"to generalize."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:13
msgid ""
"Before diving into the code example, we will briefly explain what is the "
"difference between these two metrics, and why are they relevant to the "
"study of Quantum Neural Networks. More information about global effective"
" dimension can be found in `this paper "
"<https://arxiv.org/pdf/2011.00027.pdf>`__, while the local effective "
"dimension was introduced in a `later work "
"<https://arxiv.org/abs/2112.04807>`__."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:25
msgid "1. Global vs. Local Effective Dimension"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:27
msgid ""
"Both classical and quantum machine learning models share a common goal: "
"being good at **generalizing**, i.e. learning insights from data and "
"applying them on unseen data."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:29
msgid ""
"Finding a good metric to assess this ability is a non-trivial matter. In "
"`The Power of Quantum Neural Networks "
"<https://arxiv.org/pdf/2011.00027.pdf>`__, the authors introduce the "
"**global** effective dimension as a useful indicator of how well a "
"particular model will be able to perform on new data. In `Effective "
"Dimension of Machine Learning Models "
"<https://arxiv.org/pdf/2112.04807.pdf>`__, the **local** effective "
"dimension is proposed as a new capacity measure that bounds the "
"generalization error of machine learning models."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:32
msgid ""
"The key difference between global (``EffectiveDimension`` class) and "
"**local** effective dimension (``LocalEffectiveDimension`` class) is "
"actually not in the way they are computed, but in the nature of the "
"parameter space that is analyzed. The global effective dimension "
"incorporates the **full parameter space** of the model, and is calculated"
" from a **large number of parameter (weight) sets**. On the other hand, "
"the local effective dimension focuses on how well the **trained** model "
"can generalize to new data, and how **expressive** it can be. Therefore, "
"the local effective dimension is calculated from **a single** set of "
"weight samples (training result). This difference is small in terms of "
"practical implementation, but quite relevant at a conceptual level."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:45
msgid "2. The Effective Dimension Algorithm"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:47
msgid ""
"Both the global and local effective dimension algorithms use the Fisher "
"Information matrix to provide a measure of complexity. The details on how"
" this matrix is calculated are provided in the `reference paper "
"<https://arxiv.org/pdf/2011.00027.pdf>`__, but in general terms, this "
"matrix captures how sensitive a neural network's output is to changes in "
"the network's parameter space."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:49
msgid "In particular, this algorithm follows 4 main steps:"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:51
msgid ""
"**Monte Carlo simulation:** the forward and backward passes (gradients) "
"of the neural network are computed for each pair of input and weight "
"samples."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:52
msgid ""
"**Fisher Matrix Computation:** these outputs and gradients are used to "
"compute the Fisher Information Matrix."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:53
msgid ""
"**Fisher Matrix Normalization:** averaging over all input samples and "
"dividing by the matrix trace"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:54
msgid ""
"**Effective Dimension Calculation:** according to the formula from `Abbas"
" et al. <https://arxiv.org/pdf/2011.00027.pdf>`__"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:66
msgid "3. Basic Example (SamplerQNN)"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:68
msgid ""
"This example shows how to set up a QNN model problem and run the global "
"effective dimension algorithm. Both Qiskit ``SamplerQNN`` (shown in this "
"example) and ``EstimatorQNN`` (shown in a later example) can be used with"
" the ``EffectiveDimension`` class."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:70
msgid ""
"We start off from the required imports and a fixed seed for the random "
"number generator for reproducibility purposes."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:108
msgid "3.1 Define QNN"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:110
msgid ""
"The first step to create a ``SamplerQNN`` is to define a parametrized "
"feature map and ansatz. In this toy example, we will use 3 qubits, and we"
" will define the circuit used in the ``SamplerQNN`` class."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:150
msgid ""
"The parametrized circuit can then be sent together with an optional "
"interpret map (parity in this case) to the ``SamplerQNN`` constructor."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:195
msgid "3.2 Set up Effective Dimension calculation"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:197
msgid ""
"In order to compute the effective dimension of our QNN using the "
"``EffectiveDimension`` class, we need a series of sets of input samples "
"and weights, as well as the total number of data samples available in a "
"dataset. The ``input_samples`` and ``weight_samples`` are set in the "
"class constructor, while the number of data samples is given during the "
"call to the effective dimension computation, to be able to test and "
"compare how this measure changes with different dataset sizes."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:208
msgid ""
"We can define the number of input samples and weight samples and the "
"class will randomly sample a corresponding array from a normal (for "
"``input_samples``) or a uniform (for ``weight_samples``) distribution. "
"Instead of passing a number of samples we can pass an array, sampled "
"manually."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:235
msgid ""
"If we want to test a specific set of input samples and weight samples, we"
" can provide it directly to the ``EffectiveDimension`` class as shown in "
"the following snippet:"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:260
msgid ""
"The effective dimension algorithm also requires a dataset size. In this "
"example, we will define an array of sizes to later see how this input "
"affects the result."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:283
msgid "3.3 Compute Global Effective Dimension"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:285
msgid ""
"Let's now calculate the effective dimension of our network for the "
"previously defined set of input samples, weights, and a dataset size of "
"5000."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:306
msgid ""
"The effective dimension values will range between 0 and ``d``, where "
"``d`` represents the dimension of the model, and it's practically "
"obtained from the number of weights of the QNN. By dividing the result by"
" ``d``, we can obtain the normalized effective dimension, which "
"correlates directly with the capacity of the model."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:359
msgid ""
"By calling the ``EffectiveDimension`` class with an array if input sizes "
"``n``, we can monitor how the effective dimension changes with the "
"dataset size."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:444
msgid "4. Local Effective Dimension Example"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:446
msgid ""
"As explained in the introduction, the local effective dimension algorithm"
" only uses **one** set of weights, and it can be used to monitor how "
"training affects the expressiveness of a neural network. The "
"``LocalEffectiveDimension`` class enforces this constraint to ensure that"
" these calculations are conceptually separate, but the rest of the "
"implementation is shared with ``EffectiveDimension``."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:448
msgid ""
"This example shows how to leverage the ``LocalEffectiveDimension`` class "
"to analyze the effect of training on QNN expressiveness."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:460
msgid "4.1 Define Dataset and QNN"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:462
msgid ""
"We start by creating a 3D binary classification dataset using "
"``make_classification`` function from scikit-learn."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:495
msgid ""
"The next step is to create a QNN, an instance of ``EstimatorQNN`` in our "
"case in the same fashion we created an instance of ``SamplerQNN``."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:519
msgid "4.2 Train QNN"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:521
msgid ""
"We can now proceed to train the QNN. The training step may take some "
"time, be patient. You can pass a callback to the classifier to observe "
"how the training process is going on. We fix ``initial_point`` for "
"reproducibility purposes as usual."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:594
msgid "The classifier can now differentiate between classes with an accuracy of:"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:642
msgid "4.3 Compute Local Effective Dimension of trained QNN"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:644
msgid ""
"Now that we have trained our network, let's evaluate the local effective "
"dimension based on the trained weights. To do that we access the trained "
"weights directly from the classifier."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:705
msgid "4.4 Compute Local Effective Dimension of untrained QNN"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:707
msgid ""
"We can compare this result with the effective dimension of the untrained "
"network, using the ``initial_point`` as our weight sample:"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:766
msgid "4.5 Plot and analyze results"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:768
msgid ""
"If we plot the effective dimension values before and after training, we "
"can see the following result:"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:806
msgid ""
"In general, we should expect the value of the local effective dimension "
"to decrease after training. This can be understood by looking back into "
"the main goal of machine learning, which is to pick a model that is "
"expressive enough to fit your data, but not too expressive that it "
"overfits and performs badly on new data samples."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:808
msgid ""
"Certain optimizers help regularize the overfitting of a model by learning"
" parameters, and this action of learning inherently reduces a model’s "
"expressiveness, as measured by the local effective dimension. Following "
"this logic, a randomly initialized parameter set will most likely produce"
" a higher effective dimension that the final set of trained weights, "
"because that model with that particular parameterization is “using more "
"parameters” unnecessarily to fit the data. After training (with the "
"implicit regularization), a trained model will not need to use so many "
"parameters and thus have more “inactive parameters” and a lower effective"
" dimension."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:811
msgid ""
"We must keep in mind though that this is the general intuition, and there"
" might be cases where a randomly selected set of weights happens to "
"provide a lower effective dimension than the trained weights for a "
"specific model."
msgstr ""

