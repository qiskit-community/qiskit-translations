# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2018, 2023, Qiskit Machine Learning Development Team
# This file is distributed under the same license as the Qiskit Machine
# Learning package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2023.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Qiskit Machine Learning 0.6.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-03-29 11:18+0000\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language: en\n"
"Language-Team: en <LL@li.org>\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"

#: ../../tutorials/07_pegasos_qsvc.ipynb:9
msgid "This page was generated from `docs/tutorials/07_pegasos_qsvc.ipynb`__."
msgstr ""

#: ../../tutorials/07_pegasos_qsvc.ipynb:9
msgid "Pegasos Quantum Support Vector Classifier"
msgstr ""

#: ../../tutorials/07_pegasos_qsvc.ipynb:11
msgid ""
"There's another SVM based algorithm that benefits from the quantum kernel"
" method. Here, we introduce an implementation of a another classification"
" algorithm, which is an alternative version to the ``QSVC`` available in "
"Qiskit Machine Learning and shown in the `\"Quantum Kernel Machine "
"Learning\" <./03_quantum_kernel.ipynb>`__ tutorial. This classification "
"algorithm implements the Pegasos algorithm from the paper \"Pegasos: "
"Primal Estimated sub-GrAdient SOlver for SVM\" by Shalev-Shwartz et al., "
"see: https://home.ttic.edu/~nati/Publications/PegasosMPB.pdf."
msgstr ""

#: ../../tutorials/07_pegasos_qsvc.ipynb:14
msgid ""
"This algorithm is an alternative to the dual optimization from the "
"``scikit-learn`` package, benefits from the kernel trick, and yields a "
"training complexity that is independent of the size of the training set. "
"Thus, the ``PegasosQSVC`` is expected to train faster than QSVC for "
"sufficiently large training sets."
msgstr ""

#: ../../tutorials/07_pegasos_qsvc.ipynb:16
msgid ""
"The algorithm can be used as direct replacement of ``QSVC`` with some "
"hyper-parameterization."
msgstr ""

#: ../../tutorials/07_pegasos_qsvc.ipynb:27
msgid "Let's generate some data:"
msgstr ""

#: ../../tutorials/07_pegasos_qsvc.ipynb:51
msgid ""
"We pre-process the data to ensure compatibility with the rotation "
"encoding and split it into the training and test datasets."
msgstr ""

#: ../../tutorials/07_pegasos_qsvc.ipynb:81
msgid ""
"We have two features in the dataset, so we set a number of qubits to the "
"number of features in the dataset."
msgstr ""

#: ../../tutorials/07_pegasos_qsvc.ipynb:83
msgid ""
"Then we set :math:`\\tau` to the number of steps performed during the "
"training procedure. Please note that, there is no early stopping "
"criterion in the algorithm. The algorithm iterates over all :math:`\\tau`"
" steps."
msgstr ""

#: ../../tutorials/07_pegasos_qsvc.ipynb:85
msgid ""
"And the last one is the hyperparameter :math:`C`. This is a positive "
"regularization parameter. The strength of the regularization is inversely"
" proportional to :math:`C`. Smaller :math:`C` induce smaller weights "
"which generally helps preventing overfitting. However, due to the nature "
"of this algorithm, some of the computation steps become trivial for "
"larger :math:`C`. Thus, larger :math:`C` improve the performance of the "
"algorithm drastically. If the data is linearly separable in feature "
"space, :math:`C` should be chosen to be large. If the separation is not "
"perfect, :math:`C` should be chosen smaller to prevent overfitting."
msgstr ""

#: ../../tutorials/07_pegasos_qsvc.ipynb:114
msgid "The algorithm will run using:"
msgstr ""

#: ../../tutorials/07_pegasos_qsvc.ipynb:116
msgid "The default fidelity instantiated in ``FidelityQuantumKernel``"
msgstr ""

#: ../../tutorials/07_pegasos_qsvc.ipynb:117
msgid "A quantum kernel created from ``ZFeatureMap``"
msgstr ""

#: ../../tutorials/07_pegasos_qsvc.ipynb:148
msgid ""
"The implementation ``PegasosQSVC`` is compatible with the ``scikit-"
"learn`` interfaces and has a pretty standard way of training a model. In "
"the constructor we pass parameters of the algorithm, in this case there "
"are a regularization hyper-parameter :math:`C` and a number of steps."
msgstr ""

#: ../../tutorials/07_pegasos_qsvc.ipynb:150
msgid ""
"Then we pass training features and labels to the ``fit`` method, which "
"trains a models and returns a fitted classifier."
msgstr ""

#: ../../tutorials/07_pegasos_qsvc.ipynb:152
msgid "Afterwards, we score our model using test features and labels."
msgstr ""

#: ../../tutorials/07_pegasos_qsvc.ipynb:206
msgid ""
"For visualization purposes we create a mesh grid of a predefined step "
"that spans our minimum and maximum values we applied in MinMaxScaler. We "
"also add some margin to the grid for better representation of the "
"training and test samples."
msgstr ""

#: ../../tutorials/07_pegasos_qsvc.ipynb:231
msgid ""
"We convert the grid to the shape compatible with the model, the shape "
"should be ``(n_samples, n_features)``. Then for each grid point we "
"predict a label. In our case predicted labels will be used for coloring "
"the grid."
msgstr ""

#: ../../tutorials/07_pegasos_qsvc.ipynb:253
msgid ""
"Finally, we plot our grid according to the labels/colors we obtained from"
" the model. We also plot training and test samples."
msgstr ""

