msgid ""
msgstr ""
"Project-Id-Version: qiskit-docs\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-07-12 22:21+0000\n"
"PO-Revision-Date: 2021-07-13 15:47\n"
"Last-Translator: \n"
"Language-Team: Indonesian\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"X-Crowdin-Project: qiskit-docs\n"
"X-Crowdin-Project-ID: 369271\n"
"X-Crowdin-Language: id\n"
"X-Crowdin-File: /master/machine-learning/docs/locale/en/LC_MESSAGES/tutorials.po\n"
"X-Crowdin-File-ID: 9528\n"
"Language: id_ID\n"

#: ../../tutorials/01_neural_networks.ipynb:13
#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:13
#: ../../tutorials/03_quantum_kernel.ipynb:13
#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:13
#: ../../tutorials/05_torch_connector.ipynb:13
msgid "Run interactively in jupyter notebook."
msgstr ""

#: ../../tutorials/01_neural_networks.ipynb:9
msgid "Quantum Neural Networks"
msgstr ""

#: ../../tutorials/01_neural_networks.ipynb:11
msgid "This notebook demonstrates the different generic quantum neural network (QNN) implementations provided in Qiskit Machine Learning. The networks are meant as application-agnostic computational units that can be used for many different use cases. Depending on the application, a particular type of network might more or less suitable and might require to be set up in a particular way. The following different available neural networks will now be discussed in more detail:"
msgstr ""

#: ../../tutorials/01_neural_networks.ipynb:13
msgid "``NeuralNetwork``: The interface for neural networks."
msgstr ""

#: ../../tutorials/01_neural_networks.ipynb:14
msgid "``OpflowQNN``: A network based on the evaluation of quantum mechanical observables."
msgstr ""

#: ../../tutorials/01_neural_networks.ipynb:15
msgid "``TwoLayerQNN``: A special ``OpflowQNN`` implementation for convenience."
msgstr ""

#: ../../tutorials/01_neural_networks.ipynb:16
msgid "``CircuitQNN``: A network based on the samples resulting from measuring a quantum circuit."
msgstr ""

#: ../../tutorials/01_neural_networks.ipynb:64
msgid "1. ``NeuralNetwork``"
msgstr ""

#: ../../tutorials/01_neural_networks.ipynb:66
msgid "The ``NeuralNetwork`` represents the interface for all neural networks available in Qiskit Machine Learning. It exposes a forward and a backward pass taking the data samples and trainable weights as input. A ``NeuralNetwork`` does not contain any training capabilities, these are pushed to the actual algorithms / applications. Thus, a ``NeuralNetwork`` also does not store the values for trainable weights. In the following, different implementations of this interfaces are introduced."
msgstr ""

#: ../../tutorials/01_neural_networks.ipynb:68
msgid "Suppose a ``NeuralNetwork`` called ``nn``. Then, the ``nn.forward(input, weights)`` pass takes either flat inputs for the data and weights of size ``nn.num_inputs`` and ``nn.num_weights``, respectively. ``NeuralNetwork`` supports batching of inputs and returns batches of output of the corresponding shape."
msgstr "Misalkan sebuah ``NeuralNetwork`` disebut ``nn``. Kemudian, ``nn.forward(input, weights)`` melewatkan masukan untuk data dan bobot dengan ukuran masing-masing ``nn.num_inputs`` dan ``nn.num_weights``. ``NeuralNetwork`` mendukung pengumpulan (batching) dari input dan mengembalikan kumpulan output dengan bentuk yang sesuai."

#: ../../tutorials/01_neural_networks.ipynb:80
msgid "2. ``OpflowQNN``"
msgstr "2. ``OpflowQNN``"

#: ../../tutorials/01_neural_networks.ipynb:82
msgid "The ``OpflowQNN`` takes a (parametrized) operator from Qiskit and leverages Qiskit’s gradient framework to provide the backward pass. Such an operator can for instance be an expected value of a quantum mechanical observable with respect to a parametrized quantum state. The Parameters can be used to load classical data as well as represent trainable weights. The ``OpflowQNN`` also allows lists of operators and more complex structures to construct more complex QNNs."
msgstr "``OpflowQNN`` mengambil sebuah operator (terparametrisasi) dari Qiskit dan memanfaatkan kerangka gradien Qiskit untuk memberikan umpan balik. Operator semacam itu misalnya dapat berupa nilai ekspektasi dari pengamatan (observable) mekanika kuantum dari keadaan kuantum terparametrisasi. Parameter ini dapat digunakan untuk memuat data klasik serta mewakili bobot yang dapat dilatih. ``OpflowQNN`` juga memungkinkan daftar operator dan struktur yang lebih kompleks untuk membangun JSTK yang lebih kompleks."

#: ../../tutorials/01_neural_networks.ipynb:321
msgid "Combining multiple observables in a ``ListOp`` also allows to create more complex QNNs"
msgstr "Menggabungkan beberapa pengamatan (observables) dalam sebuah ``ListOp`` juga memungkinkan untuk membuat JSTK yang lebih kompleks"

#: ../../tutorials/01_neural_networks.ipynb:412
msgid "3. ``TwoLayerQNN``"
msgstr "3. ``TwoLayerQNN``"

#: ../../tutorials/01_neural_networks.ipynb:414
msgid "The ``TwoLayerQNN`` is a special ``OpflowQNN`` on :math:`n` qubits that consists of first a feature map to insert data and second an ansatz that is trained. The default observable is :math:`Z^{\\otimes n}`, i.e., parity."
msgstr "``TwoLayerQNN`` adalah ``OpflowQNN`` khusus untuk :math:`n` qubit yang terdiri dari: sebuah peta fitur untuk menyisipkan data dan sebuah ansatz yang dilatih. Pengamatan (observable) standar yang biasa digunakan adalah :math:`Z^{\\otimes n}`, yaitu, paritas."

#: ../../tutorials/01_neural_networks.ipynb:612
msgid "4. ``CircuitQNN``"
msgstr "4. ``CircuitQNN``"

#: ../../tutorials/01_neural_networks.ipynb:614
msgid "The ``CircuitQNN`` is based on a (parametrized) ``QuantumCircuit``. This can take input as well as weight parameters and produces samples from the measurement. The samples can either be interpreted as probabilities of measuring the integer index corresponding to a bitstring or directly as a batch of binary output. In the case of probabilities, gradients can be estimated efficiently and the ``CircuitQNN`` provides a backward pass as well. In case of samples, differentiation is not possible and the backward pass returns ``(None, None)``."
msgstr "``CircuitQNN`` didasarkan dari (parameterisasi) ``QuantumCircuit``. Ini dapat mengambil input serta bobot parameter dan menghasilkan sampel dari pengukuran. Sampel dapat diinterpretasikan sebagai probabilitas dari pengukuran indeks bilangan bulat sesuai bitstring tertentu atau secara langsung sebagai kumpulan output biner. Untuk kasus probabilitas, gradien dapat diestimasi secara efisien dan juga ``CircuitQNN`` menyediakan umpan mundur. Untuk kasus sampel, pembedaan tidak mungkin dilakukan dan umpan umpan mundur akan mengembalikan ``(None, None)``."

#: ../../tutorials/01_neural_networks.ipynb:617
msgid "Further, the ``CircuitQNN`` allows to specify an ``interpret`` function to post-process the samples. This is expected to take a measured integer (from a bitstring) and map it to a new index, i.e. non-negative integer. In this case, the output shape needs to be provided and the probabilities are aggregated accordingly."
msgstr "Selanjutnya, ``CircuitQNN`` memungkinkan untuk merincikan sebuah fungsi ``interpretasi`` untuk mem-post-process sampel. Satu contoh yang umum digunakan misalnya fungsi untuk mengkonversi sebuah bilangan bulat yang terukur (dari bitstring) dan memetakannya ke dalam sebuah indeks baru, yaitu bilangan bulat non-negatif. Dalam hal ini, bentuk (shape) output perlu didefinisikan dan probabilitas hasil pengukuran akan dikelompokkan sesuai dengan definisi yang diberikan."

#: ../../tutorials/01_neural_networks.ipynb:619
msgid "A ``CircuitQNN`` can be configured to return sparse as well as dense probability vectors. If no ``interpret`` function is used, the dimension of the probability vector scales exponentially with the number of qubits and a sparse recommendation is usually recommended. In case of an ``interpret`` function it depends on the expected outcome. If, for instance, an index is mapped to the parity of the corresponding bitstring, i.e., to 0 or 1, a dense output makes sense and the result will be a probability vector of length 2."
msgstr "Sebuah ``CircuitQNN`` dapat dikonfigurasi untuk mengembalikan vektor probabilitas yang jarang (sparse) ataupun padat (dense). Jika tidak ada fungsi ``interpret`` yang digunakan, dimensi dari vektor probabilitas terskala secara eksponensial dengan banyaknya qubit dan rekomendasi jarang (sparse) biasanya direkomendasikan. Untuk kasus fungsi ``interpret`` bergantung pada hasil yang diinginkan. Jika, sebagai contoh, sebuah indeks dipetakan ke paritas sesuai bitstring-nya, misal, ke 0 atau 1, output yang padat akan masuk akal dan hasilnya akan menjadi vektor probabilitas dengan panjang 2."

#: ../../tutorials/01_neural_networks.ipynb:662
msgid "4.1 Output: sparse integer probabilities"
msgstr "4.1 Output: probabilitas integer yang jarang (sparse)"

#: ../../tutorials/01_neural_networks.ipynb:761
msgid "4.2 Output: dense parity probabilities"
msgstr "4.2 Output: probabilitas paritas yang padat"

#: ../../tutorials/01_neural_networks.ipynb:869
msgid "4.3 Output: Samples"
msgstr "4.3 Output: Sampel-Sampel"

#: ../../tutorials/01_neural_networks.ipynb:985
msgid "4.4 Output: Parity Samples"
msgstr "4.4 Output: Sampel-Sampel Paritas"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:9
msgid "Neural Network Classifier & Regressor"
msgstr "Jaringan Saraf Tiruan dan Regressor"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:11
msgid "In this tutorial we show how the ``NeuralNetworkClassifier`` and ``NeuralNetworkRegressor`` are used. Both take as an input a (Quantum) ``NeuralNetwork`` and leverage it in a specific context. In both cases we also provide a pre-configured variant for convenience, the Variational Quantum Classifier (``VQC``) and Variational Quantum Regressor (``VQR``). The tutorial is structured as follows:"
msgstr "Pada tutorial ini kami menunjukkan bagaimana ``NeuralNetworkClassifier`` dan ``NeuralNetworkRegressor`` digunakan. Keduanya mengambil input (Kuantum) ``NeuralNetwork`` dan mempengaruhinya pada konteks tertentu. Pada kedua kasus kami menyediakan variasi yang sudah dikonfigurasi untuk memudahkan, yaitu Pengklasifikasi Kuantum Variasional (Variational Quantum Classifier, ``VQC``) dan Regressor Kuantum Variasional (Variational Quantum Regressor, ``VQR``). Struktur tutorial ini sebagai berikut:"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:13
msgid "`Classification <#Classification>`__"
msgstr "`Klasifikasi <#Classification>`__"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:15
msgid "Classification with an ``OpflowQNN``"
msgstr "Klasifikasi dengan ``OpflowQNN``"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:16
#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:249
msgid "Classification with a ``CircuitQNN``"
msgstr "Klasifikasi dengan ``CircuitQNN``"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:17
#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:398
msgid "Variational Quantum Classifier (``VQC``)"
msgstr "Pengklasifikasi Kuantum Variasional (Variational Quantum Classifier, ``VQC``)"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:19
msgid "`Regression <#Regression>`__"
msgstr "`Regresi <#Regression>`__"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:21
#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:539
msgid "Regression with an ``OpflowQNN``"
msgstr "Regresi dengan ``OpflowQNN``"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:22
msgid "Variational Quantum Regressor (``VQR``)"
msgstr "Regressor Kuantum Variasional (Variational Quantum Regressor, ``VQR``)"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:70
#: ../../tutorials/03_quantum_kernel.ipynb:53
msgid "Classification"
msgstr ""

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:72
msgid "We prepare a simple classification dataset to illustrate the following algorithms."
msgstr "Kami menyediakan himpunan data klasifikasi sederhana untuk mengilustrasikan algoritme setelahnya."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:117
msgid "Classification with the an ``OpflowQNN``"
msgstr "Klasifikasi dengan ``OpflowQNN``"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:119
msgid "First we show how an ``OpflowQNN`` can be used for classification within a ``NeuralNetworkClassifier``. In this context, the ``OpflowQNN`` is expected to return one-dimensional output in :math:`[-1, +1]`. This only works for binary classification and we assign the two classes to :math:`\\{-1, +1\\}`. For convenience, we use the ``TwoLayerQNN``, which is a special type of ``OpflowQNN`` defined via a feature map and an ansatz."
msgstr "Pertama-tama kami menunjukkan bagaimana ``OpflowQNN`` dapat digunakan untuk klasifikasi dalam ``NeuralNetworkClassifier``. Pada konteks ini, ``OpflowQNN`` diharapkan mengembalikan output satu dimensi pada :math:`[-1, +1]`. Ini hanya akan bekerja pada klasifikasi biner dan kami menetapkan kedua kelas menjadi :math:`\\{-1, +1\\}`. Untuk memudahkan, kami menggunakan ``TwoLayerQNN``, yang merupakan tipe khusus dari ``OpflowQNN`` yang didefinisikan melalui peta fitur dan ansatz."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:251
msgid "Next we show how a ``CircuitQNN`` can be used for classification within a ``NeuralNetworkClassifier``. In this context, the ``CircuitQNN`` is expected to return :math:`d`-dimensional probability vector as output, where :math:`d` denotes the number of classes. Sampling from a ``QuantumCircuit`` automatically results in a probability distribution and we just need to define a mapping from the measured bitstrings to the different classes. For binary classification we use the parity mapping."
msgstr "Selanjutnya kami menunjukkan bagaimana ``CircuitQNN`` dapat digunakan untuk klasfikasi dalam ``NeuralNetworkClassifier``. Pada konteks ini, ``CircuitQNN`` diharapkan mengembalikan vektor probabilitas berdimensi :math:`d` sebagai output, di mana :math:`d` menyatakan jumlah kelas. Penyampelan dari ``QuantumCircuit`` menghasilkan distributsi probabilitas secara otomatis dan kami hanya perlu mendefinisikan pemetaan dari bitstring yang terukur ke berbagai kelas berbeda. Untuk klasifikasi biner kami menggunakan pemetaan paritas."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:400
msgid "The ``VQC`` is a special variant of the ``NeuralNetworkClassifier`` with a ``CircuitQNN``. It applies a parity mapping (or extensions to multiple classes) to map from the bitstring to the classification, which results in a probability vector, which is interpreted as a one-hot encoded result. By default, it applies this the ``CrossEntropyLoss`` function that expects labels given in one-hot encoded format and will return predictions in that format too."
msgstr "``VQC`` merupakan variasi khusus dari ``NeuralNetworkClassifier`` dengan ``CircuitQNN``. ``VQC`` menerapkan pemetaan paritas (atau ekstensi ke berbagai kelas) untuk memetakan dari bitstring ke klasifikasi, yang menghasilkan vektor probabilitas, yang diintepretasikan sebagai hasil enkode one-hot. Secara umum, ``VQC`` menerapkan fungsi ``CrossEntropyLoss`` yang mengharapkan label yang diberikan dienkode dalam format one-hot dan akan mengembalikan prediksi dalam format itu juga."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:496
msgid "Regression"
msgstr "Regresi"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:498
msgid "We prepare a simple regression dataset to illustrate the following algorithms."
msgstr "Kami menyiapkan sebuah himpunan data regresi sederhana untuk mengilustrasikan algoritme selanjutnya."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:541
msgid "Here we restrict to regression with an ``OpflowQNN`` that returns values in :math:`[-1, +1]`. More complex and also multi-dimensional models could be constructed, also based on ``CircuitQNN`` but that exceeds the scope of this tutorial."
msgstr "Di sini kami membatasi regresi dengan ``OpflowQNN`` yang mengembalikan nilai pada :math:`[-1, +1]`. Model yang lebih kompleks dan berbagai dimensi dapat dikonstruksi, juga berdasarkan ``CircuitQNN``, tetapi itu melebihi lingkupan tutorial ini."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:648
msgid "Regression with the Variational Quantum Regressor (``VQR``)"
msgstr "Regresi dengan Regressor Kuantum Variasional (Variational Quantum Regressor, ``VQR``)"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:650
msgid "Similar to the ``VQC`` for classification, the ``VQR`` is a special variant of the ``NeuralNetworkRegressor`` with a ``OpflowQNN``. By default it considers the ``L2Loss`` function to minimize the mean squared error between predictions and targets."
msgstr "Mirip dengan ``VQC`` untuk klasifikasi, ``VQR`` merupakan variasi khusus ``NeuralNetworkRegressor`` dengan ``OpflowQNN``. Secara umum, ``VQC`` mempertimbangkan fungsi ``L2Loss`` untuk meminimisasi kesalahan kuadrat rata-rata antara prediksi dan target."

#: ../../tutorials/03_quantum_kernel.ipynb:9
msgid "Quantum Kernel Machine Learning"
msgstr "Kernel Pembelajaran Mesin Kuantum"

#: ../../tutorials/03_quantum_kernel.ipynb:11
msgid "The general task of machine learning is to find and study patterns in data. For many datasets, the datapoints are better understood in a higher dimensional feature space, through the use of a kernel function: :math:`k(\\vec{x}_i, \\vec{x}_j) = \\langle f(\\vec{x}_i), f(\\vec{x}_j) \\rangle` where :math:`k` is the kernel function, :math:`\\vec{x}_i, \\vec{x}_j` are :math:`n` dimensional inputs, :math:`f` is a map from :math:`n`-dimension to :math:`m`-dimension space and :math:`\\langle a,b \\rangle` denotes the dot product. When considering finite data, a kernel function can be represented as a matrix: :math:`K_{ij} = k(\\vec{x}_i,\\vec{x}_j)`."
msgstr "Tugas umum pembelajaran mesin adalah mencari dan mempelajari pola pada data. Untuk banyak himpunan data, titik data lebih baik dimenegerti pada ruang fitur yang berdimensi lebih tinggi, melalui penggunaan fungsi kernel: :math:`k(\\vec{x}_i, \\vec{x}_j) = \\langle f(\\vec{x}_i), f(\\vec{x}_j) \\rangle` di mana :math:`k` merupakan fungsi kernel, :math:`\\vec{x}_i, \\vec{x}_j` adalah input :math:`n` dimensi, :math:`f` adalah peta dari ruang dimensi-:math:`n` ke dimensi-:math:`m` dan :math:`\\langle a,b \\rangle` menyatakan produk titik. Saat mempertimbangkan data berhingga, fungsi kernel dapat direpresentasikan sebagai matriks: :math:`K_{ij} = k(\\vec{x}_i,\\vec{x}_j)`."

#: ../../tutorials/03_quantum_kernel.ipynb:14
msgid "In quantum kernel machine learning, a quantum feature map :math:`\\phi(\\vec{x})` is used to map a classical feature vector :math:`\\vec{x}` to a quantum Hilbert space, :math:`| \\phi(\\vec{x})\\rangle \\langle \\phi(\\vec{x})|`, such that :math:`K_{ij} = \\left| \\langle \\phi^\\dagger(\\vec{x}_j)| \\phi(\\vec{x}_i) \\rangle \\right|^{2}`. See `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__ for more details."
msgstr "Pada kernel pembelajaran mesin kuantum, fitur peta kuantum :math:`\\phi(\\vec{x})` digunakan untuk memetakan vektor fitur klasik :math:`\\vec{x}` ke ruang Hilbert kuantum, :math:`| \\phi(\\vec{x})\\rangle \\langle \\phi(\\vec{x})|`, sehingga :math:`K_{ij} = \\left| \\langle \\phi^\\dagger(\\vec{x}_j)| \\phi(\\vec{x}_i) \\rangle \\right|^{2}`. Lihat `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__ untuk lebih jelasnya."

#: ../../tutorials/03_quantum_kernel.ipynb:16
msgid "In this notebook, we use ``qiskit`` to calculate a kernel matrix using a quantum feature map, then use this kernel matrix in ``scikit-learn`` classification and clustering algorithms."
msgstr "Pada notebook ini, kita menggunakan ``qiskit`` untuk mengkalkulasi matriks kernel menggunakan peta fitur kuantum, lalu menggunakan matriks kernel ini pada algoritme klasifikasi dan pengelompokan ``scikit-learn``."

#: ../../tutorials/03_quantum_kernel.ipynb:55
msgid "For our classification example, we will use the *ad hoc dataset* as described in `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__, and the ``scikit-learn`` `support vector machine <https://scikit-learn.org/stable/modules/svm.html>`__ classification (``svc``) algorithm."
msgstr "Untuk contoh klasifikasi, kita akan menggunakan *himpunan data ad hoc* seperti dijelaskan dalam `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__, dan algoritme `support vector machine <https://scikit-learn.org/stable/modules/svm.html>`__ classification (``svc``) algorithm dari ``scikit-learn``."

#: ../../tutorials/03_quantum_kernel.ipynb:111
msgid "With our training and testing datasets ready, we set up the ``QuantumKernel`` class to calculate a kernel matrix using the `ZZFeatureMap <https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZZFeatureMap.html>`__, and the ``BasicAer`` ``qasm_simulator`` using 1024 shots."
msgstr "Setelah himpunan data latih dan uji siap, kita siapkan kelas ``QuantumKernel`` untuk mengkalkulasi matriks kernel menggunakan `ZZFeatureMap <https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZZFeatureMap.html>`__, dan ``qasm_simulator`` ``BasicAer`` menggunakan 1024 tembakan."

#: ../../tutorials/03_quantum_kernel.ipynb:138
msgid "The ``scikit-learn`` ``svc`` algorithm allows us to define a `custom kernel <https://scikit-learn.org/stable/modules/svm.html#custom-kernels>`__ in two ways: by providing the kernel as a callable function or by precomputing the kernel matrix. We can do either of these using the ``QuantumKernel`` class in ``qiskit``."
msgstr "Algoritma ``svc`` dari ``scikit-learn`` membolehkan kita untuk mendefinisikan `kernel kustom <https://scikit-learn.org/stable/modules/svm.html#custom-kernels>`__ dalam dua cara: dengan menyediakan kernel sebagai fungsi yang dapat dipanggil atau dengan menghitung terlebih dahulu matriks kernel. Kita dapat melakukan keduanya menggunakan kelas ``QuantumKernel`` pada ``qiskit``."

#: ../../tutorials/03_quantum_kernel.ipynb:140
msgid "The following code gives the kernel as a callable function:"
msgstr "Kode berikut memberikan kernel sebagai fungsi yang dapat dipanggil:"

#: ../../tutorials/03_quantum_kernel.ipynb:184
msgid "The following code precomputes and plots the training and testing kernel matrices before providing them to the ``scikit-learn`` ``svc`` algorithm:"
msgstr "Kode berikut menghitung terlebih dahulu dan plot matriks kernel latih dan uji sebelum menyediakan algoritme ``svc`` ``scikit-learn``:"

#: ../../tutorials/03_quantum_kernel.ipynb:250
msgid "``qiskit`` also contains the ``qsvc`` class that extends the ``sklearn svc`` class, that can be used as follows:"
msgstr "``qiskit`` juga mengandung kelas ``qsvc`` yang memperluas kelas ``sklearn svc``, yang dapat digunakan sebagai berikut:"

#: ../../tutorials/03_quantum_kernel.ipynb:295
msgid "Clustering"
msgstr "Pengelompokan"

#: ../../tutorials/03_quantum_kernel.ipynb:297
msgid "For our clustering example, we will again use the *ad hoc dataset* as described in `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__, and the ``scikit-learn`` ``spectral`` clustering algorithm."
msgstr "Untuk contoh pengelompokan, kita akan kembali menggunan *himpunan data ad hoc* seperti dijelaskan pada `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__, dan algoritme pengelompokan ``spectral`` ``scikit-learn``."

#: ../../tutorials/03_quantum_kernel.ipynb:299
msgid "We will regenerate the dataset with a larger gap between the two classes, and as clustering is an unsupervised machine learning task, we don’t need a test sample."
msgstr "Kita akan membuat kembali himpunan data dengan jarak lebih besar antara kedua kelas, dan karena pengelompokan merupakan tugas pembelajaran mesin tak terarah, kita tidak memerlukan sampel uji."

#: ../../tutorials/03_quantum_kernel.ipynb:350
msgid "We again set up the ``QuantumKernel`` class to calculate a kernel matrix using the `ZZFeatureMap <https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZZFeatureMap.html>`__, and the BasicAer ``qasm_simulator`` using 1024 shots."
msgstr "Kita kembali menyiapkan kelas ``QuantumKernel`` untuk mengkalkulasi matriks kernel menggunakan `ZZFeatureMap <https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZZFeatureMap.html>`__, dan ``qasm_simulator`` BasicAer menggunakan 1024 tembakan."

#: ../../tutorials/03_quantum_kernel.ipynb:377
msgid "The scikit-learn spectral clustering algorithm allows us to define a [custom kernel] in two ways: by providing the kernel as a callable function or by precomputing the kernel matrix. Using the QuantumKernel class in qiskit, we can only use the latter."
msgstr "Algoritma pengelompokan ``spectral`` ``scikit-learn`` mengizinkan kita untuk mendefinisikan [kernel kustom] dalam dua cara: dengan menyediakan kernel sebagai fungsi yang dapat dipanggil atau dengan menghitung terlebih dahulu matriks kernel. Menggunakan kelas ``QuantumKernel`` pada qiskit, kita hanya dapat menggunakan yang terakhir."

#: ../../tutorials/03_quantum_kernel.ipynb:379
msgid "The following code precomputes and plots the kernel matrices before providing it to the scikit-learn spectral clustering algorithm, and scoring the labels using normalized mutual information, since we a priori know the class labels."
msgstr "Kode berikut menghitung terlebih dahulu dan plot matriks kernel sebelum menyediakan algoritme ``spectral`` ``scikit-learn``, dan menilai label menggunakan informasi bersama ternormalisasi, karena kita sudah mengetahui label kelasnya."

#: ../../tutorials/03_quantum_kernel.ipynb:439
msgid "``scikit-learn`` has other algorithms that can use a precomputed kernel matrix, here are a few:"
msgstr "``scikit-learn`` mempunyai algoritme lainnya yang dapat mengkalkulasi terlebih dahulu matriks, beberapa yang ada:"

#: ../../tutorials/03_quantum_kernel.ipynb:441
msgid "`Agglomerative clustering <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html>`__"
msgstr "`Agglomerative clustering <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html>`__"

#: ../../tutorials/03_quantum_kernel.ipynb:442
msgid "`Support vector regression <https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html>`__"
msgstr "`Support vector regression <https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html>`__"

#: ../../tutorials/03_quantum_kernel.ipynb:443
msgid "`Ridge regression <https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html>`__"
msgstr "`Ridge regression <https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html>`__"

#: ../../tutorials/03_quantum_kernel.ipynb:444
msgid "`Gaussian process regression <https://scikit-learn.org/stable/modules/gaussian_process.html>`__"
msgstr "`Gaussian process regression <https://scikit-learn.org/stable/modules/gaussian_process.html>`__"

#: ../../tutorials/03_quantum_kernel.ipynb:445
msgid "`Principal component analysis <https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html>`__"
msgstr "`Principal component analysis <https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html>`__"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:9
msgid "qGANs for Loading Random Distributions"
msgstr "qGAN untuk Memuat Distribusi Acak"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:11
msgid "Given :math:`k`-dimensional data samples, we employ a quantum Generative Adversarial Network (qGAN) to learn the data’s underlying random distribution and to load it directly into a quantum state:"
msgstr "Diberikan sampel data dimensi-:math:`k`, kita gunakan *quantum Generative Adversarial Network* (qGAN) untuk mempelajari distribusi acak yang mendasari data dan memuatnya secara langsung ke dalam sebuah keadaan kuantum:"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:13
msgid "\\big| g_{\\theta}\\rangle = \\sum_{j=0}^{2^n-1} \\sqrt{p_{\\theta}^{j}}\\big| j \\rangle\n\n"
msgstr "\\big| g_{\\theta}\\rangle = \\sum_{j=0}^{2^n-1} \\sqrt{p_{\\theta}^{j}}\\big| j \\rangle\n\n"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:15
msgid "where :math:`p_{\\theta}^{j}` describe the occurrence probabilities of the basis states :math:`\\big| j\\rangle`."
msgstr "di mana :math:`p_{\\theta}^{j}` menyatakan kemungkinan terjadinya probabilitas dari keadaan basis :math:`\\big| j\\rangle`."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:17
msgid "The aim of the qGAN training is to generate a state :math:`\\big| g_{\\theta}\\rangle` where :math:`p_{\\theta}^{j}`, for :math:`j\\in \\left\\{0, \\ldots, {2^n-1} \\right\\}`, describe a probability distribution that is close to the distribution underlying the training data :math:`X=\\left\\{x^0, \\ldots, x^{k-1} \\right\\}`."
msgstr "Tujuan dari pelatihan qGAN adalah untuk menghasilkan keadaan :math:`\\big| g_{\\theta}\\rangle` di mana :math:`p_{\\theta}^{j}`, for :math:`j\\in \\left\\{0, \\ldots, {2^n-1} \\right\\}`, menjelaskan distribusi probabilitas yang dekat dengan distribusi yang mendasari data latih :math:`X=\\left\\{x^0, \\ldots, x^{k-1} \\right\\}`."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:19
msgid "For further details please refer to `Quantum Generative Adversarial Networks for Learning and Loading Random Distributions <https://arxiv.org/abs/1904.00043>`__ *Zoufal, Lucchi, Woerner* [2019]."
msgstr "Untuk lebih jelasnya silakan mengacu pada `Quantum Generative Adversarial Networks for Learning and Loading Random Distributions <https://arxiv.org/abs/1904.00043>`__ *Zoufal, Lucchi, Woerner* [2019]."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:21
msgid "For an example of how to use a trained qGAN in an application, the pricing of financial derivatives, please see the `Option Pricing with qGANs <https://github.com/Qiskit/qiskit-finance/tree/main/docs/tutorials/10_qgan_option_pricing.ipynb>`__ tutorial."
msgstr "Sebagai contoh bagaimana menggunakan qGAN yang telah dilatih dalam suatu aplikasi, harga derivatif keuangan, silakan lihat `Option Pricing with qGANs <https://github.com/Qiskit/qiskit-finance/tree/main/docs/tutorials/10_qgan_option_pricing.ipynb>`__ tutorial."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:56
msgid "Load the Training Data"
msgstr "Memuat Data Latih"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:58
msgid "First, we need to load the :math:`k`-dimensional training data samples (here k=1)."
msgstr "Pertama, kita perlu memuat sampel data latih dimensi-:math:`k` (di sini k=1)."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:60
msgid "Next, the data resolution is set, i.e. the min/max data values and the number of qubits used to represent each data dimension."
msgstr "Selanjutnya, resolusi data ditetapkan, yaitu nilai min/max data dan jumlah qubit yang digunakan untuk merepresentasikan setiap dimensi data."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:95
msgid "Initialize the qGAN"
msgstr "Inisialisasi qGAN"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:97
msgid "The qGAN consists of a quantum generator :math:`G_{\\theta}`, i.e., an ansatz, and a classical discriminator :math:`D_{\\phi}`, a neural network."
msgstr "qGAN terdiri dari generator kuantum :math:`G_{\\theta}`, yaitu sebuah ansatz, dan sebuah diskriminator klasik :math:`D_{\\phi}`, sebuah jaringan saraf tiruan."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:99
msgid "To implement the quantum generator, we choose a depth-\\ :math:`1` ansatz that implements :math:`R_Y` rotations and :math:`CZ` gates which takes a uniform distribution as an input state. Notably, for :math:`k>1` the generator’s parameters must be chosen carefully. For example, the circuit depth should be :math:`>1` because higher circuit depths enable the representation of more complex structures."
msgstr "Untuk mengimplementasi generator kuantum, kita memilih ansatz kedalaman-\\ :math:`1` yang mengimplementasikan rotasi :math:`R_Y` dan gerbang :math:`CZ` yang mengambil distribusi seragam sebagai keadaan input. Terutama, untuk :math:`k>1 parameter dari generator perlu dipilih secara hati-hati. Sebagai contoh, kedalaman sirkuit perlu :math:`>1` karena kedalaman sirkuit yang lebih tinggi membuat representasi struktur yang lebih kompleks."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:101
msgid "The classical discriminator used here is based on a neural network implementation using NumPy. There is also a discriminator based on PyTorch which is not installed by default when installing Qiskit - see `Optional Install <https://github.com/Qiskit/qiskit-machine-learning#optional-installs>`__ for more information."
msgstr "Diskriminator klasik yang digunakan di sini didasarkan dari implementasi jaringan saraf tiruan menggunakan NumPy. Ada juga diskriminator berdasarkan PyTorch yang tidak terpasang secara default saat menginstal Qiskit - lihat `Instalasi Opsional <https://github.com/Qiskit/qiskit-machine-learning#optional-installs>`__ untuk informasi lebih lanjut."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:103
msgid "Here, both networks are updated with the ADAM optimization algorithm (ADAM is qGAN optimizer default)."
msgstr "Di sini, kedua jaringan diperbarui dengan algoritme optimasi ADAM (ADAM adalah pengoptimasi qGAN default)."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:164
msgid "Run the qGAN Training"
msgstr "Menjalankan Pelatihan qGAN"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:166
msgid "During the training the discriminator’s and the generator’s parameters are updated alternately w.r.t the following loss functions:"
msgstr "Selama pelatihan, parameter dari diskriminator dan generator diperbarui secara bergantian dengan fungsi loss berikut:"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:168
msgid "L_G\\left(\\phi, \\theta\\right) = -\\frac{1}{m}\\sum\\limits_{l=1}^{m}\\left[\\log\\left(D_{\\phi}\\left(g^{l}\\right)\\right)\\right]\n\n"
msgstr "L_G\\left(\\phi, \\theta\\right) = -\\frac{1}{m}\\sum\\limits_{l=1}^{m}\\left[\\log\\left(D_{\\phi}\\left(g^{l}\\right)\\right)\\right]\n\n"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:170
msgid "and"
msgstr "dan"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:172
msgid "L_D\\left(\\phi, \\theta\\right) =\n"
"  \\frac{1}{m}\\sum\\limits_{l=1}^{m}\\left[\\log D_{\\phi}\\left(x^{l}\\right) + \\log\\left(1-D_{\\phi}\\left(g^{l}\\right)\\right)\\right],"
msgstr "L_D\\left(\\phi, \\theta\\right) =\n"
"  \\frac{1}{m}\\sum\\limits_{l=1}^{m}\\left[\\log D_{\\phi}\\left(x^{l}\\right) + \\log\\left(1-D_{\\phi}\\left(g^{l}\\right)\\right)\\right],"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:177
msgid "with :math:`m` denoting the batch size and :math:`g^l` describing the data samples generated by the quantum generator."
msgstr "dengan :math:`m` menyatakan ukuran kumpulan dan :math:`g^l` mendeskripsikan sampel data yang dihasilkan oleh generator kuantum."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:179
msgid "Please note that the training, for the purpose of this notebook, has been kept briefer by the selection of a known initial point (``init_params``). Without such prior knowledge be aware training may take some while."
msgstr "Harap diperhatikan bahwa pelatihan, untuk tujuan notebook ini, dipersingkat dengan pemilihan titik awal yang diketahui (``init_params``). Tanpa pengetahuan awal ini, ketahui bahwa pelatihan dapat memakan waktu lebih lama."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:245
msgid "Training Progress & Outcome"
msgstr "Kemajuan & Hasil Pelatihan"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:247
msgid "Now, we plot the evolution of the generator’s and the discriminator’s loss functions during the training, as well as the progress in the relative entropy between the trained and the target distribution."
msgstr "Sekarang, kita plot evolusi fungsi loss dari generator dan diskriminator selama pelatihan, serta kemajuan entropi relatif antara distribusi yang dilatih dan target."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:249
msgid "Finally, we also compare the cumulative distribution function (CDF) of the trained distribution to the CDF of the target distribution."
msgstr "Akhirnya, kita juga membandingkan fungsi distribusi kumulatif (FDK) dari distribusi yang dilatih dan FDK dari distribusi target."

#: ../../tutorials/05_torch_connector.ipynb:9
msgid "Torch Connector and Hybrid QNNs"
msgstr "Konektor Torch dan QNN hibrida"

#: ../../tutorials/05_torch_connector.ipynb:11
msgid "This tutorial introduces Qiskit’s ``TorchConnector`` class, and demonstrates how the ``TorchConnector`` allows for a natural integration of any ``NeuralNetwork`` from Qiskit Machine Learning into a PyTorch workflow. ``TorchConnector`` takes a Qiskit ``NeuralNetwork`` and makes it available as a PyTorch ``Module``. The resulting module can be seamlessly incorporated into PyTorch classical architectures and trained jointly without additional considerations, enabling the development and testing of novel **hybrid quantum-classical** machine learning architectures."
msgstr "Tutorial ini mengenalkan kelas ``TorchConnector`` Qiskit, dan mendemonstrasikan bagaimana ``TorchConnector`` mengizinkan integrasi natural ``NeuralNetwork`` apa saja dari Qiskit Machine Learning ke dalam alur kerja PyTorch. ``TorchConnector`` mengambil ``NeuralNetwork`` Qiskit dan membuatnya tersedia oleh ``Module`` PyTorch. Hasil modul dapat dimasukkan secara mulus ke arsitektur klasik PyTorch dan dilatih bersama tanpa pertimbangan tambahan, memungkinkan pengembangan dan pengujian dari arsitektur pembelajaran mesin **hybrid quantum-classical**."

#: ../../tutorials/05_torch_connector.ipynb:15
msgid "Content:"
msgstr "Isi:"

#: ../../tutorials/05_torch_connector.ipynb:17
msgid "`Part 1: Simple Classification & Regression <#Part-1:-Simple-Classification-&-Regression>`__"
msgstr "'Bagian 1: Klasifikasi & Regresi Sederhana <#Part-1:-Simple-Classification-&-Regression>`__"

#: ../../tutorials/05_torch_connector.ipynb:19
msgid "The first part of this tutorial shows how quantum neural networks can be trained using PyTorch’s automatic differentiation engine (``torch.autograd``, `link <https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html>`__) for simple classification and regression tasks."
msgstr "Bagian pertama dari tutorial ini menunjukkan bagaimana jaringan saraf tiruan kuantum dapat dilatih menggunakan mesin diferensiasi otomatis PyTorch (``torch.autograd``, `link <https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html>`__) for untuk tugas klasifikasi dan regresi sederhana."

#: ../../tutorials/05_torch_connector.ipynb:21
msgid "`Classification <#1.-Classification>`__"
msgstr "`Klasifikasi <#1.-Classification>`__"

#: ../../tutorials/05_torch_connector.ipynb:23
msgid "Classification with PyTorch and ``OpflowQNN``"
msgstr "Klasifikasi dengan PyTorch dan ``OpflowQNN``"

#: ../../tutorials/05_torch_connector.ipynb:24
msgid "Classification with PyTorch and ``CircuitQNN``"
msgstr "Klasifikasi dengan PyTorch dan ``CircuitQNN``"

#: ../../tutorials/05_torch_connector.ipynb:26
msgid "`Regression <#2.-Regression>`__"
msgstr "`Regresi <#2.-Regression>`__"

#: ../../tutorials/05_torch_connector.ipynb:28
msgid "Regression with PyTorch and ``OpflowQNN``"
msgstr "Regresi dengan PyTorch dan ``OpflowQNN``"

#: ../../tutorials/05_torch_connector.ipynb:30
msgid "`Part 2: MNIST Classification, Hybrid QNNs <#Part-2:-MNIST-Classification,-Hybrid-QNNs>`__"
msgstr "`Bagian 2: Klasifikasi MNIST, QNN Hibrida <#Part-2:-MNIST-Classification,-Hybrid-QNNs>`__"

#: ../../tutorials/05_torch_connector.ipynb:32
msgid "The second part of this tutorial illustrates how to embed a (Quantum) ``NeuralNetwork`` into a target PyTorch workflow (in this case, a typical CNN architecture) to classify MNIST data in a hybrid quantum-classical manner."
msgstr "Bagian kedua tutorial ini menggambarkan bagaimana untuk melekatkan ``NeuralNetwork`` (Kuantum) ke alur kerja PyTorch target (pada kasus ini, arsitektur CNN tipikal) untuk mengklasifikasi data MNIST dalam cara kuantum-klasik hibrida."

#: ../../tutorials/05_torch_connector.ipynb:74
msgid "Part 1: Simple Classification & Regression"
msgstr "Bagian 1: Klasifikasi & Regresi Sederhana"

#: ../../tutorials/05_torch_connector.ipynb:86
msgid "1. Classification"
msgstr "1. Klasifikasi"

#: ../../tutorials/05_torch_connector.ipynb:88
msgid "First, we show how ``TorchConnector`` allows to train a Quantum ``NeuralNetwork`` to solve a classification tasks using PyTorch’s automatic differentiation engine. In order to illustrate this, we will perform **binary classification** on a randomly generated dataset."
msgstr "Pertama, kita menunjukkan bagaimana ``TorchConnector`` memungkinkan untuk melatih ``NeuralNetwork`` Kuantum untuk menyelesaikan masalah klasifikasi menggunakan mesin diferensiasi otomatis PyTorch. Untuk menggambarkan hal ini, kita akan melakukan **klasifikasi biner** pada himpunan data yang dihasilkan secara acak."

#: ../../tutorials/05_torch_connector.ipynb:144
msgid "A. Classification with PyTorch and ``OpflowQNN``"
msgstr "A. Klasifikasi dengan PyTorch dan ``OpflowQNN``"

#: ../../tutorials/05_torch_connector.ipynb:146
msgid "Linking an ``OpflowQNN`` to PyTorch is relatively straightforward. Here we illustrate this using the ``TwoLayerQNN``, a sub-case of ``OpflowQNN`` introduced in previous tutorials."
msgstr "Menghubungkan ``OpflowQNN`` ke PyTorch relatif mudah. Di sini kita mengilustrasikan hal ini menggunakan ``TwoLayerQNN``, sub-kasus dari ``OpflowQNN`` yang diperkenalkan pada tutorial sebelumnya."

#: ../../tutorials/05_torch_connector.ipynb:254
msgid "Optimizer"
msgstr "Pengoptimasi"

#: ../../tutorials/05_torch_connector.ipynb:256
msgid "The choice of optimizer for training any machine learning model can be crucial in determining the success of our training’s outcome. When using ``TorchConnector``, we get access to all of the optimizer algorithms defined in the [``torch.optim``] package (`link <https://pytorch.org/docs/stable/optim.html>`__). Some of the most famous algorithms used in popular machine learning architectures include *Adam*, *SGD*, or *Adagrad*. However, for this tutorial we will be using the L-BFGS algorithm (``torch.optim.LBFGS``), one of the most well know second-order optimization algorithms for numerical optimization."
msgstr "Pemilihan pengoptimasiuntuk pelatihan model pembelajaran mesin apapun dapat menjadi sangat penting dalam menentukan keberhasilan hasil latihan. Saat menggunakan ``TorchConnector``, kita mendapatkan akses ke semua algoritme pengoptimasi yang didefinisikan di paket [``torch.optim``] (`link <https://pytorch.org/docs/stable/optim.html>`__). Sebagian dari algoritme yang paling terkenal dipakai dalam arsitektur pembelajaran mesin termasuk *Adam*, *SGD*, atau *Adagard*. Tetapi, untuk tutorial ini kita akan menggunakan algoritme L-BFGS (``torch.optim.LBFGS``), salah satu dari algoritme optimasi urutan-kedua yang banyak diketahui untuk optimasi numerikal."

#: ../../tutorials/05_torch_connector.ipynb:260
msgid "Loss Function"
msgstr "Fungsi Loss"

#: ../../tutorials/05_torch_connector.ipynb:262
msgid "As for the loss function, we can also take advantage of PyTorch’s pre-defined modules from ``torch.nn``, such as the `Cross-Entropy <https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html>`__ or `Mean Squared Error <https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html>`__ losses."
msgstr "Untuk fungsi loss, kita dapat memanfaatkan modul PyTorch yang sudah didefinisikan dari ``torch.nn``, seperti loss `Cross-Entropy <https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html>`__ atau `Mean Squared Error <https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html>`__."

#: ../../tutorials/05_torch_connector.ipynb:264
msgid "**💡 Clarification :** In classical machine learning, the general rule of thumb is to apply a Cross-Entropy loss to classification tasks, and MSE loss to regression tasks. However, this recommendation is given under the assumption that the output of the classification network is a class probability value in the [0,1] range (usually this is achieved through a Softmax layer). Because the following example for ``TwoLayerQNN`` does not include such layer, and we don’t apply any mapping to the output (the following section shows an example of application of parity mapping with ``CircuitQNNs``), the QNN’s output can take any value in the range [-1,1]. In case you were wondering, this is the reason why this particular example uses MSELoss for classification despite it not being the norm (but we encourage you to experiment with different loss functions and see how they can impact training results)."
msgstr "**💡 Klarifikasi :** Pada pembelajaran mesin klasik, aturan umum adalah mengaplikasikan loss Cross-Entropy untuk tugas klasifikasi, dan loss MSE untuk tugas regresi. Tetapi, rekomendasi ini diberikan dalam asumsi bahwa output dari jaringan klasifikasi adalah nilai probablitas kelas dalam rentang [0,1] (biasanya ini dapat dicapai menggunakan layer Softmax). Karena contoh berikut untuk ``TwoLayerQNN`` tidak meliputi layer itu, kita tidak mengaplikasikan pemetaan apapun ke output (bagian berikutnya menunjukkan contoh aplikasi dari pemetaan paritas dengan ``CircuitQNNs``), output QNN dapat mengambil nilai dengan rentang [-1,1]. Paada kasus Anda bingung, ini adalah alasan mengapa contoh khusus ini menggunakan MSELoss untuk klasifikasi meskipun ini bukan merupakan kebiasaannya (tetapi kami mendorong Anda untuk bereksperimen dengan berbagai fungsi loss dan melihat bagaimana mereka dapat berpengaruh terhadap hasil pelatihan)."

#: ../../tutorials/05_torch_connector.ipynb:442
#: ../../tutorials/05_torch_connector.ipynb:674
msgid "The red circles indicate wrongly classified data points."
msgstr "Lingkaran merah menunjukkan titik data yang salah terklasifikasi."

#: ../../tutorials/05_torch_connector.ipynb:454
msgid "B. Classification with PyTorch and ``CircuitQNN``"
msgstr "B. Klasifikasi dengan PyTorch dan ``CircuitQNN``"

#: ../../tutorials/05_torch_connector.ipynb:456
msgid "Linking an ``CircuitQNN`` to PyTorch requires a bit more attention than ``OpflowQNN``. Without the correct setup, backpropagation is not possible."
msgstr "Menghubungkan ``CircuitQNN`` ke PyTorch memerlukan sedikit lebih banyak perhatian dibandingkan ``OpflowQNN``. Tanpa pengaturan yang tepat, perambatan mundur (backpropagation) tidak mungkin."

#: ../../tutorials/05_torch_connector.ipynb:458
msgid "In particular, we must make sure that we are returning a dense array of probabilities in the network’s forward pass (``sparse=False``). This parameter is set up to ``False`` by default, so we just have to make sure that it has not been changed."
msgstr "Secara khusus, kita perlu memastikah bahwa kita mengembalikan probabilitas dari array padat dalam jaringan umpan maju (``sparse=False``). Parameter ini diatur ke ``False`` secara default, jadi kita hanya perlu memastikan bahwa itu tidak diubah."

#: ../../tutorials/05_torch_connector.ipynb:460
msgid "**⚠️ Attention:** If we define a custom interpret function ( in the example: ``parity``), we must remember to explicitly provide the desired output shape ( in the example: ``2``). For more info on the initial parameter setup for ``CircuitQNN``, please check out the `official qiskit documentation <https://qiskit.org/documentation/machine-learning/stubs/qiskit_machine_learning.neural_networks.CircuitQNN.html>`__."
msgstr "**⚠️ Perhatian: ** Jika kita mendefinisikan fungsi interpretasi (dalam contoh: ``parity``), kita harus ingat untuk memberikan bentuk keluaran yang diinginkan secara eksplisit (dalam contoh: ``2``). Untuk informasi lebih lanjut tentang pengaturan parameter awal untuk ``CircuitQNN``, silahkan lihat `dokumentasi resmi qiskit <https://qiskit.org/documentation/machine-learning/stubs/qiskit_machine_learning.neural_networks.CircuitQNN.html>` __."

#: ../../tutorials/05_torch_connector.ipynb:523
#: ../../tutorials/05_torch_connector.ipynb:815
msgid "For a reminder on optimizer and loss function choices, you can go back to `this section <#Optimizer>`__."
msgstr "Untuk pengingat pada pilihan pengoptimasi dan fungsi loss, Anda dapat kembali ke ` bagian ini <#Optimizer> ` __."

#: ../../tutorials/05_torch_connector.ipynb:686
msgid "2. Regression"
msgstr "2. Regresi"

#: ../../tutorials/05_torch_connector.ipynb:688
msgid "We use a model based on the ``TwoLayerQNN`` to also illustrate how to perform a regression task. The chosen dataset in this case is randomly generated following a sine wave."
msgstr "Kita menggunakan model berdasarkan ``TwoLayerQNN`` untuk menggambarkan bagaimana melakukan regresi. Himpunan data yang dipilih dalam kasus ini dihasilkan secara acak mengikuti gelombang sinus."

#: ../../tutorials/05_torch_connector.ipynb:730
msgid "A. Regression with PyTorch and ``OpflowQNN``"
msgstr "A. Regresi dengan PyTorch dan ``OpflowQNN``"

#: ../../tutorials/05_torch_connector.ipynb:741
msgid "The network definition and training loop will be analogous to those of the classification task using ``TwoLayerQNN``. In this case, we define our own feature map and ansatz, instead of using the default values."
msgstr "Definisi jaringan dan loop pelatihan akan analogi dengan klasifikasi menggunakan ``TwoLayerQNN``. Pada kasus ini, kita mendefinisikan sendiri peta fitur dan ansatz, bukan menggunakan nilai default."

#: ../../tutorials/05_torch_connector.ipynb:963
msgid "Part 2: MNIST Classification, Hybrid QNNs"
msgstr "Bagian 2: Klasifikasi MNIST, QNN hibrida"

#: ../../tutorials/05_torch_connector.ipynb:965
msgid "In this second part, we show how to leverage a hybrid quantum-classical neural network using ``TorchConnector``, to perform a more complex image classification task on the MNIST handwritten digits dataset."
msgstr "Pada bagian kedua, kami menunjukkan bagaimana cara memanfaatkan jaringan saraf kuantum-klasik hibrida menggunakan ``TorchConnector``, untuk melakukan klasifikasi gambar yang lebih kompleks dalam himpunan data angka yang ditulis tangan MNIST."

#: ../../tutorials/05_torch_connector.ipynb:967
msgid "For a more detailed (pre-``TorchConnector``) explanation on hybrid quantum-classical neural networks, you can check out the corresponding section in the `Qiskit Textbook <https://qiskit.org/textbook/ch-machine-learning/machine-learning-qiskit-pytorch.html>`__."
msgstr "Untuk penjelasan (sebelum ``TorchConnector``) lebih rinci mengenai jaringan saraf kuantum-klasik, Anda dapat memeriksa bagian yang sesuai dalam `Qiskit Textbook <https://qiskit.org/textbook/ch-machine-learning/machine-learning-qiskit-pytorch.html>`__."

#: ../../tutorials/05_torch_connector.ipynb:996
msgid "Step 1: Defining Data-loaders for train and test"
msgstr "Langkah 1: Mendefinisikan Pemuat-data untuk latihan dan uji"

#: ../../tutorials/05_torch_connector.ipynb:1007
msgid "We take advantage of the ``torchvision`` `API <https://pytorch.org/vision/stable/datasets.html>`__ to directly load a subset of the `MNIST dataset <https://en.wikipedia.org/wiki/MNIST_database>`__ and define torch ``DataLoader``\\ s (`link <https://pytorch.org/docs/stable/data.html>`__) for train and test."
msgstr "Kita mengambil keuntungan dari `API <https://pytorch.org/vision/stable/datasets.html>`__ ``torchvision`` untuk langsung memuat bagian data dari `himpunan data MNIST <https://en.wikipedia.org/wiki/MNIST_database>`__ dan mendefinisikan torch ``DataLoader`` (`link <https://pytorch.org/docs/stable/data.html>`__) untuk latihan dan uji."

#: ../../tutorials/05_torch_connector.ipynb:1048
msgid "If we perform a quick visualization we can see that the train dataset consists of images of handwritten 0s and 1s."
msgstr "Jika kita melakukan visualisasi secara cepat kita dapat melihat bahwa himpunan data latih berisi gambar tulisan tangan 0 dan 1."

#: ../../tutorials/05_torch_connector.ipynb:1120
msgid "Step 2: Defining the QNN and Hybrid Model"
msgstr "Langkah 2: Mendefinisikan QNN dan Model Hibrida"

#: ../../tutorials/05_torch_connector.ipynb:1131
msgid "This second step shows the power of the ``TorchConnector``. After defining our quantum neural network layer (in this case, a ``TwoLayerQNN``), we can embed it into a layer in our torch ``Module`` by initializing a torch connector as ``TorchConnector(qnn)``."
msgstr "Langkah kedua ini menunjukkan kekuatan ``TorchConnector``. Setelah mendefinisikan layer jaringan saraf tiruan (dalam hal ini ``TwoLayerQNN``), kita melekatkan itu ke layer dalam ``Module`` torch kita dengan menginisialisasi konektor torch sebagai ``TorchConnector(qnn)``."

#: ../../tutorials/05_torch_connector.ipynb:1133
msgid "**⚠️ Attention:** In order to have an adequate gradient backpropagation in hybrid models, we MUST set the initial parameter ``input_gradients`` to TRUE during the qnn initialization."
msgstr "**⚠️ Perhatian:** Supaya memiliki gradien perambatan mundur yang memadai pada model hibrida, kita PERLU mengatur parameter awal ``input_gradients`` ke TRUE saat inisialisasi qnn."

#: ../../tutorials/05_torch_connector.ipynb:1235
msgid "Step 3: Training"
msgstr "Langkah 3: Latihan"

#: ../../tutorials/05_torch_connector.ipynb:1337
msgid "Step 4: Evaluation"
msgstr "Langkah 4: Evaluasi"

#: ../../tutorials/05_torch_connector.ipynb:1440
msgid "🎉🎉🎉🎉 **You are now able to experiment with your own hybrid datasets and architectures using Qiskit Machine Learning.** **Good Luck!**"
msgstr "🎉🎉🎉🎉 **Anda sekarang dapat bereksperimen dengan himpunan data dan arsitektur hibrida sendiri menggunakan Qiskit Machine Learning.** **Semoga berhasil!**"

#: ../../tutorials/index.rst:3
msgid "Machine Learning Tutorials"
msgstr "Tutorial Pembelajaran Mesin"

