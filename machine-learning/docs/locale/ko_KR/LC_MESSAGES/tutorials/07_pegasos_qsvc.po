msgid ""
msgstr ""
"Project-Id-Version: qiskit-docs\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-03-28 08:39+0000\n"
"PO-Revision-Date: 2023-03-28 10:35\n"
"Last-Translator: \n"
"Language: ko\n"
"Language-Team: Korean\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"
"X-Crowdin-Project: qiskit-docs\n"
"X-Crowdin-Project-ID: 369271\n"
"X-Crowdin-Language: ko\n"
"X-Crowdin-File: /master/machine-learning/docs/locale/en/LC_MESSAGES/tutorials/07_pegasos_qsvc.po\n"
"X-Crowdin-File-ID: 9721\n"

#: ../../tutorials/07_pegasos_qsvc.ipynb:9
msgid "This page was generated from `docs/tutorials/07_pegasos_qsvc.ipynb`__."
msgstr "이 페이지는 `docs/tutorials/07_pegasos_qsvc.ipynb`에서 생성되었다."

#: ../../tutorials/07_pegasos_qsvc.ipynb:9
msgid "Pegasos Quantum Support Vector Classifier"
msgstr "Pegasos Quantum Support Vector Classifier"

#: ../../tutorials/07_pegasos_qsvc.ipynb:11
msgid "There's another SVM based algorithm that benefits from the quantum kernel method. Here, we introduce an implementation of a another classification algorithm, which is an alternative version to the ``QSVC`` available in Qiskit Machine Learning and shown in the `\"Quantum Kernel Machine Learning\" <./03_quantum_kernel.ipynb>`__ tutorial. This classification algorithm implements the Pegasos algorithm from the paper \"Pegasos: Primal Estimated sub-GrAdient SOlver for SVM\" by Shalev-Shwartz et al., see: https://home.ttic.edu/~nati/Publications/PegasosMPB.pdf."
msgstr ""

#: ../../tutorials/07_pegasos_qsvc.ipynb:14
msgid "This algorithm is an alternative to the dual optimization from the ``scikit-learn`` package, benefits from the kernel trick, and yields a training complexity that is independent of the size of the training set. Thus, the ``PegasosQSVC`` is expected to train faster than QSVC for sufficiently large training sets."
msgstr "이 알고리즘은 ``scikit-learn`` 패키지에 있는 dual optimization의 대안이고, 커널 트릭의 이점을 가지며 학습 복잡도가 학습 세트의 크기와 무관하다. 따라서 ``PegasosQSVC`` 는 방대한 크기의 학습 세트에 대해 QSVC보다 빠르게 학습할 수 있을 것이다."

#: ../../tutorials/07_pegasos_qsvc.ipynb:16
msgid "The algorithm can be used as direct replacement of ``QSVC`` with some hyper-parameterization."
msgstr "이 알고리즘은 일부 하이퍼 파라미터로 ``QSVC``를 직접 대체하는데 사용될 수 있다."

#: ../../tutorials/07_pegasos_qsvc.ipynb:27
msgid "Let's generate some data:"
msgstr ""

#: ../../tutorials/07_pegasos_qsvc.ipynb:51
msgid "We pre-process the data to ensure compatibility with the rotation encoding and split it into the training and test datasets."
msgstr "회전 인코딩과 호환성을 보장하기 위해 데이터를 전처리하고 이를 학습 및 테스트 데이터 세트로 분할한다."

#: ../../tutorials/07_pegasos_qsvc.ipynb:81
msgid "We have two features in the dataset, so we set a number of qubits to the number of features in the dataset."
msgstr "데이터 세트에는 두 가지 특징이 있으므로 데이터 세트의 특징 수에 따라 큐비트를 설정한다."

#: ../../tutorials/07_pegasos_qsvc.ipynb:83
msgid "Then we set :math:`\\tau` to the number of steps performed during the training procedure. Please note that, there is no early stopping criterion in the algorithm. The algorithm iterates over all :math:`\\tau` steps."
msgstr "그리고 나서 :math:`tau`를 학습 과정에서 수행되는 단계 수로 설정한다. 알고리즘에는 조기 종료 기준이 없다. 알고리즘은 모든 :math:`tau` 단계를 반복한다."

#: ../../tutorials/07_pegasos_qsvc.ipynb:85
msgid "And the last one is the hyperparameter :math:`C`. This is a positive regularization parameter. The strength of the regularization is inversely proportional to :math:`C`. Smaller :math:`C` induce smaller weights which generally helps preventing overfitting. However, due to the nature of this algorithm, some of the computation steps become trivial for larger :math:`C`. Thus, larger :math:`C` improve the performance of the algorithm drastically. If the data is linearly separable in feature space, :math:`C` should be chosen to be large. If the separation is not perfect, :math:`C` should be chosen smaller to prevent overfitting."
msgstr "그리고 마지막은 하이퍼 파라미터 :math:`C`이다. 이것은 양의 정규화 매개변수다. 정규화의 강도는 :math:`C`에 반비례한다. 더 작은 :math:`C`는 일반적으로 과적합을 방지하는데 도움이 되는 작은 가중치를 유도한다. 그러나 이 알고리즘의 특성으로 인해 일부 계산 단계는 큰 :math:`C`이 된다. 그러므로 :math:`C`가 클수록 알고리즘의 성능이 크게 향상된다. 만약 데이터가 feature space에서 선형으로 분리할 수 있으면 :math:`C`를 큰 값으로 선택해야 한다. 그렇지 않다면, 과적합을 방지하기 위해 :math:`C`를 더 작게 선택해야 한다."

#: ../../tutorials/07_pegasos_qsvc.ipynb:114
msgid "The algorithm will run using:"
msgstr "알고리즘은 다음을 사용하여 실행된다:"

#: ../../tutorials/07_pegasos_qsvc.ipynb:116
msgid "The default fidelity instantiated in ``FidelityQuantumKernel``"
msgstr ""

#: ../../tutorials/07_pegasos_qsvc.ipynb:117
msgid "A quantum kernel created from ``ZFeatureMap``"
msgstr "``ZFeatureMap`` 에서 생성된 양자 커널"

#: ../../tutorials/07_pegasos_qsvc.ipynb:148
msgid "The implementation ``PegasosQSVC`` is compatible with the ``scikit-learn`` interfaces and has a pretty standard way of training a model. In the constructor we pass parameters of the algorithm, in this case there are a regularization hyper-parameter :math:`C` and a number of steps."
msgstr "``PegasosQSVC`` 구현은 ``scikit-learn`` 인터페이스와 호환되며 매우 표준적인 모델 훈련 방식을 가지고 있다. 생성자에서 알고리즘의 매개변수를 전달하는데, 이 경우 정규화 하이퍼-파라미터 :math:`C` 와 여러 단계가 있다."

#: ../../tutorials/07_pegasos_qsvc.ipynb:150
msgid "Then we pass training features and labels to the ``fit`` method, which trains a models and returns a fitted classifier."
msgstr "그런 다음 모델을 훈련하고 적합한 분류기를 반환하는 ``fit`` 메서드에 훈련 데이터의 특징과 레이블을 전달한다."

#: ../../tutorials/07_pegasos_qsvc.ipynb:152
msgid "Afterwards, we score our model using test features and labels."
msgstr "그런 다음 테스트 데이터의 특징과 레이블을 사용하여 모델에 점수를 매긴다."

#: ../../tutorials/07_pegasos_qsvc.ipynb:206
msgid "For visualization purposes we create a mesh grid of a predefined step that spans our minimum and maximum values we applied in MinMaxScaler. We also add some margin to the grid for better representation of the training and test samples."
msgstr "시각화를 위해 MinMaxScaler에 적용한 최소값과 최대값을 포괄하는 사전 정의된 단계의 격자점을 만든다. 또한 훈련 및 테스트 샘플의 더 나은 표현을 위해 격자에 약간의 마진(margin)을 추가한다."

#: ../../tutorials/07_pegasos_qsvc.ipynb:231
msgid "We convert the grid to the shape compatible with the model, the shape should be ``(n_samples, n_features)``. Then for each grid point we predict a label. In our case predicted labels will be used for coloring the grid."
msgstr "격자를 모델과 호환되는 모양으로 변환하며 모양은 ``(n_samples, n_features)`` 이어야 한다. 그런 다음 각 격자점에 대해 레이블을 예측한다. 예측된 레이블은 격자점의 색상을 지정하는데 사용된다."

#: ../../tutorials/07_pegasos_qsvc.ipynb:253
msgid "Finally, we plot our grid according to the labels/colors we obtained from the model. We also plot training and test samples."
msgstr "마지막으로 모델에서 얻은 레이블/색상에 따라 격자를 그린다. 훈련 및 테스트 샘플들도 그림에 표시한다."

