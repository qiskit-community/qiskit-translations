msgid ""
msgstr ""
"Project-Id-Version: qiskit-docs\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-02-07 03:44+0000\n"
"PO-Revision-Date: 2023-02-07 05:16\n"
"Last-Translator: \n"
"Language: ko\n"
"Language-Team: Korean\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.11.0\n"
"X-Crowdin-Project: qiskit-docs\n"
"X-Crowdin-Project-ID: 369271\n"
"X-Crowdin-Language: ko\n"
"X-Crowdin-File: /master/machine-learning/docs/locale/en/LC_MESSAGES/tutorials/10_effective_dimension.po\n"
"X-Crowdin-File-ID: 9730\n"

#: ../../tutorials/10_effective_dimension.ipynb:9
msgid "This page was generated from `docs/tutorials/10_effective_dimension.ipynb`__."
msgstr "이 페이지는  `docs/tutorials/10_effective_dimension.ipynb`__ 에서 생성되었다."

#: ../../tutorials/10_effective_dimension.ipynb:9
msgid "Effective Dimension of Qiskit Neural Networks"
msgstr "Qiskit Neural Networks의 유효 차원"

#: ../../tutorials/10_effective_dimension.ipynb:11
msgid "In this tutorial, we will take advantage of the ``EffectiveDimension`` and ``LocalEffectiveDimension`` classes to evaluate the power of Quantum Neural Network models. These are metrics based on information geometry that connect to notions such as trainability, expressibility or ability to generalize."
msgstr "이 튜토리얼에서는 ``EffectiveDimension`` 및 ``LocalEffectiveDimension`` 클래스의 이점을 활용하여 양자 신경망 모델의 성능을 평가할 것이다. 이 것들은 학습 가능성, 표현 가능성 또는 일반화 능력과 같은 개념과 연결되는 정보 기하학을 기반으로 하는 메트릭이다."

#: ../../tutorials/10_effective_dimension.ipynb:13
msgid "Before diving into the code example, we will briefly explain what is the difference between these two metrics, and why are they relevant to the study of Quantum Neural Networks. More information about global effective dimension can be found in `this paper <https://arxiv.org/pdf/2011.00027.pdf>`__, while the local effective dimension was introduced in a `later work <https://arxiv.org/abs/2112.04807>`__."
msgstr "코드 예제로 들어가기 전, 이 두 측정항목의 차이점과 양자 신경망 연구와 관련된 이유를 간략하게 살펴보자. 글로벌 유효 차원에 대한 자세한 내용은 `본 논문 <https://arxiv.org/pdf/2011.00027.pdf>`__ 에서 확인할 수 있으며, 로컬 유효 차원은 `심화 읽기 자료 <https://arxiv. org/abs/2112.04807>`__ 에서 확인 할 수 있다."

#: ../../tutorials/10_effective_dimension.ipynb:25
msgid "1. Global vs. Local Effective Dimension"
msgstr "1. 글로벌 대 로컬 유효 차원"

#: ../../tutorials/10_effective_dimension.ipynb:27
msgid "Both classical and quantum machine learning models share a common goal: being good at **generalizing**, i.e. learning insights from data and applying them on unseen data."
msgstr "클래식 및 양자 머신 러닝 모델은 모두 **일반화** 를 잘 하는 것을 목표로 한다. 즉, 기존의 데이터에서 통찰력을 학습하고 이를 새 데이터에 적용하는 것이다."

#: ../../tutorials/10_effective_dimension.ipynb:29
msgid "Finding a good metric to assess this ability is a non-trivial matter. In `The Power of Quantum Neural Networks <https://arxiv.org/pdf/2011.00027.pdf>`__, the authors introduce the **global** effective dimension as a useful indicator of how well a particular model will be able to perform on new data. In `Effective Dimension of Machine Learning Models <https://arxiv.org/pdf/2112.04807.pdf>`__, the **local** effective dimension is proposed as a new capacity measure that bounds the generalization error of machine learning models."
msgstr "이 능력을 평가하기 위한 좋은 지표를 찾는 것은 중요한 문제이다. `The Power of Quantum Neural Networks <https://arxiv.org/pdf/2011.00027.pdf>`__ 에서 저자는 **글로벌(global)** 유효 차원을 새로운 데이터에 대해 특정 모델이 얼마나 잘 수행할 수 있는지에 대한 유용한 지표로 소개한다.   `Effective Dimension of Machine Learning Models <https://arxiv.org/pdf/2112.04807.pdf>`__ 에서는 기계 학습 모델의 일반화 오차를 제한하는 새로운 용량 척도로 **로컬(local)** 유효 차원이 제안되었다."

#: ../../tutorials/10_effective_dimension.ipynb:32
msgid "The key difference between global (``EffectiveDimension`` class) and **local** effective dimension (``LocalEffectiveDimension`` class) is actually not in the way they are computed, but in the nature of the parameter space that is analyzed. The global effective dimension incorporates the **full parameter space** of the model, and is calculated from a **large number of parameter (weight) sets**. On the other hand, the local effective dimension focuses on how well the **trained** model can generalize to new data, and how **expressive** it can be. Therefore, the local effective dimension is calculated from **a single** set of weight samples (training result). This difference is small in terms of practical implementation, but quite relevant at a conceptual level."
msgstr "글로벌 (``EffectiveDimension`` 클래스)과 **로컬** 유효 차원(``LocalEffectiveDimension`` 클래스)의 주요 차이점은 실제로 계산되는 방식이 아니라 분석되는 매개변수 공간의 특성에 있다 . 글로벌 유효 차원은 모델의 **전체 매개변수 공간** 을 통합하며 **많은 수의 매개변수(가중치) 세트** 에서 된다. 반면에 로컬 유효 차원은 **훈련된** 모델이 새 데이터에 얼마나 잘 일반화될 수 있는지, 그리고 얼마나 **표현적** 일 수 있는지에 중점을 둔다. 따라서 로컬 유효 차원은 **단일** 가중치 샘플 세트(훈련 결과)에서 계산된다. 이 차이는 실제 구현 측면에서는 차이는 크지 않지만 개념적 수준에서는 상당한 의미를 지닌다. "

#: ../../tutorials/10_effective_dimension.ipynb:45
msgid "2. The Effective Dimension Algorithm"
msgstr "2. 효과적인 차원 알고리즘"

#: ../../tutorials/10_effective_dimension.ipynb:47
msgid "Both the global and local effective dimension algorithms use the Fisher Information matrix to provide a measure of complexity. The details on how this matrix is calculated are provided in the `reference paper <https://arxiv.org/pdf/2011.00027.pdf>`__, but in general terms, this matrix captures how sensitive a neural network’s output is to changes in the network’s parameter space."
msgstr "글로벌 및 로컬 유효 차원 알고리즘은 피셔 정보 (Fisher Information) 행렬을 사용하여 복잡성의 척도를 제공한다. 이 행렬이 계산되는 방법에 대한 자세한 내용은 `reference paper <https://arxiv.org/pdf/2011.00027.pdf>`__ 에 제공되지만, 일반적으로 이 행렬은 신경망의 출력이 네트워크 매개 변수 공간의 변화에 얼마나 민감한지를 보여준다."

#: ../../tutorials/10_effective_dimension.ipynb:49
msgid "In particular, this algorithm follows 4 main steps:"
msgstr "특히, 이 알고리즘은 4가지 주요 단계로 구성되어 있다."

#: ../../tutorials/10_effective_dimension.ipynb:51
msgid "**Monte Carlo simulation:** the forward and backward passes (gradients) of the neural network are computed for each pair of input and weight samples."
msgstr "**Monte Carlo simulation:** 신경망의 전방 및 후방 패스(기울기) 는 입력 및 가중치 샘플의 각 쌍에 대해 계산된다."

#: ../../tutorials/10_effective_dimension.ipynb:52
msgid "**Fisher Matrix Computation:** these outputs and gradients are used to compute the Fisher Information Matrix."
msgstr "**Fisher Matrix Computation:** 이 출력 과 그래디언트들은 피셔 정보 행렬을 계산하는 데 사용된다."

#: ../../tutorials/10_effective_dimension.ipynb:53
msgid "**Fisher Matrix Normalization:** averaging over all input samples and dividing by the matrix trace"
msgstr "**Fisher Matrix Normalization:** 모든 입력 샘플에 대한 평균을 구하고 매트릭스 트레이스로 나눈다."

#: ../../tutorials/10_effective_dimension.ipynb:54
msgid "**Effective Dimension Calculation:** according to the formula from `Abbas et al. <https://arxiv.org/pdf/2011.00027.pdf>`__"
msgstr "**Effective Dimension Calculation:** `Abbas et al. <https://arxiv.org/pdf/2011.00027.pdf>`__ 공식을 따른다."

#: ../../tutorials/10_effective_dimension.ipynb:66
msgid "3. Basic Example (SamplerQNN)"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:68
msgid "This example shows how to set up a QNN model problem and run the global effective dimension algorithm. Both Qiskit ``SamplerQNN`` (shown in this example) and ``EstimatorQNN`` (shown in a later example) can be used with the ``EffectiveDimension`` class."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:70
msgid "We start off from the required imports and a fixed seed for the random number generator for reproducibility purposes."
msgstr "필요한 라이브러리들을 불러옴과 동시에 재현성을 위해 난수 생성기를 위한 고정 시드를 생성한다."

#: ../../tutorials/10_effective_dimension.ipynb:108
msgid "3.1 Define QNN"
msgstr "3.1 QNN 정의"

#: ../../tutorials/10_effective_dimension.ipynb:110
msgid "The first step to create a ``SamplerQNN`` is to define a parametrized feature map and ansatz. In this toy example, we will use 3 qubits, and we will define the circuit used in the ``SamplerQNN`` class."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:150
msgid "The parametrized circuit can then be sent together with an optional interpret map (parity in this case) to the ``SamplerQNN`` constructor."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:195
msgid "3.2 Set up Effective Dimension calculation"
msgstr "3.2 유효 차원 계산 설정"

#: ../../tutorials/10_effective_dimension.ipynb:197
msgid "In order to compute the effective dimension of our QNN using the ``EffectiveDimension`` class, we need a series of sets of input samples and weights, as well as the total number of data samples available in a dataset. The ``input_samples`` and ``weight_samples`` are set in the class constructor, while the number of data samples is given during the call to the effective dimension computation, to be able to test and compare how this measure changes with different dataset sizes."
msgstr "``EffectiveDimension`` 클래스를 사용해서 QNN의 유효 차원을 계산하려면, 데이터 세트에서 사용할 수 있는 전체 데이터 샘플 수를 비롯한 일련의 입력 샘플 및 가중치 세트가 필요하다. ``input_samples`` 과 ``weight_samples`` 는 클래스 생성자에서 설정하고, 데이터 샘플의 수는 유효 차원을 계산하는 호출에서 얻는다. 이로써 데이터 세트의 크기가 다를 경우 이러한 측정값이 어떻게 바뀌는지 테스트하고 비교할 수 있다."

#: ../../tutorials/10_effective_dimension.ipynb:208
msgid "We can define the number of input samples and weight samples and the class will randomly sample a corresponding array from a normal (for ``input_samples``) or a uniform (for ``weight_samples``) distribution. Instead of passing a number of samples we can pass an array, sampled manually."
msgstr "사용자는 입력 샘플과 가중치 샘플의 수를 정의할 수 있으며 클래스는 정규(``input_samples`` 의 경우) 또는 균일(``weight_samples`` 의 경우) 분포에서 해당 배열을 무작위로 샘플링한다. 샘플의 갯수를 전달하는 대신 수동으로 샘플링된 배열을 전달할 수 있다."

#: ../../tutorials/10_effective_dimension.ipynb:235
msgid "If we want to test a specific set of input samples and weight samples, we can provide it directly to the ``EffectiveDimension`` class as shown in the following snippet:"
msgstr "특정 세트의 입력 샘플과 가중치 샘플을 테스트하려는 경우 다음 스니펫과 같이 ``EffectiveDimension`` 클래스에 직접 전달할 수 있다."

#: ../../tutorials/10_effective_dimension.ipynb:260
msgid "The effective dimension algorithm also requires a dataset size. In this example, we will define an array of sizes to later see how this input affects the result."
msgstr "유효 차원 알고리즘을 위해 데이터 세트 크기도 제공해야 한다. 이 예에서는 나중에 이 입력이 결과에 어떤 영향을 미치는지 확인하기 위해 크기 배열을 설정한다."

#: ../../tutorials/10_effective_dimension.ipynb:283
msgid "3.3 Compute Global Effective Dimension"
msgstr "3.3 글로벌 유효 차원 계산 "

#: ../../tutorials/10_effective_dimension.ipynb:285
msgid "Let’s now calculate the effective dimension of our network for the previously defined set of input samples, weights, and a dataset size of 5000."
msgstr "이제, 사전 정의된 입력 샘플 세트, 가중치 및 5000개의 데이터 세트 크기에 대해 네트워크의 유효 차원을 계산해 보자."

#: ../../tutorials/10_effective_dimension.ipynb:306
msgid "The effective dimension values will range between 0 and ``d``, where ``d`` represents the dimension of the model, and it’s practically obtained from the number of weights of the QNN. By dividing the result by ``d``, we can obtain the normalized effective dimension, which correlates directly with the capacity of the model."
msgstr "유효 차원 값의 범위는 0부터 ``d`` 까지이다. 여기서 ``d`` 는 모델의 차원을 나타내며 실제로 QNN의 가중치 수에서 얻는다. 결과를 ``d`` 로 나누면 정규화된 유효 차원을 얻을 수 있으며 이 값은 모델의 용량(capacity)과 직접적인 상관관계가 있다."

#: ../../tutorials/10_effective_dimension.ipynb:359
msgid "By calling the ``EffectiveDimension`` class with an array if input sizes ``n``, we can monitor how the effective dimension changes with the dataset size."
msgstr "입력 크기가 ``n`` 인 경우 배열로 ``EffectiveDimension`` 클래스를 호출하면 데이터 세트 크기에 따라 유효 차원이 어떻게 변하는지 모니터링할 수 있다."

#: ../../tutorials/10_effective_dimension.ipynb:444
msgid "4. Local Effective Dimension Example"
msgstr "4. 로컬 유효 차원 예제"

#: ../../tutorials/10_effective_dimension.ipynb:446
msgid "As explained in the introduction, the local effective dimension algorithm only uses **one** set of weights, and it can be used to monitor how training affects the expressiveness of a neural network. The ``LocalEffectiveDimension`` class enforces this constraint to ensure that these calculations are conceptually separate, but the rest of the implementation is shared with ``EffectiveDimension``."
msgstr "앞에서 살펴본 것과 같이, 로컬 유효 차원 알고리즘은 **하나의** 가중치 집합만 사용하며 훈련이 신경망의 표현력에 미치는 영향을 모니터링하는 데 사용할 수 있다. ``LocalEffectiveDimension`` 클래스가 제약 조건을 적용함으로서 계산의 중요한 기능을 구현하는 동시에 계산의 나머지 부분은 ``EffectiveDimension`` 과 공유된다."

#: ../../tutorials/10_effective_dimension.ipynb:448
msgid "This example shows how to leverage the ``LocalEffectiveDimension`` class to analyze the effect of training on QNN expressiveness."
msgstr "이 예제는 학습이 QNN 표현력에 미치는 영향을 분석하기 위해 ``LocalEffectiveDimension`` 클래스를 활용하는 방법을 보여준다."

#: ../../tutorials/10_effective_dimension.ipynb:460
msgid "4.1 Define Dataset and QNN"
msgstr "4.1 데이터셋 및 QNN 정의"

#: ../../tutorials/10_effective_dimension.ipynb:462
msgid "We start by creating a 3D binary classification dataset using ``make_classification`` function from scikit-learn."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:495
msgid "The next step is to create a QNN, an instance of ``EstimatorQNN`` in our case in the same fashion we created an instance of ``SamplerQNN``."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:519
msgid "4.2 Train QNN"
msgstr "4.2 QNN 학습"

#: ../../tutorials/10_effective_dimension.ipynb:521
msgid "We can now proceed to train the QNN. The training step may take some time, be patient. You can pass a callback to the classifier to observe how the training process is going on. We fix ``initial_point`` for reproducibility purposes as usual."
msgstr "이제 QNN 학습을 시작해보자. 학습 단계는 다소 시간이 걸릴 수 있다. 학습 과정이 어떻게 진행되고 있는지 살펴보기 위해 분류기에 콜백을 전달할 수도 있다. 다른 예제에서와 같이 재현성을 위해 initial_point 를 고정한다."

#: ../../tutorials/10_effective_dimension.ipynb:594
msgid "The classifier can now differentiate between classes with an accuracy of:"
msgstr "만들어진 분류기는 아래와 같은 정확도로 클래스를 구별할 수 있다:"

#: ../../tutorials/10_effective_dimension.ipynb:642
msgid "4.3 Compute Local Effective Dimension of trained QNN"
msgstr "4.3 학습된 QNN의 로컬 유효 차원 계산"

#: ../../tutorials/10_effective_dimension.ipynb:644
msgid "Now that we have trained our network, let’s evaluate the local effective dimension based on the trained weights. To do that we access the trained weights directly from the classifier."
msgstr "네트워크 학습이 마무리 되었으므로, 학습된 가중치를 기반으로 로컬 유효 차원을 평가해 보자. 이를 위해 분류기에서 직접 학습된 가중치에 액세스한다."

#: ../../tutorials/10_effective_dimension.ipynb:705
msgid "4.4 Compute Local Effective Dimension of untrained QNN"
msgstr "4.4 학습되지 않은 QNN의 로컬 유효 차원 계산"

#: ../../tutorials/10_effective_dimension.ipynb:707
msgid "We can compare this result with the effective dimension of the untrained network, using the ``initial_point`` as our weight sample:"
msgstr "가중치 샘플로 ``initial_point`` 를 사용하여 이 결과를 훈련되지 않은 네트워크의 유효 차원과 비교할 수 있다."

#: ../../tutorials/10_effective_dimension.ipynb:766
msgid "4.5 Plot and analyze results"
msgstr "4.5 결과를 도식화 하고(Plot) 분석하기"

#: ../../tutorials/10_effective_dimension.ipynb:768
msgid "If we plot the effective dimension values before and after training, we can see the following result:"
msgstr "학습 전과 후의 유효 차원 값을 플롯하면 아래와 같은 결과를 볼 수 있다:"

#: ../../tutorials/10_effective_dimension.ipynb:806
msgid "In general, we should expect the value of the local effective dimension to decrease after training. This can be understood by looking back into the main goal of machine learning, which is to pick a model that is expressive enough to fit your data, but not too expressive that it overfits and performs badly on new data samples."
msgstr "일반적으로 훈련 후에 로컬 유효 차원의 값이 감소할 것으로 예상할 수 있다. 이는 사용된 데이터에 적합할 만큼 표현력이 뛰어나면서도 과적합되어 새로운 데이터 샘플에 대해 성능이 나쁘지 않은 적절한 모델을 선택해야 하는 것과 같은, 머신 러닝의 중요한 목표와도 맥락을 같이 한다."

#: ../../tutorials/10_effective_dimension.ipynb:808
msgid "Certain optimizers help regularize the overfitting of a model by learning parameters, and this action of learning inherently reduces a model’s expressiveness, as measured by the local effective dimension. Following this logic, a randomly initialized parameter set will most likely produce a higher effective dimension that the final set of trained weights, because that model with that particular parameterization is “using more parameters” unnecessarily to fit the data. After training (with the implicit regularization), a trained model will not need to use so many parameters and thus have more “inactive parameters” and a lower effective dimension."
msgstr "특정 옵티마이저는 매개변수를 학습하여 모델의 과적합을 조절하는데 도움을 주기도 하며 이 과정은 본질적으로 로컬 유효차원으로 측정할 수 있는 모덿의 표현력을 감소시킨다. 같은 논리로 무작위로 초기화된 매개 변수 세트는 훈련된 가중치의 최종 학습 세트보다 더 높은 유효차원을 가질 확률이 높으며, 이는 이 무작위 변수 세트를 지닌 모델이 데이터를 모두 맞추기 위해 불필요하게 ”더 많은 매개변수”를 사용하게 되기 때문이다. 학습이 종료 후(어느정도 조정이 이루어진) 학습된 모델은 더 적은 수의 매개변수를 사용해도 충분하기 때문에  더 많은 ”비활성 매개변수”와 더 낮은 유효 차원을 갖게 된다."

#: ../../tutorials/10_effective_dimension.ipynb:811
msgid "We must keep in mind though that this is the general intuition, and there might be cases where a randomly selected set of weights happens to provide a lower effective dimension than the trained weights for a specific model."
msgstr "이것은 그저 보편적인 경우에 대한 통찰일 뿐이며 무작위로 선택된 가중치 세트가 특정 모델에 대해 훈련된 가중치보다 낮은 유효 차원을 제공하는 경우도 있음을 유념한다."

