msgid ""
msgstr ""
"Project-Id-Version: qiskit-docs\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-08-11 21:47+0000\n"
"PO-Revision-Date: 2022-08-17 09:11\n"
"Last-Translator: \n"
"Language-Team: Korean\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.10.3\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"X-Crowdin-Project: qiskit-docs\n"
"X-Crowdin-Project-ID: 369271\n"
"X-Crowdin-Language: ko\n"
"X-Crowdin-File: /master/machine-learning/docs/locale/en/LC_MESSAGES/tutorials/09_saving_and_loading models.po\n"
"X-Crowdin-File-ID: 9728\n"
"Language: ko_KR\n"

#: ../../tutorials/09_saving_and_loading models.ipynb:9
msgid "This page was generated from `docs/tutorials/09_saving_and_loading models.ipynb`__."
msgstr "이 페이지는 `docs/tutorials/09_saving_and_loading models.ipynb`__ 에서 생성되었다."

#: ../../tutorials/09_saving_and_loading models.ipynb:9
msgid "Saving, Loading Qiskit Machine Learning Models and Continuous Training"
msgstr "저장, Qiskit 머신 러닝 모델 로딩 및 연속 학습"

#: ../../tutorials/09_saving_and_loading models.ipynb:11
msgid "In this tutorial we will show how to save and load Qiskit machine learning models. Ability to save a model is very important, especially when a significant amount of time is invested in training a model on a real hardware. Also, we will show how to resume training of the previously saved model."
msgstr "이 튜토리얼에서는 Qiskit 머신 러닝 모델을 저장하고 로드하는 방법을 다룬다. 모델을 저장하는 기능은 특히 실제 하드웨어에서 모델을 학습하는 데에 많은 시간을 소모되는 경우 매우 중요합니다. 또한 이전에 저장한 학습 모델을 재 사용하는 방법을 보여준다."

#: ../../tutorials/09_saving_and_loading models.ipynb:13
msgid "In this tutorial we will cover how to:"
msgstr "이 사용 지침서에서는 다음을 수행하는 방법에 대해 설명한다:"

#: ../../tutorials/09_saving_and_loading models.ipynb:15
msgid "Generate a simple dataset, split it into training/test datasets and plot them"
msgstr "간단한 데이터셋을 생성하고 학습/테스트 데이터셋으로 분할하고 그래프를 그려본다."

#: ../../tutorials/09_saving_and_loading models.ipynb:16
msgid "Train and save a model"
msgstr "모델 훈련후 저장"

#: ../../tutorials/09_saving_and_loading models.ipynb:17
msgid "Load a saved model and resume training"
msgstr "저장된 모델 로드 및 학습 재개"

#: ../../tutorials/09_saving_and_loading models.ipynb:18
msgid "Evaluate performance of models"
msgstr "모델 성능 평가"

#: ../../tutorials/09_saving_and_loading models.ipynb:19
msgid "PyTorch hybrid models"
msgstr "PyTorch 하이브리드 모델"

#: ../../tutorials/09_saving_and_loading models.ipynb:30
msgid "First off, we start from the required imports. We’ll heavily use SciKit-Learn on the data preparation step. In the next cell we also fix a random seed for reproducibility purposes."
msgstr "먼저 필수적인 라이브러리들을 불러들인다. 데이터 준비단계에서는 SciKit-Learn을 아주 많이 사용할 것이다. 다음 셀에서는 결과를 재현할 수 있도록 랜덤 시드값을 고정한다."

#: ../../tutorials/09_saving_and_loading models.ipynb:64
msgid "We will be using two quantum simulators. We’ll start training on the QASM simulator then will resume training on the statevector simulator. The approach shown in this tutorial can be used to train a model on a real hardware available on the cloud and then re-use the model for inference on a local simulator."
msgstr "두 개의 양자 시뮬레이터를 사용한다. QASM simulator를 먼저 학습에 사용한 후 statevector simulator를 사용해 학습을 재개한다. 이 튜토리얼의 접근 방식을 사용하여 클라우드에서 사용 가능한 실제 하드웨어에서 모델을 학습한 다음 로컬 시뮬레이터에서 추론을 위해 모델을 재사용한다."

#: ../../tutorials/09_saving_and_loading models.ipynb:97
msgid "1. Prepare a dataset"
msgstr "1. 데이터셋 준비"

#: ../../tutorials/09_saving_and_loading models.ipynb:99
msgid "Next step is to prepare a dataset. Here, we generate some data in the same way as in other tutorials. The difference is that we apply some transformations to the generated data. We generates ``40`` samples, each sample has ``2`` features, so our features is an array of shape ``(40, 2)``. Labels are obtained by summing up features by columns and if the sum is more than ``1`` then this sample is labeled as ``1`` and ``0`` otherwise."
msgstr "다음 단계는 데이터셋을 준비하는 것이다. 여기서는 다른 튜토리얼과 동일한 방법으로 일부 데이터를 생성한다. 차이점은 생성된 데이터에 일부 변환을 적용한다는 것이다. ``40`` 개의 샘플을 생성하고, 각 샘플은 ``2`` 개의 특징을 가지고 있으므로 특징은 크기가 ``(40, 2)`` 인 배열이다. 레이블은 열별로 특징들을 합산하여 얻으며 합계가 ``1`` 보다 크면 이 샘플은 ``1`` 그렇지 않으면 ``0`` 으로 표기한다."

#: ../../tutorials/09_saving_and_loading models.ipynb:123
msgid "Then, we scale down our features into a range of ``[0, 1]`` by applying ``MinMaxScaler`` from SciKit-Learn. Model training convergence is better when this transformation is applied."
msgstr "그런 다음 SciKit-Learn의 ``MinMaxScaler`` 를 적용하여 기능을 ``[0, 1]`` 범위로 축소한다. 이러한 변환을 적용하면 모델을 학습시킬 때 더 잘 수렴한다."

#: ../../tutorials/09_saving_and_loading models.ipynb:170
msgid "Let’s take a look at the features of the first ``5`` samples of our dataset after the transformation."
msgstr "변환 후 우리 데이터셋의 첫 번째 ``5`` 견본의 특징을 살펴보자."

#: ../../tutorials/09_saving_and_loading models.ipynb:228
msgid "We choose ``VQC`` or Variational Quantum Classifier as a model we will train. This model, by default, takes one-hot encoded labels, so we have to transform the labels that are in the set of ``{0, 1}`` into one-hot representation. We employ SciKit-Learn for this transformation as well. Please note that the input array must be reshaped to ``(num_samples, 1)`` first. The ``OneHotEncoder`` encoder does not work with 1D arrays and our labels is a 1D array. In this case a user must decide either an array has only one feature(our case!) or has one sample. Also, by default the encoder returns sparse arrays, but for dataset plotting it is easier to have dense arrays, so we set ``sparse`` to ``False``."
msgstr "훈련할 모델로는 ``VQC`` 혹은 변분 양자 분류기(Variational Quantum Classifier)를 선택한다. 기본적으로 이 모델은 one-hot 인코딩된 레이블을 사용하므로, ``{0, 1}`` 세트에 있는 레이블을 one-hot 표현법으로 변환해야 한다. 이 변환을 위해서 동일하게 SciKit-Learn를 이용한다. 입력 배열이 ``(num_samples, 1)`` 과 같은 형태로 바뀌게 됨에 주의한다. ``OneHotEncoder`` 인코더는 1D 배열에는 작동하지 않지만, 우리의 레이블은 1D 배열 이므로, 이 경우 사용자는 배열이 우리가 직면한 문제와 같이 하나의 feature 만을 갖는지, 또는 하나의 sample 만을 갖는 것인지 결정해야 한다. 또한 기본적으로 인코더는 희소 배열을 반환하지만, 조밀한 배열을 갖는 것이 데이터 세트를 플롯팅하기에 더 쉽기 때문에,  ``sparse`` 를 ``False`` 로 설정한다."

#: ../../tutorials/09_saving_and_loading models.ipynb:276
msgid "Let’s take a look at the labels of the first ``5`` labels of the dataset. The labels should be one-hot encoded."
msgstr "데이터 셋의 첫 ``5`` 개 레이블을 살펴보도록 하자. 이 레이블은 one-hot 인코딩되어야 한다."

#: ../../tutorials/09_saving_and_loading models.ipynb:334
#, python-format
msgid "Now we split our dataset into two parts: a training dataset and a test one. As a rule of thumb, 80% of a full dataset should go into a training part and 20% into a test one. Our training dataset has ``30`` samples. The test dataset should be used only once, when a model is trained to verify how well the model behaves on unseen data. We employ ``train_test_split`` from SciKit-Learn."
msgstr "이제 데이터 세트를 훈련 데이터 세트와 테스트 데이터 세트의 두 부분으로 나눈다. 일반적으로 전체 데이터 세트의 80%는 훈련 부분에, 20%는 테스트 부분에 할당한다. 우리의 경우 훈련 데이터 세트에는 ``30`` 개의 샘플이 있다. 테스트 데이터 세트는 모델이 보이지 않는 데이터에 대해 모델이 얼마나 잘 작동하는지 확인하도록 훈련될 때 한 번만 사용하도록 한다. SciKit-Learn의 ``train_test_split``을 사용하여 데이터를 나눈다."

#: ../../tutorials/09_saving_and_loading models.ipynb:383
msgid "Now it is time to see how our dataset looks like. Let’s plot it."
msgstr "이제 우리의 데이터 셋이 어떻게 생겼는지 확인할 시간이다. 플롯 해보자."

#: ../../tutorials/09_saving_and_loading models.ipynb:450
msgid "On the plot above we see:"
msgstr "위의 그래프를 보면 다음과 같다:"

#: ../../tutorials/09_saving_and_loading models.ipynb:452
msgid "Solid blue dots are the samples from the training dataset labeled as ``0``"
msgstr "안이 채워진 청색 점은 ``0`` 으로 표시된 훈련 데이터 셋의 샘플이다."

#: ../../tutorials/09_saving_and_loading models.ipynb:453
msgid "Empty blue dots are the samples from the test dataset labeled as ``0``"
msgstr "비어 있는 파란색 점은 ``0`` 으로 레이블 된 테스트 데이터 셋의 샘플이다."

#: ../../tutorials/09_saving_and_loading models.ipynb:454
msgid "Solid green dots are the samples from the training dataset labeled as ``1``"
msgstr "안이 채워진 녹색 점은 ``1`` 로 표시된 훈련 데이터 셋의 샘플이다."

#: ../../tutorials/09_saving_and_loading models.ipynb:455
msgid "Empty green dots are the samples from the test dataset labeled as ``1``"
msgstr "비어 있는 초록색 점은 ``1`` 로 레이블된 테스트 데이터 셋의 샘플이다."

#: ../../tutorials/09_saving_and_loading models.ipynb:457
msgid "We’ll train our model using solid dots and verify it using empty dots."
msgstr "안이 채워져 있는 점들을 이용하여 모델을 훈련하고, 비어있는 점을 사용하여 이를 검증할 것이다."

#: ../../tutorials/09_saving_and_loading models.ipynb:469
msgid "2. Train a model and save it"
msgstr "2. 모델을 훈련 시키고 저장하기"

#: ../../tutorials/09_saving_and_loading models.ipynb:471
msgid "We’ll train our model in two steps. On the first step we train our model in ``20`` iterations."
msgstr "모델은 두가지 단계로 훈련된다. 첫 번째 단계에서는 모델을 ``20`` 회 반복하여 훈련한다."

#: ../../tutorials/09_saving_and_loading models.ipynb:492
msgid "Create an empty array for callback to store values of the objective function."
msgstr "목표 함수의 값을 저장하는 콜백을 위해서 비어 있는 배열을 생성하라."

#: ../../tutorials/09_saving_and_loading models.ipynb:513
msgid "We re-use a callback function from the Neural Network Classifier & Regressor tutorial to plot iteration versus objective function value with some minor tweaks to plot objective values at each step."
msgstr "각 단계에서 목표 값을 플롯하기 위해, Neural Network Classifier & Regressor tutorial의 콜백 함수를 다시 사용하여 약간의 수정을 가한 목표 함수의 값을 iteration에 대해 플롯한다."

#: ../../tutorials/09_saving_and_loading models.ipynb:556
msgid "As mentioned above we train a ``VQC`` model and set ``COBYLA`` as an optimizer with a chosen value of the ``maxiter`` parameter. Then we evaluate performance of the model to see how well it was trained. Then we save this model for a file. On the second step we load this model and will continue to work with it."
msgstr "위에서 언급한 것과 같이 ``VQC`` 모델을 훈련시키고 ``maxiter`` 매개변수를 갖는 ``COBYLA`` 를 최적화 알고리즘으로 설정했다. 그런 다음 모델의 성능을 평가하여 얼마나 잘 훈련되었는지 확인하고, 이 모델을 파일로 저장한다. 두 번째 단계에서는 이 모델을 불러와서 작업을 계속한다."

#: ../../tutorials/09_saving_and_loading models.ipynb:558
msgid "Here, we manually construct an ansatz to fix an initial point where to start optimization from."
msgstr "여기에서,  최적화를 시작할 초기 지점을 조정하기 위해 직접 ansatz를 구성한다."

#: ../../tutorials/09_saving_and_loading models.ipynb:582
msgid "We create a model and set a quantum instance to the QASM simulator we created earlier."
msgstr "우리는 모델을 생성하고 퀀텀 인스턴스를 이전에 작성한 QASM 시뮬레이터로 설정한다."

#: ../../tutorials/09_saving_and_loading models.ipynb:605
msgid "Now it is time to train the model."
msgstr "이제 우리의 모델을 학습해보자."

#: ../../tutorials/09_saving_and_loading models.ipynb:660
msgid "Let’s see how well our model performs after the first step of training."
msgstr "첫 학습 후에 우리의 모델이 얼마나 잘 수행되는지 보도록 하자."

#: ../../tutorials/09_saving_and_loading models.ipynb:709
msgid "Next, we save the model. You may choose any file name you want. Please note that the ``save`` method does not append an extension if it is not specified in the file name."
msgstr "다음으로 모델을 저장한다. 원하는 파일 이름을 선택할 수 있다. 파일 이름이 지정되지 않은 경우 ``save`` 메소드는 확장자를 추가하지 않는다."

#: ../../tutorials/09_saving_and_loading models.ipynb:731
msgid "3. Load a model and continue training"
msgstr "3. 모델을 불러오고 계속 학습하기"

#: ../../tutorials/09_saving_and_loading models.ipynb:733
msgid "To load a model a user have to call a class method ``load`` of the corresponding model class. In our case it is ``VQC``. We pass the same file name we used in the previous section where we saved our model."
msgstr "모델을 불러오려면 사용자가 해당 모델 클래스의 ``load`` 클래스 메소드를 호출해야 한다. 우리의 경우에는 ``VQC`` 이다. 우리는 이전 섹션에서 사용한 것과 같은 파일 이름을 사용하여 모델을 저장했다."

#: ../../tutorials/09_saving_and_loading models.ipynb:754
msgid "Next, we want to alter the model in a way it can be trained further and on another simulator. To do so, we set the ``warm_start`` property. When it is set to ``True`` and ``fit()`` is called again the model uses weights from previous fit to start a new fit. We also set quantum instance of the underlying network to the statevector simulator we created in the beginning of the tutorial. Finally, we create and set a new optimizer with ``maxiter`` is set to ``80``, so the total number of iterations is ``100``."
msgstr "다음으로, 다른 시뮬레이터에서 추가로 학습시킬 수 있도록 모델을 변경한다. 이를 위해 ``warm_start`` 속성을 설정한다. 이 속성을 ``True`` 로 설정하고 ``fit()`` 이 다시 호출되면 모델은 이전 fit 에서의 가중치를 사용해서 새로운 fit을 시작한다. 기본 네트워크의 양자 인스턴스(quantum instance) 를 지침서의 시작부분에서 만든 상태벡터 시뮬레이터로 설정한다. 끝으로 새로운 최적화 알고리즘을 만든다음 ``maxiter`` 는 ``80`` 으로 설정한다. 그러면 전체 반복 수는 ``100`` 이 된다."

#: ../../tutorials/09_saving_and_loading models.ipynb:778
msgid "Now we continue training our model from the state we finished in the previous section."
msgstr "이제 이전 섹션을 완료한 상태에서 모델을 계속 학습한다."

#: ../../tutorials/09_saving_and_loading models.ipynb:871
msgid "Let’s see which data points were misclassified. First, we call ``predict`` to infer predicted values from the training and test features."
msgstr "어떤 데이터 포인트가 잘못 분류되었는지 살펴보자. 우선, 우리는 ``predict`` 를 호출하여 학습 및 테스트 기능으로부터 예측된 값을 추론한다."

#: ../../tutorials/09_saving_and_loading models.ipynb:893
msgid "Plot the whole dataset and the highlight the points that were classified incorrectly."
msgstr "전체 데이터 세트의 그래프를 그리고, 잘못 분류된 점을 강조 표시한다."

#: ../../tutorials/09_saving_and_loading models.ipynb:969
msgid "So, if you have a large dataset or a large model you can train it in multiple steps as shown in this tutorial."
msgstr "따라서 거대한 데이터 세트가 있거나 거대한 모델이 있는 경우, 이 튜토리얼에 표시된 여러 단계를 거쳐 학습할 수 있다."

#: ../../tutorials/09_saving_and_loading models.ipynb:981
msgid "4. PyTorch hybrid models"
msgstr "4. PyTorch 하이브리드 모델"

#: ../../tutorials/09_saving_and_loading models.ipynb:983
msgid "To save and load hybrid models, when using the TorchConnector, follow the PyTorch recommendations of saving and loading the models. For more details please refer to the PyTorch Connector tutorial `here <https://qiskit.org/documentation/machine-learning/tutorials/05_torch_connector.html>`__ where a short snippet shows how to do it."
msgstr "하이브리드 모델을 저장 및 로드하려면 TorchConnector를 사용하여 모델을 저장 및 로드하는 PyTorch 권장사항을 따른다. 자세한 내용은 PyTorch Connector 지침서 `여기 <https://qiskit.org/documentation/machine-learning/tutorials/05_torch_connector.html>`__ 를 참조하시오. 여기서 짧은 스니펫(snippet)을 통해 이를 수행하는 방법을 알 수 있다."

#: ../../tutorials/09_saving_and_loading models.ipynb:985
msgid "Take a look at this pseudo-like code to get the idea:"
msgstr "아이디어를 얻기 위해서 다음의 준 유사 코드를 살펴보자."

