msgid ""
msgstr ""
"Project-Id-Version: qiskit-docs\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-11-28 18:30+0000\n"
"PO-Revision-Date: 2022-12-27 16:01\n"
"Last-Translator: \n"
"Language-Team: Korean\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.11.0\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"X-Crowdin-Project: qiskit-docs\n"
"X-Crowdin-Project-ID: 369271\n"
"X-Crowdin-Language: ko\n"
"X-Crowdin-File: /master/machine-learning/docs/locale/en/LC_MESSAGES/tutorials/01_neural_networks.po\n"
"X-Crowdin-File-ID: 9628\n"
"Language: ko_KR\n"

#: ../../tutorials/01_neural_networks.ipynb:9
msgid "This page was generated from `docs/tutorials/01_neural_networks.ipynb`__."
msgstr "이 페이지는 `docs/tutorials/01_neural_networks.ipynb`__ 에서 생성되었다."

#: ../../tutorials/01_neural_networks.ipynb:9
msgid "Quantum Neural Networks"
msgstr "양자 신경망"

#: ../../tutorials/01_neural_networks.ipynb:11
msgid "This notebook demonstrates the different generic quantum neural network (QNN) implementations provided in Qiskit Machine Learning. The networks are meant as application-agnostic computational units that can be used for many different use cases. Depending on the application, a particular type of network might be more or less suitable and might require to be set up in a particular way. The following different available neural networks will now be discussed in more detail:"
msgstr "이 노트북은 Qiskit Machine Learning에서 제공하는 여러 가지 QNN (일반적인 양자 신경망)을 시연한다. 네트워크는 애플리케이션에 구애받지 않는 컴퓨터적인 단위로써 다양한 예제에 사용될 수 있다 . 애플리케이션에 따라 특정 유형의 네트워크가 더 적합하거나 적합하지 않을 수 있으며,  경우에 따라 네트워크를 특별한 방식으로 설정해야 할 수도 있다. 이제 다음과 같은 다른 사용 가능한 신경망을 자세히 설명한다."

#: ../../tutorials/01_neural_networks.ipynb:13
msgid "``NeuralNetwork``: The interface for neural networks. This is an abstract class."
msgstr "``NeuralNetwork``: 신경망을 위한 인터페이스. 이것은 추상적인 클래스다."

#: ../../tutorials/01_neural_networks.ipynb:14
msgid "``EstimatorQNN``: A network based on the evaluation of quantum mechanical observables."
msgstr "``EstimatorQNN``: 양자역학적 관측 수행을 기반으로 한 네트워크."

#: ../../tutorials/01_neural_networks.ipynb:15
msgid "``SamplerQNN``: A network based on the samples resulting from measuring a quantum circuit."
msgstr "``SamplerQNN``: 양자회로 측정으로 얻은 샘플을 기반으로 한 네트워크."

#: ../../tutorials/01_neural_networks.ipynb:17
msgid "Each implementation, ``EstimatorQNN`` and ``SamplerQNN``, takes in an optional instance of the corresponding Qiskit primitive, namely ``BaseEstimator`` and ``BaseSampler``. The latter two define two interfaces of the primitives. Qiskit provides the reference implementation as well as a backend-based implementation of the primitives. The primitives is a frontend to either a simulator or a real quantum hardware. By default, if no instance is passed to a network, an instance of the Qiskit reference primitive is created automatically by the network. For more information about primitives please refer to `Qiskit primitives <https://qiskit.org/documentation/apidoc/primitives.html>`__."
msgstr ""

#: ../../tutorials/01_neural_networks.ipynb:45
msgid "1. ``NeuralNetwork``"
msgstr "1. ``NeuralNetwork``"

#: ../../tutorials/01_neural_networks.ipynb:47
msgid "The ``NeuralNetwork`` represents the interface for all neural networks available in Qiskit Machine Learning. It exposes a forward and a backward pass taking the data samples and trainable weights as input. A ``NeuralNetwork`` does not contain any training capabilities, these are pushed to the actual algorithms / applications. Thus, a ``NeuralNetwork`` also does not store the values for trainable weights. In the following, different implementations of this interface are introduced."
msgstr "``NeuralNetwork`` 은 Qiskit Machine Learning에서 사용할 수 있는 모든 신경망에 대한 인터페이스를 나타낸다. 신경망은 데이터 샘플과 학습 가능한 가중치를 입력으로하는 정방향 및 역방향 경로를 나타낸다. ``NeuralNetwork`` 에는 훈련 기능이 포함되어 있지 않으며 실제 알고리즘/애플리케이션으로 푸시된다. 이와 같이 ``NeuralNetwork`` 은 학습 가능한 가중치를 저장하지 않는다. 다음은 이 인터페이스의 다양한 구현 예제가 소개한다."

#: ../../tutorials/01_neural_networks.ipynb:49
msgid "Suppose a ``NeuralNetwork`` called ``nn``. Then, the ``nn.forward(input, weights)`` pass takes both flat inputs for the data and weights of size ``nn.num_inputs`` and ``nn.num_weights``, respectively. ``NeuralNetwork`` supports batching of inputs and returns batches of output of the corresponding shape."
msgstr "``NeuralNetwork`` 을 ``nn`` 이라 부르자. 그러면 ``nn.forward(input, weights)`` 경로는 데이터의 입력과 가중치를 ``nn.num_inputs`` 과 ``nn.num_weights`` 로 받는다. ``NeuralNetwork`` 는 입력의 배치처리를 지원하고 해당 형상의 출력 배치를 반환한다."

#: ../../tutorials/01_neural_networks.ipynb:61
msgid "2. ``EstimatorQNN``"
msgstr ""

#: ../../tutorials/01_neural_networks.ipynb:63
msgid "The ``EstimatorQNN`` takes in a parametrized quantum circuit with the combined network’s feature map (input parameters) and ansatz (weight parameters), as well as an optional quantum mechanical observable, and outputs expectation value computations for the forward pass. The quantum circuit parameters can be used to load classical data as well as represent trainable weights. The ``EstimatorQNN`` also allows lists of observables to construct more complex QNNs."
msgstr ""

#: ../../tutorials/01_neural_networks.ipynb:109
msgid "We create an observable manually. If it is set, then The default observable :math:`Z^{\\otimes n}`, where :math:`n` is the number of qubits, is created automatically."
msgstr ""

#: ../../tutorials/01_neural_networks.ipynb:132
msgid "Construct EstimatorQNN with the observable, input parameters, and weight parameters. We don’t set the ``estimator`` parameter, the network will create an instance of the reference ``Estimator`` primitive for us."
msgstr ""

#: ../../tutorials/01_neural_networks.ipynb:323
msgid "Combining multiple observables in a list allows to create more complex QNNs."
msgstr ""

#: ../../tutorials/01_neural_networks.ipynb:430
msgid "4. ``SamplerQNN``"
msgstr ""

#: ../../tutorials/01_neural_networks.ipynb:432
msgid "The ``SamplerQNN`` is based on a (parametrized) ``QuantumCircuit``. This can take input as well as weight parameters and produces samples from the measurement. The samples are interpreted as probabilities of measuring the integer index corresponding to a bitstring. Gradients can be estimated efficiently and the ``SamplerQNN`` provides a backward pass as well."
msgstr ""

#: ../../tutorials/01_neural_networks.ipynb:434
msgid "Further, the ``SamplerQNN`` allows to specify an ``interpret`` function to post-process the samples. This is expected to take a measured integer (from a bitstring) and map it to a new index, i.e. non-negative integer. In this case, the output shape needs to be provided and the probabilities are aggregated accordingly."
msgstr "또한, ``SamplerQNN`` 은 샘플들을 후처리하기 위한 ``interpret`` 함수를 설정할 수 있다. 이것은 비트열에서 측정된 정수를 가져와 새로운 인덱스 (예를 들면, 음이 아닌 정수)에 mapping 하게 된다. 이 경우, 출력 형태가 제공되어야 하며 이에 따라 확률이 집계되어야 한다."

#: ../../tutorials/01_neural_networks.ipynb:436
msgid "If no ``interpret`` function is used, the dimension of the probability vector scales exponentially with the number of qubits. In case of an ``interpret`` function it depends on the expected outcome. If, for instance, an index is mapped to the parity of the corresponding bitstring, i.e., to 0 or 1, a dense output makes sense and the result will be a probability vector of length 2."
msgstr "``interpret`` 함수가 사용되지 않는 경우, 확률 벡터의 차원은 큐비트의 수에 지수적으로 비례한다. ``interpret`` 함수의 경우, 이는 기대되는 출력값에 의존한다. 예를 들어, 만약 index가 해당 비트열의 패리티에 mapping되었을 경우 (예를 들면 0 or 1), 밀집 출력이 알맞으며 그 결과는 길이가 2인 확률 벡터가 될 것이다."

#: ../../tutorials/01_neural_networks.ipynb:478
msgid "As in the previous example, we don’t set the ``sampler`` parameter, the network will create an instance of the reference ``Sampler`` primitive for us."
msgstr ""

