msgid ""
msgstr ""
"Project-Id-Version: qiskit-docs\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-02-07 03:44+0000\n"
"PO-Revision-Date: 2023-02-07 05:09\n"
"Last-Translator: \n"
"Language: ko\n"
"Language-Team: Korean\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.11.0\n"
"X-Crowdin-Project: qiskit-docs\n"
"X-Crowdin-Project-ID: 369271\n"
"X-Crowdin-Language: ko\n"
"X-Crowdin-File: /master/machine-learning/docs/locale/en/LC_MESSAGES/tutorials/02_neural_network_classifier_and_regressor.po\n"
"X-Crowdin-File-ID: 9630\n"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:9
msgid "This page was generated from `docs/tutorials/02_neural_network_classifier_and_regressor.ipynb`__."
msgstr "이 페이지는 `docs/tutorials/02_neural_network_classifier_and_regressor.ipynb`__ 에서 생성되었다."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:9
msgid "Neural Network Classifier & Regressor"
msgstr "신경망 분류기 & 회귀기"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:11
msgid "In this tutorial we show how the ``NeuralNetworkClassifier`` and ``NeuralNetworkRegressor`` are used. Both take as an input a (Quantum) ``NeuralNetwork`` and leverage it in a specific context. In both cases we also provide a pre-configured variant for convenience, the Variational Quantum Classifier (``VQC``) and Variational Quantum Regressor (``VQR``). The tutorial is structured as follows:"
msgstr "이 튜토리얼에서는 ``NeuralNetworkClassifier`` 와 ``NeuralNetworkRegressor`` 가 어떻게 사용되는지를 보여준다. 두 가지 모두 (양자) ``NeuralNetwork`` 을 입력하고 이를 특정한 문맥에서 활용한다. 두 가지 모두 편의상 사전 구성된 Variational Quantum Classifier (``VQC``) 와 Variational Quantum Regressor (``VQR``) 도 제공한다. 이 튜토리얼은 다음과 같이 구성된다."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:13
msgid "`Classification <#Classification>`__"
msgstr "`분류 <#Classification>`__"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:15
msgid "Classification with an ``EstimatorQNN``"
msgstr ""

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:16
#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:380
msgid "Classification with a ``SamplerQNN``"
msgstr ""

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:17
#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:600
msgid "Variational Quantum Classifier (``VQC``)"
msgstr "변분 양자분류기 (``VQC``, Variational Quantum Classifier)"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:19
msgid "`Regression <#Regression>`__"
msgstr "`회귀 <#Regression>`__"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:21
#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:1048
msgid "Regression with an ``EstimatorQNN``"
msgstr ""

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:22
msgid "Variational Quantum Regressor (``VQR``)"
msgstr "변분 양자회귀기 (``VQR``, Variational Quantum Regressor)"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:57
msgid "Classification"
msgstr "분류"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:59
msgid "We prepare a simple classification dataset to illustrate the following algorithms."
msgstr "다음 알고리즘을 설명하기 위해 간단한 분류 데이터 셋을 준비한다."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:104
msgid "Classification with the an ``EstimatorQNN``"
msgstr ""

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:106
msgid "First we show how an ``EstimatorQNN`` can be used for classification within a ``NeuralNetworkClassifier``. In this context, the ``EstimatorQNN`` is expected to return one-dimensional output in :math:`[-1, +1]`. This only works for binary classification and we assign the two classes to :math:`\\{-1, +1\\}`."
msgstr ""

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:142
msgid "Create a quantum neural network"
msgstr ""

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:201
msgid "We will add a callback function called ``callback_graph``. This will be called for each iteration of the optimizer and will be passed two parameters: the current weights and the value of the objective function at those weights. For our function, we append the value of the objective function to an array so we can plot iteration versus objective function value and update the graph with each iteration. However, you can do whatever you want with a callback function as long as it gets the two parameters mentioned passed."
msgstr "``callback_graph`` 라는 콜백 함수를 추가할 것이다. 이 함수는 최적화 단계가 반복될 때마다 호출되며 두 개의 매개변수(현재 가중치와 해당 가중치에 대한 최적화 시키려는 목적 함수의 값)가 전달된다. 목적 함수의 값을 배열에 추가함으로써 반복 횟수 대비 목적 함수의 값을 그래프로 출력하고 반복적으로 해당 그래프를 갱신할 수 있게 한다. 그러나 이 외에도 콜백 함수 및 함수로 전달되는 두 개의 매개변수를 이용해 원하는 동작은 무엇이든 할 수 있다."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:330
msgid "Now, when the model is trained, we can explore the weights of the neural network. Please note, the number of weights is defined by ansatz."
msgstr ""

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:382
msgid "Next we show how a ``SamplerQNN`` can be used for classification within a ``NeuralNetworkClassifier``. In this context, the ``SamplerQNN`` is expected to return :math:`d`-dimensional probability vector as output, where :math:`d` denotes the number of classes. The underlying ``Sampler`` primitive returns quasi-distributions of bit strings and we just need to define a mapping from the measured bitstrings to the different classes. For binary classification we use the parity mapping."
msgstr ""

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:553
msgid "Again, once the model is trained we can take a look at the weights. As we set ``reps=1`` explicitly in our ansatz, we can see less parameters than in the previous model."
msgstr ""

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:602
msgid "The ``VQC`` is a special variant of the ``NeuralNetworkClassifier`` with a ``SamplerQNN``. It applies a parity mapping (or extensions to multiple classes) to map from the bitstring to the classification, which results in a probability vector, which is interpreted as a one-hot encoded result. By default, it applies this the ``CrossEntropyLoss`` function that expects labels given in one-hot encoded format and will return predictions in that format too."
msgstr ""

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:721
msgid "Multiple classes with VQC"
msgstr ""

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:723
msgid "In this section we generate an artificial dataset that contains samples of three classes and show how to train a model to classify this dataset. This example shows how to tackle more interesting problems in machine learning. Of course, for a sake of short training time we prepare a tiny dataset. We employ ``make_classification`` from SciKit-Learn to generate a dataset. There 10 samples in the dataset, 2 features, that means we can still have a nice plot of the dataset, as well as no redundant features, these are features are generated as a combinations of the other features. Also, we have 3 different classes in the dataset, each classes one kind of centroid and we set class separation to ``2.0``, a slight increase from the default value of ``1.0`` to ease the classification problem."
msgstr ""

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:726
msgid "Once the dataset is generated we scale the features into the range ``[0, 1]``."
msgstr ""

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:759
msgid "Let’s see how our dataset looks like."
msgstr ""

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:814
msgid "We also transform labels and make them categorical."
msgstr ""

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:863
msgid "We create an instance of ``VQC`` similar to the previous example, but in this case we pass a minimal set of parameters. Instead of feature map and ansatz we pass just the number of qubits that is equal to the number of features in the dataset, an optimizer with a low number of iteration to reduce training time, a quantum instance, and a callback to observe progress."
msgstr ""

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:888
msgid "Start the training process in the same way as in previous examples."
msgstr ""

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:954
msgid "Despite we had the low number of iterations, we achieved quite a good score. Let see the output of the ``predict`` method and compare the output with the ground truth."
msgstr ""

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:1005
msgid "Regression"
msgstr "회귀"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:1007
msgid "We prepare a simple regression dataset to illustrate the following algorithms."
msgstr "다음 알고리즘을 설명하기 위해 간단한 회귀 데이터셋을 준비한다."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:1050
msgid "Here we restrict to regression with an ``EstimatorQNN`` that returns values in :math:`[-1, +1]`. More complex and also multi-dimensional models could be constructed, also based on ``SamplerQNN`` but that exceeds the scope of this tutorial."
msgstr ""

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:1187
msgid "Similarly to the classification models, we can obtain an array of trained weights by querying a corresponding property of the model. In this model we have only one parameter defined as ``param_y`` above."
msgstr ""

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:1234
msgid "Regression with the Variational Quantum Regressor (``VQR``)"
msgstr "Variational Quantum Regressor (``VQR``) 를 이용한 회귀"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:1236
msgid "Similar to the ``VQC`` for classification, the ``VQR`` is a special variant of the ``NeuralNetworkRegressor`` with a ``EstimatorQNN``. By default it considers the ``L2Loss`` function to minimize the mean squared error between predictions and targets."
msgstr ""

