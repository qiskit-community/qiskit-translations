msgid ""
msgstr ""
"Project-Id-Version: qiskit-docs\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-11-08 23:29+0000\n"
"PO-Revision-Date: 2022-11-09 00:16\n"
"Last-Translator: \n"
"Language-Team: Korean\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.11.0\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"X-Crowdin-Project: qiskit-docs\n"
"X-Crowdin-Project-ID: 369271\n"
"X-Crowdin-Language: ko\n"
"X-Crowdin-File: /master/machine-learning/docs/locale/en/LC_MESSAGES/tutorials/04_torch_qgan.po\n"
"X-Crowdin-File-ID: 9885\n"
"Language: ko_KR\n"

#: ../../tutorials/04_torch_qgan.ipynb:9
msgid "This page was generated from `docs/tutorials/04_torch_qgan.ipynb`__."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:9
msgid "PyTorch qGAN Implementation"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:12
msgid "Overview"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:14
msgid "This tutorial introduces step-by-step how to build a PyTorch-based Quantum Generative Adversarial Network algorithm."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:17
msgid "Context"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:19
msgid "The qGAN [1] is a hybrid quantum-classical algorithm used for generative modeling tasks. The algorithm uses the interplay of a quantum generator :math:`G_{\\theta}`, i.e., an ansatz, and a classical discriminator :math:`D_{\\phi}`, a neural network, to learn the underlying probability distribution given training data."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:21
msgid "The generator and discriminator are trained in alternating optimization steps, where the generator aims at generating samples that will be classified by the discriminator as training data samples (i.e, samples extracted from the real training distribution), and the discriminator tries to differentiate between original training data samples and data samples from the generator (in other words, telling apart the real and generated distributions). The final goal is for the quantum generator to learn a representation for the training data’s underlying probability distribution. The trained quantum generator can, thus, be used to load a quantum state which is an approximate model of the target distribution."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:24
msgid "**References:**"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:26
msgid "[1] Zoufal et al., `Quantum Generative Adversarial Networks for learning and loading random distributions <https://www.nature.com/articles/s41534-019-0223-2>`__"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:29
msgid "Application: qGANs for Loading Random Distributions"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:31
msgid "Given :math:`k`-dimensional data samples, we employ a quantum Generative Adversarial Network (qGAN) to learn the data’s underlying random distribution and to load it directly into a quantum state:"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:33
msgid "\\big| g_{\\theta}\\rangle = \\sum_{j=0}^{2^n-1} \\sqrt{p_{\\theta}^{j}}\\big| j \\rangle\n\n"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:35
msgid "where :math:`p_{\\theta}^{j}` describe the occurrence probabilities of the basis states :math:`\\big| j\\rangle`."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:37
msgid "The aim of the qGAN training is to generate a state :math:`\\big| g_{\\theta}\\rangle` where :math:`p_{\\theta}^{j}`, for :math:`j\\in \\left\\{0, \\ldots, {2^n-1} \\right\\}`, describe a probability distribution that is close to the distribution underlying the training data :math:`X=\\left\\{x^0, \\ldots, x^{k-1} \\right\\}`."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:39
msgid "For further details please refer to `Quantum Generative Adversarial Networks for Learning and Loading Random Distributions <https://arxiv.org/abs/1904.00043>`__ *Zoufal, Lucchi, Woerner* [2019]."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:41
msgid "For an example of how to use a trained qGAN in an application, the pricing of financial derivatives, please see the `Option Pricing with qGANs <https://github.com/Qiskit/qiskit-finance/tree/main/docs/tutorials/10_qgan_option_pricing.ipynb>`__ tutorial."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:44
msgid "Tutorial"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:47
msgid "Data and Representation"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:49
msgid "First, we need to load our training data :math:`X`."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:51
msgid "In this tutorial, the training data is given by samples from a 2D multivariate normal distribution."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:53
msgid "The goal of the generator is to learn how to represent such distribution, and the trained generator should correspond to an :math:`n`-qubit quantum state :nbsphinx-math:`\\begin{equation} |g_{\\text{trained}}\\rangle=\\sum\\limits_{j=0}^{k-1}\\sqrt{p_{j}}|x_{j}\\rangle, \\end{equation}` where the basis states :math:`|x_{j}\\rangle` represent the data items in the training data set :math:`X={x_0, \\ldots, x_{k-1}}` with :math:`k\\leq 2^n` and :math:`p_j` refers to the sampling probability of :math:`|x_{j}\\rangle`."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:57
msgid "To facilitate this representation, we need to map the samples from the multivariate normal distribution to discrete values. The number of values that can be represented depends on the number of qubits used for the mapping. Hence, the data resolution is defined by the number of qubits. If we use :math:`3` qubits to represent one feature, we have :math:`2^3 = 8` discrete values."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:59
msgid "We first begin by fixing seeds in the random number generators, then we will import the libraries and packages we will need for this tutorial."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:84
msgid "Then, we sample some data from a 2D multivariate normal distribution as described above."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:132
msgid "Let’s visualize our distribution and samples. The ``discretize_and_truncate`` function discretized the classical data we got from the multivariate normal so, a plot of the probability density function looks very coarse. But it is still a bell-shaped bivariate normal distribution."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:194
msgid "On the next figure we prove that the training data was discretized. We passed data resolution as an array ``[3, 3]``, This array defined the number of qubits used to represent each data dimension. Thus, we can define :math:`2^3 = 8` discrete values and all training samples should fall under one of these discrete values. On the next figure there is a histogram for each random variable and to see actual discretization we increased the number of bins. Some of the bins are empty and there are 8 non-empty bins for each variable."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:260
msgid "We move to PyTorch modeling and start from converting data arrays into tensors and create a data loader from our training data."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:289
msgid "Backend Configurations"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:291
msgid "Next, we need to choose a backend that is used to run the quantum generator. The presented method is compatible with all shot-based backends (qasm, fake hardware, real hardware) provided by Qiskit."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:293
msgid "First, we create a quantum instance for the training where the batch size defines the number of shots."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:318
msgid "Then we create a quantum instance for the evaluation purposes, we choose a higher number of shots to get better insights."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:340
msgid "Initialize the quantum neural network ansatz"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:342
msgid "Now, we define the parameterized quantum circuit :math:`G\\left(\\boldsymbol{\\theta}\\right)` with :math:`\\boldsymbol{\\theta} = {\\theta_1, ..., \\theta_k}` which will be used in our quantum generator."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:344
msgid "To implement the quantum generator, we choose a depth-\\ :math:`2` ansatz that implements :math:`R_Y` rotations and :math:`CX` gates which takes a uniform distribution as an input state. Notably, for :math:`k>1` the generator’s parameters must be chosen carefully. For example, the circuit depth should be more than :math:`1` because higher circuit depths enable the representation of more complex structures."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:374
msgid "Let’s draw our circuit and see what it looks like."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:405
msgid "Definition of the quantum generator"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:407
msgid "Next, we define a function that creates the quantum generator from a given parameterized quantum circuit. As parameters this function takes a quantum instance to be used for data sampling. We wrap a created quantum neural network in ``TorchConnector`` to make use of PyTorch-based training."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:444
msgid "Definition of the classical discriminator"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:446
msgid "Next, we define a PyTorch-based classical neural network that represents the classical discriminator. The underlying gradients can be automatically computed with PyTorch."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:485
msgid "Definition of the loss functions"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:487
msgid "We want to train the generator and the discriminator with binary cross entropy as loss function:"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:489
msgid "L\\left(\\boldsymbol{\\theta}\\right)=\\sum_jp_j\\left(\\boldsymbol{\\theta}\\right)\\left[y_j\\log(x_j) + (1-y_j)\\log(1-x_j)\\right],\n\n"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:491
msgid "where :math:`x_j` refers to a data sample and :math:`y_j` to the corresponding label."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:517
msgid "Evaluation of custom gradients for the generator BCE loss function"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:519
msgid "The evaluation of custom gradients for the quantum generator requires us to combine quantum gradients :math:`\\frac{\\partial p_j\\left(\\boldsymbol{\\theta}\\right)}{\\partial \\theta_l}` that we compute with Qiskit’s gradient framework with the binary cross entropy as follows:"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:521
msgid "\\frac{\\partial L\\left(\\boldsymbol{\\theta}\\right)}{\\partial \\theta_l} = \\sum_j \\frac{\\partial p_j\\left(\\boldsymbol{\\theta}\\right)}{\\partial \\theta_l} \\left[y_j\\log(x_j) + (1-y_j)\\log(1-x_j)\\right].\n\n"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:523
msgid "First, we need to define how gradients will be evaluated. Depending on the backend, the gradients may be returned in a sparse format. Since PyTorch provides only a limited support for sparse gradients, we need to write a custom function for gradient based training."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:548
msgid "Here, we define the custom function that evaluates gradients of the generator loss function considering the custom gradients of the quantum generator. As the parameters the function takes a list of parameter values and the discriminator as a classical neural network. The function returns a list of gradient values."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:590
msgid "Relative entropy as benchmarking metric"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:592
msgid "The relative entropy describes a distance metric for distributions. Hence, we can use it to benchmark how close/far away the trained distribution is from the target distribution."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:594
msgid "In the next function we computes relative entropy between target and trained distribution."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:624
msgid "Definition of the optimizers"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:626
msgid "In order to train the generator and discriminator, we need to define optimization schemes. In the following, we employ a momentum based optimizer called Adam, see `Kingma et al., Adam: A method for stochastic optimization <https://arxiv.org/abs/1412.6980>`__ for more details."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:662
msgid "Visualization of the training process"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:664
msgid "We will visualize what is happening during the training by plotting the evolution of the generator’s and the discriminator’s loss functions during the training, as well as the progress in the relative entropy between the trained and the target distribution. We define a function that plots the loss functions and relative entropy. We call this function once an epoch of training is complete."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:666
msgid "Visualization of the training process begins when training data is collected across two epochs."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:715
msgid "Training"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:717
msgid "Now, we are ready to train our model. It may take some time to train the model so be patient."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:804
msgid "Results: cumulative distribution functions"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:806
msgid "In the final section we compare the cumulative distribution function (CDF) of the trained distribution to the CDF of the target distribution."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:808
msgid "We create a new generator for sampling that takes more shots and, hence, gives more information. Recall, the sampling quantum instance was created with a larger number of shots. Then, we set the weights of the sampling generator to the values obtained in the training process."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:830
msgid "Next, we wet the generator data samples, corresponding sampling probabilities, and plot the cumulative distribution functions."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:898
msgid "On the plot above, in the blue color the generated distribution is shown and in the orange color the training one. You may find that both CDFs are similar to each other."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:910
msgid "Conclusion"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:912
msgid "Quantum generative adversarial networks employ the interplay of a generator and discriminator to map an approximate representation of a probability distribution underlying given data samples into a quantum channel. This tutorial presents a self-standing PyTorch-based qGAN implementation where the generator is given by a quantum channel, i.e., a variational quantum circuit, and the discriminator by a classical neural network, and discusses the application of efficient learning and loading of generic probability distributions – implicitly given by data samples – into quantum states. Since, this approximate loading requires only :math:`\\mathscr{O}\\left(poly\\left(n\\right)\\right)` gates and can enable the use of potentially advantageous quantum algorithms by offering an efficient data loading scheme."
msgstr ""

