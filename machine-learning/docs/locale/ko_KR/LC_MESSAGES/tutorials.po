msgid ""
msgstr ""
"Project-Id-Version: qiskit-docs\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-07-12 22:21+0000\n"
"PO-Revision-Date: 2021-07-13 04:18\n"
"Last-Translator: \n"
"Language-Team: Korean\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"X-Crowdin-Project: qiskit-docs\n"
"X-Crowdin-Project-ID: 369271\n"
"X-Crowdin-Language: ko\n"
"X-Crowdin-File: /master/machine-learning/docs/locale/en/LC_MESSAGES/tutorials.po\n"
"X-Crowdin-File-ID: 9528\n"
"Language: ko_KR\n"

#: ../../tutorials/01_neural_networks.ipynb:13
#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:13
#: ../../tutorials/03_quantum_kernel.ipynb:13
#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:13
#: ../../tutorials/05_torch_connector.ipynb:13
msgid "Run interactively in jupyter notebook."
msgstr "jupyter notebook에서 대화식으로 실행한다."

#: ../../tutorials/01_neural_networks.ipynb:9
msgid "Quantum Neural Networks"
msgstr "양자 신경망"

#: ../../tutorials/01_neural_networks.ipynb:11
msgid "This notebook demonstrates the different generic quantum neural network (QNN) implementations provided in Qiskit Machine Learning. The networks are meant as application-agnostic computational units that can be used for many different use cases. Depending on the application, a particular type of network might more or less suitable and might require to be set up in a particular way. The following different available neural networks will now be discussed in more detail:"
msgstr "이 노트북은 Qiskit Machine Learning에서 제공하는 여러 가지 QNN (일반적인 양자 신경망)을 시연한다. 네트워크는 애플리케이션에 구애받지 않는 컴퓨터적인 단위로써 다양한 예제에 사용될 수 있다 . 애플리케이션에 따라 특정 유형의 네트워크가 더 적합하거나 적합하지 않을 수 있으며,  경우에 따라 네트워크를 특별한 방식으로 설정해야 할 수도 있다. 이제 다음과 같은 다른 사용 가능한 신경망을 자세히 설명한다."

#: ../../tutorials/01_neural_networks.ipynb:13
msgid "``NeuralNetwork``: The interface for neural networks."
msgstr "``NeuralNetwork`` : 신경망을 위한 인터페이스."

#: ../../tutorials/01_neural_networks.ipynb:14
msgid "``OpflowQNN``: A network based on the evaluation of quantum mechanical observables."
msgstr "``OpflowQNN`` : 양자역학적 관측 수행을 기반으로 한 네트워크."

#: ../../tutorials/01_neural_networks.ipynb:15
msgid "``TwoLayerQNN``: A special ``OpflowQNN`` implementation for convenience."
msgstr "``TwoLayerQNN`` : 편의를 위한 특수한 ``OpflowQNN`` 구현."

#: ../../tutorials/01_neural_networks.ipynb:16
msgid "``CircuitQNN``: A network based on the samples resulting from measuring a quantum circuit."
msgstr "``CircuitQNN`` : 양자회로 측정으로 얻은 샘플을 기반으로 한 네트워크."

#: ../../tutorials/01_neural_networks.ipynb:64
msgid "1. ``NeuralNetwork``"
msgstr "1. ``NeuralNetwork``"

#: ../../tutorials/01_neural_networks.ipynb:66
msgid "The ``NeuralNetwork`` represents the interface for all neural networks available in Qiskit Machine Learning. It exposes a forward and a backward pass taking the data samples and trainable weights as input. A ``NeuralNetwork`` does not contain any training capabilities, these are pushed to the actual algorithms / applications. Thus, a ``NeuralNetwork`` also does not store the values for trainable weights. In the following, different implementations of this interfaces are introduced."
msgstr "``NeuralNetwork`` 은 Qiskit Machine Learning에서 사용할 수 있는 모든 신경망에 대한 인터페이스를 나타낸다. 신경망은 데이터 샘플과 학습 가능한 가중치를 입력으로하는 정방향 및 역방향 경로를 나타낸다. ``NeuralNetwork`` 에는 훈련 기능이 포함되어 있지 않으며 실제 알고리즘/애플리케이션으로 푸시된다. 이와 같이 ``NeuralNetwork`` 은 학습 가능한 가중치를 저장하지 않는다. 다음은 이 인터페이스의 다양한 구현 예제가 소개한다."

#: ../../tutorials/01_neural_networks.ipynb:68
msgid "Suppose a ``NeuralNetwork`` called ``nn``. Then, the ``nn.forward(input, weights)`` pass takes either flat inputs for the data and weights of size ``nn.num_inputs`` and ``nn.num_weights``, respectively. ``NeuralNetwork`` supports batching of inputs and returns batches of output of the corresponding shape."
msgstr "``NeuralNetwork`` 을 ``nn`` 이라 부르자. 그러면 ``nn.forward(input, weights)`` 경로는 데이터의 입력과 가중치를 ``nn.num_inputs`` 과 ``nn.num_weights`` 로 받는다. ``NeuralNetwork`` 는 입력의 배치처리를 지원하고 해당 형상의 출력 배치를 반환한다."

#: ../../tutorials/01_neural_networks.ipynb:80
msgid "2. ``OpflowQNN``"
msgstr "2. ``OpflowQNN``"

#: ../../tutorials/01_neural_networks.ipynb:82
msgid "The ``OpflowQNN`` takes a (parametrized) operator from Qiskit and leverages Qiskit’s gradient framework to provide the backward pass. Such an operator can for instance be an expected value of a quantum mechanical observable with respect to a parametrized quantum state. The Parameters can be used to load classical data as well as represent trainable weights. The ``OpflowQNN`` also allows lists of operators and more complex structures to construct more complex QNNs."
msgstr "``OpflowQNN`` 은 Qiskit의 (매개 변수화된) 연산자를 취하여 Qiskit의 기울기 프레임워크를 활용해 역방향 경로를 제공한다. 예를 들어, 이러한 연산자는 매개 변수화된 양자 상태에 관한 양자 기계적 관찰의 기댓값이 될 수 있다. 매개변수를 사용하여 훈련 가능한 가중치를 표시할 뿐만 아니라 고전적 데이터를 불러오는데 사용할 수 있다. 또한 ``OpflowQNN`` 은 더욱 복잡한 QNN을 구성하기 위한 연산자 목록과 복잡한 구조를 가능하게 한다."

#: ../../tutorials/01_neural_networks.ipynb:321
msgid "Combining multiple observables in a ``ListOp`` also allows to create more complex QNNs"
msgstr "``ListOp`` 에서 여러 관측 자료를 결합하면 보다 복잡한 QNN을 생성할 수 있다."

#: ../../tutorials/01_neural_networks.ipynb:412
msgid "3. ``TwoLayerQNN``"
msgstr "3. ``TwoLayerQNN``"

#: ../../tutorials/01_neural_networks.ipynb:414
msgid "The ``TwoLayerQNN`` is a special ``OpflowQNN`` on :math:`n` qubits that consists of first a feature map to insert data and second an ansatz that is trained. The default observable is :math:`Z^{\\otimes n}`, i.e., parity."
msgstr "``TwoLayerQNN`` 은 :math:`n` 큐비트에 작용하는 특별한 종류의 ``OpflowQNN`` 으로서 두 가지 요소로 구성되어 있다. 첫 번째는 데이터를 삽입하기 위한 feature map이고 두 번째는 훈련된 ansatz이다. 기본 관측값은 :math:`Z^{\\otimes n}`, 즉 parity이다."

#: ../../tutorials/01_neural_networks.ipynb:612
msgid "4. ``CircuitQNN``"
msgstr "4. ``CircuitQNN``"

#: ../../tutorials/01_neural_networks.ipynb:614
msgid "The ``CircuitQNN`` is based on a (parametrized) ``QuantumCircuit``. This can take input as well as weight parameters and produces samples from the measurement. The samples can either be interpreted as probabilities of measuring the integer index corresponding to a bitstring or directly as a batch of binary output. In the case of probabilities, gradients can be estimated efficiently and the ``CircuitQNN`` provides a backward pass as well. In case of samples, differentiation is not possible and the backward pass returns ``(None, None)``."
msgstr "``CircuitQNN`` 은 (파라미터화된) ``QuantumCircuit`` 에 기반으로 한다. 이것은 가중치뿐만 아니라 입력을 취할 수 있고, 측정을 통해 샘플을 생성할 수 있다. 샘플들은 비트열에 대응하는 정수 인덱스를 측정하는 확률 또는 이진 출력의 배치(batch) 로서 직접적으로 해석될 수 있다. 확률의 경우 기울기는 효율적으로 추정될 수 있고, ``CircuitQNN`` 은 역방향 경로 또한 제공된다. 샘플의 경우에는 미분이 가능하지 않으며 역방향 경로는 ``(None, None)`` 을 반환한다."

#: ../../tutorials/01_neural_networks.ipynb:617
msgid "Further, the ``CircuitQNN`` allows to specify an ``interpret`` function to post-process the samples. This is expected to take a measured integer (from a bitstring) and map it to a new index, i.e. non-negative integer. In this case, the output shape needs to be provided and the probabilities are aggregated accordingly."
msgstr "또한, ``CircuitQNN`` 은 샘플들을 후처리하기 위한 ``interpret`` 함수를 설정할 수 있다. 이것은 비트열에서 측정된 정수를 가져와 새로운 인덱스 (예를 들면, 음이 아닌 정수)에 mapping 하게 된다. 이 경우, 출력 형태가 제공되어야 하며 이에 따라 확률이 집계되어야 한다."

#: ../../tutorials/01_neural_networks.ipynb:619
msgid "A ``CircuitQNN`` can be configured to return sparse as well as dense probability vectors. If no ``interpret`` function is used, the dimension of the probability vector scales exponentially with the number of qubits and a sparse recommendation is usually recommended. In case of an ``interpret`` function it depends on the expected outcome. If, for instance, an index is mapped to the parity of the corresponding bitstring, i.e., to 0 or 1, a dense output makes sense and the result will be a probability vector of length 2."
msgstr "``CircuitQNN`` 은 밀집 확률 벡터뿐만 아니라, 희소 확률 벡터를 반환하도록 조정될 수 있다. ``interpret`` 함수가 사용되지 않는 경우, 확률 벡터의 차원은 큐비트의 수에 지수적으로 비례하고, 보통 희소 방식이 권장된다. ``interpret`` 함수의 경우, 이는 기대되는 출력값에 의존한다. 예를 들어, 만약 index가 해당 비트열의 패리티에 mapping되었을 경우 (예를 들면 0 or 1), 밀집 출력이 알맞으며 그 결과는 길이가 2인 확률 벡터가 될 것이다."

#: ../../tutorials/01_neural_networks.ipynb:662
msgid "4.1 Output: sparse integer probabilities"
msgstr "4.1 출력: 희소 정수 확률"

#: ../../tutorials/01_neural_networks.ipynb:761
msgid "4.2 Output: dense parity probabilities"
msgstr "4.2 출력: 조밀한 패리티 확률"

#: ../../tutorials/01_neural_networks.ipynb:869
msgid "4.3 Output: Samples"
msgstr "4.3 출력: 샘플"

#: ../../tutorials/01_neural_networks.ipynb:985
msgid "4.4 Output: Parity Samples"
msgstr "4.4 출력: 패리티 샘플"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:9
msgid "Neural Network Classifier & Regressor"
msgstr "신경망 분류기 & 회귀기"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:11
msgid "In this tutorial we show how the ``NeuralNetworkClassifier`` and ``NeuralNetworkRegressor`` are used. Both take as an input a (Quantum) ``NeuralNetwork`` and leverage it in a specific context. In both cases we also provide a pre-configured variant for convenience, the Variational Quantum Classifier (``VQC``) and Variational Quantum Regressor (``VQR``). The tutorial is structured as follows:"
msgstr "이 튜토리얼에서는 ``NeuralNetworkClassifier`` 와 ``NeuralNetworkRegressor`` 가 어떻게 사용되는지를 보여준다. 두 가지 모두 (양자) ``NeuralNetwork`` 을 입력하고 이를 특정한 문맥에서 활용한다. 두 가지 모두 편의상 사전 구성된 Variational Quantum Classifier (``VQC``) 와 Variational Quantum Regressor (``VQR``) 도 제공한다. 이 튜토리얼은 다음과 같이 구성된다."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:13
msgid "`Classification <#Classification>`__"
msgstr "`분류 <#Classification>`__"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:15
msgid "Classification with an ``OpflowQNN``"
msgstr "``OpflowQNN`` 를 이용한 분류"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:16
#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:249
msgid "Classification with a ``CircuitQNN``"
msgstr "``CircuitQNN`` 를 이용한 분류"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:17
#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:398
msgid "Variational Quantum Classifier (``VQC``)"
msgstr "변분 양자분류기 (``VQC``, Variational Quantum Classifier)"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:19
msgid "`Regression <#Regression>`__"
msgstr "`회귀 <#Regression>`__"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:21
#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:539
msgid "Regression with an ``OpflowQNN``"
msgstr "``OpflowQNN`` 을 통한 회귀"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:22
msgid "Variational Quantum Regressor (``VQR``)"
msgstr "변분 양자회귀기 (``VQR``, Variational Quantum Regressor)"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:70
#: ../../tutorials/03_quantum_kernel.ipynb:53
msgid "Classification"
msgstr "분류"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:72
msgid "We prepare a simple classification dataset to illustrate the following algorithms."
msgstr "다음 알고리즘을 설명하기 위해 간단한 분류 데이터 셋을 준비한다."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:117
msgid "Classification with the an ``OpflowQNN``"
msgstr "``OpflowQNN`` 를 이용한 분류"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:119
msgid "First we show how an ``OpflowQNN`` can be used for classification within a ``NeuralNetworkClassifier``. In this context, the ``OpflowQNN`` is expected to return one-dimensional output in :math:`[-1, +1]`. This only works for binary classification and we assign the two classes to :math:`\\{-1, +1\\}`. For convenience, we use the ``TwoLayerQNN``, which is a special type of ``OpflowQNN`` defined via a feature map and an ansatz."
msgstr "먼저 ``OpflowQNN`` 가 어떻게 ``NeuralNetworkClassifier`` 에서 분류하는데 사용될 수 있는지 살펴보자. 이런 맥락에서 ``OpflowQNN`` 은 :math:`[-1, +1]` 에서 1차원 출력을 반환할 것으로 예상된다. 이는 이진 분류에서만 작동하며 두 클래스를 :math:`\\{-1, +1\\}` 에 지정한다. 편의상, 특징맵과 ansatz를 통해 정의된 ``OpflowQNN`` 의 특별한 유형인 ``TwoLayerQNN`` 을 사용한다."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:251
msgid "Next we show how a ``CircuitQNN`` can be used for classification within a ``NeuralNetworkClassifier``. In this context, the ``CircuitQNN`` is expected to return :math:`d`-dimensional probability vector as output, where :math:`d` denotes the number of classes. Sampling from a ``QuantumCircuit`` automatically results in a probability distribution and we just need to define a mapping from the measured bitstrings to the different classes. For binary classification we use the parity mapping."
msgstr "다음으로 우리는 어떻게 ``CircuitQNN`` 이 ``NeuralNetworkClassifier`` 에서 분류 문제에 사용될 수 있는지 알아볼 것이다. 여기서 ``CircuitQNN`` 이라 하면 :math:`d` - 차원 확률 벡터를 출력으로 반환하며, :math:`d` 는 카테고리(class)의 수에 해당한다. ``QuantumCircuit`` 을 통해 표본추출을 하면 자동적으로 확률 분포를 생성되기 때문에 우리는 그저 측정된 비트열에서 다른 카테고리로 이어지는 매핑을 정의하면 된다. 이진 분류 문제의 경우 패리티 매핑이 사용된다."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:400
msgid "The ``VQC`` is a special variant of the ``NeuralNetworkClassifier`` with a ``CircuitQNN``. It applies a parity mapping (or extensions to multiple classes) to map from the bitstring to the classification, which results in a probability vector, which is interpreted as a one-hot encoded result. By default, it applies this the ``CrossEntropyLoss`` function that expects labels given in one-hot encoded format and will return predictions in that format too."
msgstr "``VQC`` 는 ``CircuitQNN`` 와 마찬가지로 ``NeuralNetworkClassifier`` 의 특별한 변종이다. 이는 비트열에서 분류로의 매핑을 위한 패리티 매핑(또는 여러 클래스들의 확장)을 제공하는데, 이는 확률 벡터를 생성하며 원-핫 인코딩으로써 해석된다. 기본적으로 이는 원-핫 인코딩된 형식으로 지정된 레이블을 예상하는 ``CrossEntropyLoss`` 기능을 적용하여 해당 형식으로 예측 결과를 반환한다."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:496
msgid "Regression"
msgstr "회귀"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:498
msgid "We prepare a simple regression dataset to illustrate the following algorithms."
msgstr "다음 알고리즘을 설명하기 위해 간단한 회귀 데이터셋을 준비한다."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:541
msgid "Here we restrict to regression with an ``OpflowQNN`` that returns values in :math:`[-1, +1]`. More complex and also multi-dimensional models could be constructed, also based on ``CircuitQNN`` but that exceeds the scope of this tutorial."
msgstr "여기서는 :math:`[-1, +1]` 의 값을 반환하는 ``OpflowQNN`` 를 통한 회귀로 케이스를 제한한다. ``CircuitQNN`` 을 기반으로 하여 보다 복잡한 다차원 모델을 구성할 수도 있지만 이 튜토리얼의 범위에서 벗어나기 때문에 다루지 않는다."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:648
msgid "Regression with the Variational Quantum Regressor (``VQR``)"
msgstr "Variational Quantum Regressor (``VQR``) 를 이용한 회귀"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:650
msgid "Similar to the ``VQC`` for classification, the ``VQR`` is a special variant of the ``NeuralNetworkRegressor`` with a ``OpflowQNN``. By default it considers the ``L2Loss`` function to minimize the mean squared error between predictions and targets."
msgstr "분류에서 ``VQC`` 와 유사하게, ``VQR`` 은 ``OpflowQNN`` 와 더불어 ``NeuralNetworkRegressor`` 의 특별한 변종이다. 기본적으로  ``L2Loss`` 함수를 고려하여 예측과 목표 사이의 평균 제곱 오차를 최소화한다."

#: ../../tutorials/03_quantum_kernel.ipynb:9
msgid "Quantum Kernel Machine Learning"
msgstr "양자 커널 기계 학습"

#: ../../tutorials/03_quantum_kernel.ipynb:11
msgid "The general task of machine learning is to find and study patterns in data. For many datasets, the datapoints are better understood in a higher dimensional feature space, through the use of a kernel function: :math:`k(\\vec{x}_i, \\vec{x}_j) = \\langle f(\\vec{x}_i), f(\\vec{x}_j) \\rangle` where :math:`k` is the kernel function, :math:`\\vec{x}_i, \\vec{x}_j` are :math:`n` dimensional inputs, :math:`f` is a map from :math:`n`-dimension to :math:`m`-dimension space and :math:`\\langle a,b \\rangle` denotes the dot product. When considering finite data, a kernel function can be represented as a matrix: :math:`K_{ij} = k(\\vec{x}_i,\\vec{x}_j)`."
msgstr "일반적으로 기계학습은 데이터에서 패턴을 찾고 학습하는 것이다. 많은 양의 데이터에서 데이터 포인트들은 더 높은 차원의 특징 공간에서 더 잘 이해되어질 수 있는데, 이는 다음과 같은 커널 함수를 사용함으로써 가능해진다:  :math:`k(\\vec{x}_i, \\vec{x}_j) = \\langle f(\\vec{x}_i), f(\\vec{x}_j) \\rangle` 에서,  :math:`k` 는 커널 함수, :math:`\\vec{x}_i, \\vec{x}_j` 은 :math:`n` 차원의 입력, :math:`f` 는 :math:`n` 차원으로부터 :math:`m` 차원 공간으로의 맵, 그리고 :math:`\\langle a,b \\rangle` 는 내적을 의미한다. 유한한 데이터를 고려할 때, 커널 함수는 다음과 같은 행렬로서 표현될 수 있다: :math:`K_{ij} = k(\\vec{x}_i,\\vec{x}_j)`"

#: ../../tutorials/03_quantum_kernel.ipynb:14
msgid "In quantum kernel machine learning, a quantum feature map :math:`\\phi(\\vec{x})` is used to map a classical feature vector :math:`\\vec{x}` to a quantum Hilbert space, :math:`| \\phi(\\vec{x})\\rangle \\langle \\phi(\\vec{x})|`, such that :math:`K_{ij} = \\left| \\langle \\phi^\\dagger(\\vec{x}_j)| \\phi(\\vec{x}_i) \\rangle \\right|^{2}`. See `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__ for more details."
msgstr "양자 커널 기계학습에서, 양자 특징 맵 :math:`\\phi(\\vec{x})` 은 고전적인 특징 벡터 :math:`\\vec{x}` 를 양자 힐베르트 공간 :math:`| \\phi(\\vec{x})\\rangle \\langle \\phi(\\vec{x})|` 으로 매핑하는데 쓰일 수 있는데, 예를 들면 :math:`K_{ij} = \\left| \\langle \\phi^\\dagger(\\vec{x}_j)| \\phi(\\vec{x}_i) \\rangle \\right|^{2}` 와 같다. 더 자세한 사항은 `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__ 을 참조하라."

#: ../../tutorials/03_quantum_kernel.ipynb:16
msgid "In this notebook, we use ``qiskit`` to calculate a kernel matrix using a quantum feature map, then use this kernel matrix in ``scikit-learn`` classification and clustering algorithms."
msgstr "이 노트북 예제에서는 양자 특성 맵을 사용하여 커널 행렬을 계산하기 위해 ``qiskit`` 을 사용하였고, ``scikit-learn`` 분류 및 군집화 알고리즘에서 앞서 계산한 커널 행렬을 사용한다."

#: ../../tutorials/03_quantum_kernel.ipynb:55
msgid "For our classification example, we will use the *ad hoc dataset* as described in `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__, and the ``scikit-learn`` `support vector machine <https://scikit-learn.org/stable/modules/svm.html>`__ classification (``svc``) algorithm."
msgstr "이 분류 예제에서는 `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__ 에 설명된 바와 같이 *ad hoc dataset* 을 다시 사용할 것이고, ``scikit-learn`` `support vector machine <https://scikit-learn.org/stable/modules/svm.html>`__ classification(``svc``) 알고리즘을 사용할 것이다."

#: ../../tutorials/03_quantum_kernel.ipynb:111
msgid "With our training and testing datasets ready, we set up the ``QuantumKernel`` class to calculate a kernel matrix using the `ZZFeatureMap <https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZZFeatureMap.html>`__, and the ``BasicAer`` ``qasm_simulator`` using 1024 shots."
msgstr "학습 데이터와 테스트 데이터를 준비하고, ``QuantumKernel`` 클래스를 설정한다. 이는 `ZZFeatureMap <https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZZFeatureMap.html>`__ 과 1024개의 샷을 사용하는 ``BasicAer`` ``qasm_simulator`` 를 통해 커널 행렬을 계산하기 위함이다."

#: ../../tutorials/03_quantum_kernel.ipynb:138
msgid "The ``scikit-learn`` ``svc`` algorithm allows us to define a `custom kernel <https://scikit-learn.org/stable/modules/svm.html#custom-kernels>`__ in two ways: by providing the kernel as a callable function or by precomputing the kernel matrix. We can do either of these using the ``QuantumKernel`` class in ``qiskit``."
msgstr "``scikit-learn`` ``svc`` 알고리즘을 통해 `사용자 정의 커널 <https://scikit-learn.org/stable/modules/svm.html#custom-kernels>`__ 을 두 가지 방법으로 정의할 수 있는데, 그 하나는 커널을 호출 가능한 함수로 제공하는 방법이고 다른 하나는 커널 행렬을 미리 계산하는 방법이다. ``qiskit`` 의 ``QuantumKernel`` 클래스를 사용하여 이들 중 하나를 수행할 수 있다."

#: ../../tutorials/03_quantum_kernel.ipynb:140
msgid "The following code gives the kernel as a callable function:"
msgstr "다음 코드는 커널을 호출 가능한 함수로 제공한다."

#: ../../tutorials/03_quantum_kernel.ipynb:184
msgid "The following code precomputes and plots the training and testing kernel matrices before providing them to the ``scikit-learn`` ``svc`` algorithm:"
msgstr "다음 코드는 훈련 및 테스트 커널 행렬을 사전 계산하고 플로팅하여 ``scikit-learn`` ``svc`` 알고리즘에 제공한다."

#: ../../tutorials/03_quantum_kernel.ipynb:250
msgid "``qiskit`` also contains the ``qsvc`` class that extends the ``sklearn svc`` class, that can be used as follows:"
msgstr "``qiskit`` 은 또한 ``sklearn svc`` 를 상속한 ``qsvc`` 클래스를 포함하고 있으며 다음과 같이 사용할 수 있다."

#: ../../tutorials/03_quantum_kernel.ipynb:295
msgid "Clustering"
msgstr "Clustering(군집화)"

#: ../../tutorials/03_quantum_kernel.ipynb:297
msgid "For our clustering example, we will again use the *ad hoc dataset* as described in `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__, and the ``scikit-learn`` ``spectral`` clustering algorithm."
msgstr "우리의 군집화 예제에서는 `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__ 에 설명된 바와 같이 *ad hoc dataset* 를 다시 사용할 것이고, ``scikit-learn`` ``spectral`` 군집화 알고리즘을 사용할 것이다."

#: ../../tutorials/03_quantum_kernel.ipynb:299
msgid "We will regenerate the dataset with a larger gap between the two classes, and as clustering is an unsupervised machine learning task, we don’t need a test sample."
msgstr "두 클래스 간에 큰 간격을 두고 데이터셋을 다시 생성하고, 군집화는 비지도학습 머신러닝 태스크이기 때문에 테스트 샘플은 준비하지 않는다."

#: ../../tutorials/03_quantum_kernel.ipynb:350
msgid "We again set up the ``QuantumKernel`` class to calculate a kernel matrix using the `ZZFeatureMap <https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZZFeatureMap.html>`__, and the BasicAer ``qasm_simulator`` using 1024 shots."
msgstr "훈련 데이터 셋과 실험 데이터 셋을 준비하고, ``QuantumKernel`` 클래스를 다시 셋업한다. 이는 `ZZFeatureMap <https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZZFeatureMap.html>`__ 과 1024개의 샷을 사용하는 BasicAer ``qasm_simulator`` 를 통해 커널 행렬을 계산하기 위함이다."

#: ../../tutorials/03_quantum_kernel.ipynb:377
msgid "The scikit-learn spectral clustering algorithm allows us to define a [custom kernel] in two ways: by providing the kernel as a callable function or by precomputing the kernel matrix. Using the QuantumKernel class in qiskit, we can only use the latter."
msgstr "scikit-learn 스펙트럼 클러스터링 알고리즘을 통해 [커스텀 커널] 을 두 가지 방법으로 정의할 수 있는데, 그 하나는 커널을 호출 가능한 함수로 제공하는 방법이고 다른 하나는 커널 매트릭스를 미리 계산하는 방법이다. qiskit의 QuantumKernel 클래스를 사용하면 두 번째 방법만 사용할 수 있다."

#: ../../tutorials/03_quantum_kernel.ipynb:379
msgid "The following code precomputes and plots the kernel matrices before providing it to the scikit-learn spectral clustering algorithm, and scoring the labels using normalized mutual information, since we a priori know the class labels."
msgstr "아래의 코드는 커널 행렬을 scikit-learn 스펙트럼 군집 알고리즘 적용전에 커널 행렬을 미리 계산하고 플롯팅한다. 우리는 클래스의 정답 라벨을 알고 있기 때문에 정규화된 상호정보(mutual information)를 사용하여 예측된 라벨을 채점한다."

#: ../../tutorials/03_quantum_kernel.ipynb:439
msgid "``scikit-learn`` has other algorithms that can use a precomputed kernel matrix, here are a few:"
msgstr "``scikit-learn`` 은 미리 계산된 커널 행렬을 사용할 수 있는 다른 알고리즘을 가지고 있는데, 이것들이 그 중 일부이다."

#: ../../tutorials/03_quantum_kernel.ipynb:441
msgid "`Agglomerative clustering <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html>`__"
msgstr "`응집 군집화 <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html>`__"

#: ../../tutorials/03_quantum_kernel.ipynb:442
msgid "`Support vector regression <https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html>`__"
msgstr "`서포트 벡터 회귀 <https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html>`__"

#: ../../tutorials/03_quantum_kernel.ipynb:443
msgid "`Ridge regression <https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html>`__"
msgstr "`Ridge 회귀 <https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html>`__"

#: ../../tutorials/03_quantum_kernel.ipynb:444
msgid "`Gaussian process regression <https://scikit-learn.org/stable/modules/gaussian_process.html>`__"
msgstr "`Gaussian 과정 회귀 <https://scikit-learn.org/stable/modules/gaussian_process.html>`__"

#: ../../tutorials/03_quantum_kernel.ipynb:445
msgid "`Principal component analysis <https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html>`__"
msgstr "`주성분 분석 <https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html>`__"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:9
msgid "qGANs for Loading Random Distributions"
msgstr "무작위 분포를 로드하기 위한 qGAN"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:11
msgid "Given :math:`k`-dimensional data samples, we employ a quantum Generative Adversarial Network (qGAN) to learn the data’s underlying random distribution and to load it directly into a quantum state:"
msgstr ":math:`k` 차원 데이터 샘플을 고려할 때, 우리는 양자 생성적 적대 신경망 (quantum Generative Adversarial Network) (qGAN) 를 골라 데이터의 근본적인 무작위 분포를 학습하고 이를 양자 상태로 직접 로드한다."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:13
msgid "\\big| g_{\\theta}\\rangle = \\sum_{j=0}^{2^n-1} \\sqrt{p_{\\theta}^{j}}\\big| j \\rangle\n\n"
msgstr "\\big| g_{\\theta}\\rangle = \\sum_{j=0}^{2^n-1} \\sqrt{p_{\\theta}^{j}}\\big| j \\rangle\n\n"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:15
msgid "where :math:`p_{\\theta}^{j}` describe the occurrence probabilities of the basis states :math:`\\big| j\\rangle`."
msgstr "여기서 :math:`p_{\\theta}^{j}` 는 기저 상태의 발생 확률 :math:`\\big| j\\rangle` 을 설명한다."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:17
msgid "The aim of the qGAN training is to generate a state :math:`\\big| g_{\\theta}\\rangle` where :math:`p_{\\theta}^{j}`, for :math:`j\\in \\left\\{0, \\ldots, {2^n-1} \\right\\}`, describe a probability distribution that is close to the distribution underlying the training data :math:`X=\\left\\{x^0, \\ldots, x^{k-1} \\right\\}`."
msgstr "qGAN 학습의 목표는 학습 데이터 :math:`X=\\left\\{x^0, \\ldots, x^{k-1} \\right\\}` 와 비슷한 확률분포 :math:`p_{\\theta}^{j}` 를 서술하는 :math:`\\big| g_{\\theta}\\rangle` 을 생성하는 것이다. 이 때 , :math:`j\\in \\left\\{0, \\ldots, {2^n-1} \\right\\}` 이다."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:19
msgid "For further details please refer to `Quantum Generative Adversarial Networks for Learning and Loading Random Distributions <https://arxiv.org/abs/1904.00043>`__ *Zoufal, Lucchi, Woerner* [2019]."
msgstr "자세한 사항은 `Quantum Generative Adversarial Networks for Learning and Loading Random Distributions <https://arxiv.org/abs/1904.00043>`__ *Zoufal, Lucchi, Woerner* [2019] 을 참조하라."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:21
msgid "For an example of how to use a trained qGAN in an application, the pricing of financial derivatives, please see the `Option Pricing with qGANs <https://github.com/Qiskit/qiskit-finance/tree/main/docs/tutorials/10_qgan_option_pricing.ipynb>`__ tutorial."
msgstr "학습된 qGAN을 사용하는 예제로, 금융 파생상품의 가격 책정 예제가 있다. `Option Pricing with qGANs <https://github.com/Qiskit/qiskit-finance/tree/main/docs/tutorials/10_qgan_option_pricing.ipynb>`__ 튜토리얼을 참고하라."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:56
msgid "Load the Training Data"
msgstr "훈련 데이터 로드하기"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:58
msgid "First, we need to load the :math:`k`-dimensional training data samples (here k=1)."
msgstr "첫째, :math:`k` 차원 훈련 데이터 샘플 (여기서 k= 1) 을 로드해야 한다."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:60
msgid "Next, the data resolution is set, i.e. the min/max data values and the number of qubits used to represent each data dimension."
msgstr "다음으로, 데이터 해상도가 설정되고, 즉, 최소/최대 데이터 값들 및 각각의 데이터 차원을 표현하기 위해 사용되는 큐비트들의 수가 설정된다."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:95
msgid "Initialize the qGAN"
msgstr "qGAN 초기화하기"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:97
msgid "The qGAN consists of a quantum generator :math:`G_{\\theta}`, i.e., an ansatz, and a classical discriminator :math:`D_{\\phi}`, a neural network."
msgstr "qGAN은 양자생성기 :math:`G_{\\theta}` , 즉 ansatz와 고전적인 식별자 :math:`D_{\\phi}` (신경망)로 구성되어 있다."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:99
msgid "To implement the quantum generator, we choose a depth-\\ :math:`1` ansatz that implements :math:`R_Y` rotations and :math:`CZ` gates which takes a uniform distribution as an input state. Notably, for :math:`k>1` the generator’s parameters must be chosen carefully. For example, the circuit depth should be :math:`>1` because higher circuit depths enable the representation of more complex structures."
msgstr "양자 생성기를 구현하기 위해서는 :math:`R_Y` 회전을 구현하는 깊이 1 의 ansatz와 입력 상태로 균일한 분포를 취하는 :math:`CZ` 게이트를 선택해야 한다. 특히 :math:`k>1` 의 경우 생성기의 매개변수를 신중하게 선택해야 한다. 예를 들어, 더 높은 회로 깊이가 더 복잡한 구조의 표현을 가능하게 하기 때문에 회로 깊이는 :math:`k>1` 이어야 한다."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:101
msgid "The classical discriminator used here is based on a neural network implementation using NumPy. There is also a discriminator based on PyTorch which is not installed by default when installing Qiskit - see `Optional Install <https://github.com/Qiskit/qiskit-machine-learning#optional-installs>`__ for more information."
msgstr "여기서 사용되는 고전적인 판별기는 NumPy를 사용하는 신경망 구현을 기반으로 한다. 또한, Qiskit을 설치할 때 기본 옵션으로 설치되지 않은 PyTorch를 기반으로 한 판별기도 있다. 자세한 정보는 `Optional Install <https://github.com/Qiskit/qiskit-machine-learning#optional-installs>`__ 에서 찾아볼 수 있다."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:103
msgid "Here, both networks are updated with the ADAM optimization algorithm (ADAM is qGAN optimizer default)."
msgstr "여기서, 두 네트워크는 ADAM 최적화 알고리즘으로 업데이트된다 (ADAM은 기본 qGAN 최적화 프로그램이다)."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:164
msgid "Run the qGAN Training"
msgstr "qGAN 학습 실행하기"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:166
msgid "During the training the discriminator’s and the generator’s parameters are updated alternately w.r.t the following loss functions:"
msgstr "훈련 중에 식별자 및 생성기의 매개변수는 다음과 같은 손실 함수가 번갈아 업데이트된다."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:168
msgid "L_G\\left(\\phi, \\theta\\right) = -\\frac{1}{m}\\sum\\limits_{l=1}^{m}\\left[\\log\\left(D_{\\phi}\\left(g^{l}\\right)\\right)\\right]\n\n"
msgstr "L_G\\left(\\phi, \\theta\\right) = -\\frac{1}{m}\\sum\\limits_{l=1}^{m}\\left[\\log\\left(D_{\\phi}\\left(g^{l}\\right)\\right)\\right]\n\n"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:170
msgid "and"
msgstr "그리고"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:172
msgid "L_D\\left(\\phi, \\theta\\right) =\n"
"  \\frac{1}{m}\\sum\\limits_{l=1}^{m}\\left[\\log D_{\\phi}\\left(x^{l}\\right) + \\log\\left(1-D_{\\phi}\\left(g^{l}\\right)\\right)\\right],"
msgstr "L_D\\left(\\phi, \\theta\\right) =\n"
"  \\frac{1}{m}\\sum\\limits_{l=1}^{m}\\left[\\log D_{\\phi}\\left(x^{l}\\right) + \\log\\left(1-D_{\\phi}\\left(g^{l}\\right)\\right)\\right],"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:177
msgid "with :math:`m` denoting the batch size and :math:`g^l` describing the data samples generated by the quantum generator."
msgstr "예를 들어, :math:`m` 은 배치 사이즈 (batch size) 를 의미하고 :math:`g^l` 는 양자 생성기 (quantum generator) 에 의해 발생된 데이터 샘플들을 의미한다."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:179
msgid "Please note that the training, for the purpose of this notebook, has been kept briefer by the selection of a known initial point (``init_params``). Without such prior knowledge be aware training may take some while."
msgstr "이 노트북의 목적상, 알려진 초기 포인트(``init_params``) 를 선택함으로써 본 교육을 보다 간략하게 진행하였음을 유념해주길 바란다. 이러한 사전 지식이 없으면 교육에 시간이 다소 걸릴 수 있다."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:245
msgid "Training Progress & Outcome"
msgstr "학습 진행 및 결과"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:247
msgid "Now, we plot the evolution of the generator’s and the discriminator’s loss functions during the training, as well as the progress in the relative entropy between the trained and the target distribution."
msgstr "이제, 학습하는 동안 생성기의 및 식별자의 손실 함수의 전개 뿐만 아니라 훈련된 및 타겟 분포 사이의 상대적 엔트로피의 진행을 플로팅한다."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:249
msgid "Finally, we also compare the cumulative distribution function (CDF) of the trained distribution to the CDF of the target distribution."
msgstr "마지막으로, 학습된 분포의 누적 분포 함수 (CDF) 를 타겟 분포의 CDF와 비교한다."

#: ../../tutorials/05_torch_connector.ipynb:9
msgid "Torch Connector and Hybrid QNNs"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:11
msgid "This tutorial introduces Qiskit’s ``TorchConnector`` class, and demonstrates how the ``TorchConnector`` allows for a natural integration of any ``NeuralNetwork`` from Qiskit Machine Learning into a PyTorch workflow. ``TorchConnector`` takes a Qiskit ``NeuralNetwork`` and makes it available as a PyTorch ``Module``. The resulting module can be seamlessly incorporated into PyTorch classical architectures and trained jointly without additional considerations, enabling the development and testing of novel **hybrid quantum-classical** machine learning architectures."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:15
msgid "Content:"
msgstr "내용:"

#: ../../tutorials/05_torch_connector.ipynb:17
msgid "`Part 1: Simple Classification & Regression <#Part-1:-Simple-Classification-&-Regression>`__"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:19
msgid "The first part of this tutorial shows how quantum neural networks can be trained using PyTorch’s automatic differentiation engine (``torch.autograd``, `link <https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html>`__) for simple classification and regression tasks."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:21
msgid "`Classification <#1.-Classification>`__"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:23
msgid "Classification with PyTorch and ``OpflowQNN``"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:24
msgid "Classification with PyTorch and ``CircuitQNN``"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:26
msgid "`Regression <#2.-Regression>`__"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:28
msgid "Regression with PyTorch and ``OpflowQNN``"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:30
msgid "`Part 2: MNIST Classification, Hybrid QNNs <#Part-2:-MNIST-Classification,-Hybrid-QNNs>`__"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:32
msgid "The second part of this tutorial illustrates how to embed a (Quantum) ``NeuralNetwork`` into a target PyTorch workflow (in this case, a typical CNN architecture) to classify MNIST data in a hybrid quantum-classical manner."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:74
msgid "Part 1: Simple Classification & Regression"
msgstr "파트 1: 단순 분류 및 회귀"

#: ../../tutorials/05_torch_connector.ipynb:86
msgid "1. Classification"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:88
msgid "First, we show how ``TorchConnector`` allows to train a Quantum ``NeuralNetwork`` to solve a classification tasks using PyTorch’s automatic differentiation engine. In order to illustrate this, we will perform **binary classification** on a randomly generated dataset."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:144
msgid "A. Classification with PyTorch and ``OpflowQNN``"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:146
msgid "Linking an ``OpflowQNN`` to PyTorch is relatively straightforward. Here we illustrate this using the ``TwoLayerQNN``, a sub-case of ``OpflowQNN`` introduced in previous tutorials."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:254
msgid "Optimizer"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:256
msgid "The choice of optimizer for training any machine learning model can be crucial in determining the success of our training’s outcome. When using ``TorchConnector``, we get access to all of the optimizer algorithms defined in the [``torch.optim``] package (`link <https://pytorch.org/docs/stable/optim.html>`__). Some of the most famous algorithms used in popular machine learning architectures include *Adam*, *SGD*, or *Adagrad*. However, for this tutorial we will be using the L-BFGS algorithm (``torch.optim.LBFGS``), one of the most well know second-order optimization algorithms for numerical optimization."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:260
msgid "Loss Function"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:262
msgid "As for the loss function, we can also take advantage of PyTorch’s pre-defined modules from ``torch.nn``, such as the `Cross-Entropy <https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html>`__ or `Mean Squared Error <https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html>`__ losses."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:264
msgid "**💡 Clarification :** In classical machine learning, the general rule of thumb is to apply a Cross-Entropy loss to classification tasks, and MSE loss to regression tasks. However, this recommendation is given under the assumption that the output of the classification network is a class probability value in the [0,1] range (usually this is achieved through a Softmax layer). Because the following example for ``TwoLayerQNN`` does not include such layer, and we don’t apply any mapping to the output (the following section shows an example of application of parity mapping with ``CircuitQNNs``), the QNN’s output can take any value in the range [-1,1]. In case you were wondering, this is the reason why this particular example uses MSELoss for classification despite it not being the norm (but we encourage you to experiment with different loss functions and see how they can impact training results)."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:442
#: ../../tutorials/05_torch_connector.ipynb:674
msgid "The red circles indicate wrongly classified data points."
msgstr "빨간색 원은 잘못 분류된 데이터 포인트들을 나타낸다."

#: ../../tutorials/05_torch_connector.ipynb:454
msgid "B. Classification with PyTorch and ``CircuitQNN``"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:456
msgid "Linking an ``CircuitQNN`` to PyTorch requires a bit more attention than ``OpflowQNN``. Without the correct setup, backpropagation is not possible."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:458
msgid "In particular, we must make sure that we are returning a dense array of probabilities in the network’s forward pass (``sparse=False``). This parameter is set up to ``False`` by default, so we just have to make sure that it has not been changed."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:460
msgid "**⚠️ Attention:** If we define a custom interpret function ( in the example: ``parity``), we must remember to explicitly provide the desired output shape ( in the example: ``2``). For more info on the initial parameter setup for ``CircuitQNN``, please check out the `official qiskit documentation <https://qiskit.org/documentation/machine-learning/stubs/qiskit_machine_learning.neural_networks.CircuitQNN.html>`__."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:523
#: ../../tutorials/05_torch_connector.ipynb:815
msgid "For a reminder on optimizer and loss function choices, you can go back to `this section <#Optimizer>`__."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:686
msgid "2. Regression"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:688
msgid "We use a model based on the ``TwoLayerQNN`` to also illustrate how to perform a regression task. The chosen dataset in this case is randomly generated following a sine wave."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:730
msgid "A. Regression with PyTorch and ``OpflowQNN``"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:741
msgid "The network definition and training loop will be analogous to those of the classification task using ``TwoLayerQNN``. In this case, we define our own feature map and ansatz, instead of using the default values."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:963
msgid "Part 2: MNIST Classification, Hybrid QNNs"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:965
msgid "In this second part, we show how to leverage a hybrid quantum-classical neural network using ``TorchConnector``, to perform a more complex image classification task on the MNIST handwritten digits dataset."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:967
msgid "For a more detailed (pre-``TorchConnector``) explanation on hybrid quantum-classical neural networks, you can check out the corresponding section in the `Qiskit Textbook <https://qiskit.org/textbook/ch-machine-learning/machine-learning-qiskit-pytorch.html>`__."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:996
msgid "Step 1: Defining Data-loaders for train and test"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:1007
msgid "We take advantage of the ``torchvision`` `API <https://pytorch.org/vision/stable/datasets.html>`__ to directly load a subset of the `MNIST dataset <https://en.wikipedia.org/wiki/MNIST_database>`__ and define torch ``DataLoader``\\ s (`link <https://pytorch.org/docs/stable/data.html>`__) for train and test."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:1048
msgid "If we perform a quick visualization we can see that the train dataset consists of images of handwritten 0s and 1s."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:1120
msgid "Step 2: Defining the QNN and Hybrid Model"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:1131
msgid "This second step shows the power of the ``TorchConnector``. After defining our quantum neural network layer (in this case, a ``TwoLayerQNN``), we can embed it into a layer in our torch ``Module`` by initializing a torch connector as ``TorchConnector(qnn)``."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:1133
msgid "**⚠️ Attention:** In order to have an adequate gradient backpropagation in hybrid models, we MUST set the initial parameter ``input_gradients`` to TRUE during the qnn initialization."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:1235
msgid "Step 3: Training"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:1337
msgid "Step 4: Evaluation"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:1440
msgid "🎉🎉🎉🎉 **You are now able to experiment with your own hybrid datasets and architectures using Qiskit Machine Learning.** **Good Luck!**"
msgstr ""

#: ../../tutorials/index.rst:3
msgid "Machine Learning Tutorials"
msgstr "머신러닝 튜토리얼"

