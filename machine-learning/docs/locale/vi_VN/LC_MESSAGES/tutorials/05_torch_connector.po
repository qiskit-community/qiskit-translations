msgid ""
msgstr ""
"Project-Id-Version: qiskit-docs\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-02-07 03:44+0000\n"
"PO-Revision-Date: 2023-02-07 05:10\n"
"Last-Translator: \n"
"Language: vi\n"
"Language-Team: Vietnamese\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.11.0\n"
"X-Crowdin-Project: qiskit-docs\n"
"X-Crowdin-Project-ID: 369271\n"
"X-Crowdin-Language: vi\n"
"X-Crowdin-File: /master/machine-learning/docs/locale/en/LC_MESSAGES/tutorials/05_torch_connector.po\n"
"X-Crowdin-File-ID: 9636\n"

#: ../../tutorials/05_torch_connector.ipynb:9
msgid "This page was generated from `docs/tutorials/05_torch_connector.ipynb`__."
msgstr "Trang nÃ y Ä‘Æ°Æ¡Ì£c táº¡o tá»« `docs/tutorials/05_torch_connector.ipynb`__."

#: ../../tutorials/05_torch_connector.ipynb:9
msgid "Torch Connector and Hybrid QNNs"
msgstr "Torch Connector vÃ  Hybrid QNNs"

#: ../../tutorials/05_torch_connector.ipynb:11
msgid "This tutorial introduces Qiskitâ€™s ``TorchConnector`` class, and demonstrates how the ``TorchConnector`` allows for a natural integration of any ``NeuralNetwork`` from Qiskit Machine Learning into a PyTorch workflow. ``TorchConnector`` takes a Qiskit ``NeuralNetwork`` and makes it available as a PyTorch ``Module``. The resulting module can be seamlessly incorporated into PyTorch classical architectures and trained jointly without additional considerations, enabling the development and testing of novel **hybrid quantum-classical** machine learning architectures."
msgstr "HÆ°á»›ng dáº«n nÃ y giá»›i thiá»‡u lá»›p ``TorchConnector`` cá»§a Qiskit vÃ  trÃ¬nh bÃ y cÃ¡ch ``TorchConnector`` cho phÃ©p tÃ­ch há»£p tá»± nhiÃªn báº¥t ká»³ ``NeuralNetwork`` nÃ o tá»« Qiskit Machine Learning vÃ o má»™t quy trÃ¬nh lÃ m viá»‡c PyTorch. ``TorchConnector`` sá»­ dá»¥ng Qiskit ``NeuralNetwork`` vÃ  cung cáº¥p nÃ³ dÆ°á»›i dáº¡ng ``Module`` PyTorch. Module káº¿t quáº£ cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ­ch há»£p liá»n máº¡ch vÃ o cÃ¡c kiáº¿n trÃºc cá»• Ä‘iá»ƒn PyTorch vÃ  Ä‘Æ°á»£c Ä‘Ã o táº¡o chung mÃ  khÃ´ng cáº§n cÃ¢n nháº¯c thÃªm, cho phÃ©p phÃ¡t triá»ƒn vÃ  thá»­ nghiá»‡m cÃ¡c kiáº¿n trÃºc há»c mÃ¡y **lai lÆ°á»£ng tá»­-cá»• Ä‘iá»ƒn** má»›i."

#: ../../tutorials/05_torch_connector.ipynb:15
msgid "Content:"
msgstr "Ná»™i dung:"

#: ../../tutorials/05_torch_connector.ipynb:17
msgid "`Part 1: Simple Classification & Regression <#Part-1:-Simple-Classification-&-Regression>`__"
msgstr "`Pháº§n 1: PhÃ¢n loáº¡i Ä‘Æ¡n giáº£n & Há»“i quy <#Part-1:-Simple-Classification-&-Regression>`__"

#: ../../tutorials/05_torch_connector.ipynb:19
msgid "The first part of this tutorial shows how quantum neural networks can be trained using PyTorchâ€™s automatic differentiation engine (``torch.autograd``, `link <https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html>`__) for simple classification and regression tasks."
msgstr "Pháº§n Ä‘áº§u tiÃªn cá»§a hÆ°á»›ng dáº«n nÃ y cho tháº¥y cÃ¡ch máº¡ng neural lÆ°á»£ng tá»­ cÃ³ thá»ƒ Ä‘Æ°á»£c Ä‘Ã o táº¡o báº±ng cÃ¡ch sá»­ dá»¥ng cÃ´ng cá»¥ phÃ¢n biá»‡t tá»± Ä‘á»™ng cá»§a PyTorch (``torch.autograd``, `link <https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html>`__) cho cÃ¡c nhiá»‡m vá»¥ phÃ¢n loáº¡i vÃ  há»“i quy Ä‘Æ¡n giáº£n."

#: ../../tutorials/05_torch_connector.ipynb:21
msgid "`Classification <#1.-Classification>`__"
msgstr "`PhÃ¢n loáº¡i <#1.-Classification>`__"

#: ../../tutorials/05_torch_connector.ipynb:23
msgid "Classification with PyTorch and ``EstimatorQNN``"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:24
msgid "Classification with PyTorch and ``SamplerQNN``"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:26
msgid "`Regression <#2.-Regression>`__"
msgstr "`Há»“i quy <#2.-Regression>`__"

#: ../../tutorials/05_torch_connector.ipynb:28
msgid "Regression with PyTorch and ``SamplerQNN``"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:30
msgid "`Part 2: MNIST Classification, Hybrid QNNs <#Part-2:-MNIST-Classification,-Hybrid-QNNs>`__"
msgstr "`Part 2: PhÃ¢n loáº¡i MNIST, Hybrid QNNs <#Part-2:-MNIST-Classification,-Hybrid-QNNs>`__"

#: ../../tutorials/05_torch_connector.ipynb:32
msgid "The second part of this tutorial illustrates how to embed a (Quantum) ``NeuralNetwork`` into a target PyTorch workflow (in this case, a typical CNN architecture) to classify MNIST data in a hybrid quantum-classical manner."
msgstr "Pháº§n thá»© hai cá»§a hÆ°á»›ng dáº«n nÃ y minh há»a cÃ¡ch nhÃºng má»™t ``NeuralNetwork`` (LÆ°á»£ng tá»­) vÃ o má»™t quy trÃ¬nh lÃ m viá»‡c PyTorch má»¥c tiÃªu (trong trÆ°á»ng há»£p nÃ y lÃ  kiáº¿n trÃºc CNN Ä‘iá»ƒn hÃ¬nh) Ä‘á»ƒ phÃ¢n loáº¡i dá»¯ liá»‡u MNIST theo cÃ¡ch lai cá»• Ä‘iá»ƒn-lÆ°á»£ng tá»­."

#: ../../tutorials/05_torch_connector.ipynb:73
msgid "Part 1: Simple Classification & Regression"
msgstr "Pháº§n 1: PhÃ¢n loáº¡i Ä‘Æ¡n giáº£n vÃ  Há»“i quy"

#: ../../tutorials/05_torch_connector.ipynb:85
msgid "1. Classification"
msgstr "1. PhÃ¢n loáº¡i"

#: ../../tutorials/05_torch_connector.ipynb:87
msgid "First, we show how ``TorchConnector`` allows to train a Quantum ``NeuralNetwork`` to solve a classification tasks using PyTorchâ€™s automatic differentiation engine. In order to illustrate this, we will perform **binary classification** on a randomly generated dataset."
msgstr "Äáº§u tiÃªn, chÃºng tÃ´i chá»‰ ra cÃ¡ch mÃ  ``TorchConnector`` cho phÃ©p Ä‘Ã o táº¡o má»™t ``NeuralNetwork`` lÆ°á»£ng tá»­ Ä‘á»ƒ giáº£i quyáº¿t cÃ¡c nhiá»‡m vá»¥ phÃ¢n loáº¡i báº±ng cÃ¡ch sá»­ dá»¥ng cÃ´ng cá»¥ phÃ¢n biá»‡t tá»± Ä‘á»™ng cá»§a PyTorch. Äá»ƒ minh há»a cho Ä‘iá»u nÃ y, chÃºng tÃ´i sáº½ thá»±c hiá»‡n **phÃ¢n loáº¡i nhá»‹ phÃ¢n** trÃªn má»™t táº­p dá»¯ liá»‡u Ä‘Æ°á»£c táº¡o ngáº«u nhiÃªn."

#: ../../tutorials/05_torch_connector.ipynb:140
msgid "A. Classification with PyTorch and ``EstimatorQNN``"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:142
msgid "Linking an ``EstimatorQNN`` to PyTorch is relatively straightforward. Here we illustrate this by using the ``EstimatorQNN`` constructed from a feature map and an ansatz."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:262
msgid "Optimizer"
msgstr "TrÃ¬nh tá»‘i Æ°u"

#: ../../tutorials/05_torch_connector.ipynb:264
msgid "The choice of optimizer for training any machine learning model can be crucial in determining the success of our trainingâ€™s outcome. When using ``TorchConnector``, we get access to all of the optimizer algorithms defined in the [``torch.optim``] package (`link <https://pytorch.org/docs/stable/optim.html>`__). Some of the most famous algorithms used in popular machine learning architectures include *Adam*, *SGD*, or *Adagrad*. However, for this tutorial we will be using the L-BFGS algorithm (``torch.optim.LBFGS``), one of the most well know second-order optimization algorithms for numerical optimization."
msgstr "Viá»‡c lá»±a chá»n trÃ¬nh tá»‘i Æ°u hÃ³a Ä‘á»ƒ Ä‘Ã o táº¡o báº¥t ká»³ mÃ´ hÃ¬nh há»c mÃ¡y nÃ o cÃ³ thá»ƒ ráº¥t quan trá»ng trong viá»‡c xÃ¡c Ä‘á»‹nh sá»± thÃ nh cÃ´ng cá»§a káº¿t quáº£ Ä‘Ã o táº¡o. Khi sá»­ dá»¥ng ``TorchConnector``, chÃºng tÃ´i cÃ³ quyá»n truy cáº­p vÃ o táº¥t cáº£ cÃ¡c thuáº­t toÃ¡n cá»§a trÃ¬nh tá»‘i Æ°u hÃ³a Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh trong gÃ³i [``torch.optim``] (`link <https://pytorch.org/docs/stable/optim.html>`__). Má»™t sá»‘ thuáº­t toÃ¡n ná»•i tiáº¿ng nháº¥t Ä‘Æ°á»£c sá»­ dá»¥ng trong kiáº¿n trÃºc há»c mÃ¡y thÃ´ng dá»¥ng bao gá»“m *Adam*, *SGD*, hoáº·c *Adagrad*. Tuy nhiÃªn, Ä‘á»‘i vá»›i hÆ°á»›ng dáº«n nÃ y, chÃºng tÃ´i sáº½ sá»­ dá»¥ng thuáº­t toÃ¡n L-BFGS (``torch.optim.LBFGS``), má»™t trong nhá»¯ng thuáº­t toÃ¡n tá»‘i Æ°u hÃ³a báº­c hai Ä‘Æ°á»£c biáº¿t Ä‘áº¿n nhiá»u nháº¥t Ä‘á»ƒ tá»‘i Æ°u hÃ³a sá»‘."

#: ../../tutorials/05_torch_connector.ipynb:268
msgid "Loss Function"
msgstr "HÃ m máº¥t mÃ¡t"

#: ../../tutorials/05_torch_connector.ipynb:270
msgid "As for the loss function, we can also take advantage of PyTorchâ€™s pre-defined modules from ``torch.nn``, such as the `Cross-Entropy <https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html>`__ or `Mean Squared Error <https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html>`__ losses."
msgstr "Äá»‘i vá»›i loss function (hÃ m máº¥t mÃ¡t), chÃºng tÃ´i cÅ©ng cÃ³ thá»ƒ táº­n dá»¥ng cÃ¡c module Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh trÆ°á»›c cá»§a PyTorch tá»« ``torch.nn``, cháº³ng háº¡n nhÆ° `Cross-Entropy <https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html>`__ hoáº·c `Mean Squared Error <https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html>`__."

#: ../../tutorials/05_torch_connector.ipynb:272
msgid "**ğŸ’¡ Clarification :** In classical machine learning, the general rule of thumb is to apply a Cross-Entropy loss to classification tasks, and MSE loss to regression tasks. However, this recommendation is given under the assumption that the output of the classification network is a class probability value in the :math:`[0, 1]` range (usually this is achieved through a Softmax layer). Because the following example for ``EstimatorQNN`` does not include such layer, and we donâ€™t apply any mapping to the output (the following section shows an example of application of parity mapping with ``SamplerQNN``\\ s), the QNNâ€™s output can take any value in the range :math:`[-1, 1]`. In case you were wondering, this is the reason why this particular example uses MSELoss for classification despite it not being the norm (but we encourage you to experiment with different loss functions and see how they can impact training results)."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:482
#: ../../tutorials/05_torch_connector.ipynb:752
msgid "The red circles indicate wrongly classified data points."
msgstr "VÃ²ng trÃ²n mÃ u Ä‘á» biá»ƒu thá»‹ cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u bá»‹ phÃ¢n loáº¡i sai."

#: ../../tutorials/05_torch_connector.ipynb:494
msgid "B. Classification with PyTorch and ``SamplerQNN``"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:496
msgid "Linking a ``SamplerQNN`` to PyTorch requires a bit more attention than ``EstimatorQNN``. Without the correct setup, backpropagation is not possible."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:498
msgid "In particular, we must make sure that we are returning a dense array of probabilities in the networkâ€™s forward pass (``sparse=False``). This parameter is set up to ``False`` by default, so we just have to make sure that it has not been changed."
msgstr "Äáº·c biá»‡t, chÃºng tÃ´i pháº£i Ä‘áº£m báº£o ráº±ng chÃºng tÃ´i Ä‘ang tráº£ vá» má»™t máº£ng dÃ y Ä‘áº·c cÃ¡c xÃ¡c suáº¥t trong tÃ­nh toÃ¡n trá»±c tiáº¿p cá»§a máº¡ng (``sparse=False``). Tham sá»‘ nÃ y Ä‘Æ°á»£c thiáº¿t láº­p máº·c Ä‘á»‹nh thÃ nh `False``, vÃ¬ váº­y chÃºng tÃ´i chá»‰ cáº§n Ä‘áº£m báº£o ráº±ng nÃ³ khÃ´ng bá»‹ thay Ä‘á»•i."

#: ../../tutorials/05_torch_connector.ipynb:500
msgid "**âš ï¸ Attention:** If we define a custom interpret function ( in the example: ``parity``), we must remember to explicitly provide the desired output shape ( in the example: ``2``). For more info on the initial parameter setup for ``SamplerQNN``, please check out the `official qiskit documentation <https://qiskit.org/documentation/machine-learning/stubs/qiskit_machine_learning.neural_networks.SamplerQNN.html>`__."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:571
#: ../../tutorials/05_torch_connector.ipynb:860
msgid "For a reminder on optimizer and loss function choices, you can go back to `this section <#Optimizer>`__."
msgstr "Äá»ƒ cÃ³ lá»i nháº¯c vá» cÃ¡c lá»±a chá»n trÃ¬nh tá»‘i Æ°u hÃ³a vÃ  hÃ m máº¥t mÃ¡t, báº¡n cÃ³ thá»ƒ quay láº¡i `pháº§n nÃ y <#Optimizer>`__."

#: ../../tutorials/05_torch_connector.ipynb:764
msgid "2. Regression"
msgstr "2. Há»“i quy"

#: ../../tutorials/05_torch_connector.ipynb:766
msgid "We use a model based on the ``EstimatorQNN`` to also illustrate how to perform a regression task. The chosen dataset in this case is randomly generated following a sine wave."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:807
msgid "A. Regression with PyTorch and ``EstimatorQNN``"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:818
msgid "The network definition and training loop will be analogous to those of the classification task using ``EstimatorQNN``. In this case, we define our own feature map and ansatz, but letâ€™s do it a little different."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:999
msgid "Part 2: MNIST Classification, Hybrid QNNs"
msgstr "Pháº§n 2: PhÃ¢n loáº¡i MNIST, cÃ¡c QNN lai"

#: ../../tutorials/05_torch_connector.ipynb:1001
msgid "In this second part, we show how to leverage a hybrid quantum-classical neural network using ``TorchConnector``, to perform a more complex image classification task on the MNIST handwritten digits dataset."
msgstr "Trong pháº§n thá»© hai nÃ y, chÃºng tÃ´i chá»‰ ra cÃ¡ch táº­n dá»¥ng má»™t neural network cá»• Ä‘iá»ƒn-lÆ°á»£ng tá»­ lai báº±ng cÃ¡ch sá»­ dá»¥ng ``TorchConnector``, Ä‘á»ƒ thá»±c hiá»‡n nhiá»‡m vá»¥ phÃ¢n loáº¡i hÃ¬nh áº£nh phá»©c táº¡p hÆ¡n trÃªn táº­p dá»¯ liá»‡u chá»¯ sá»‘ viáº¿t tay MNIST."

#: ../../tutorials/05_torch_connector.ipynb:1003
msgid "For a more detailed (pre-``TorchConnector``) explanation on hybrid quantum-classical neural networks, you can check out the corresponding section in the `Qiskit Textbook <https://qiskit.org/textbook/ch-machine-learning/machine-learning-qiskit-pytorch.html>`__."
msgstr "Äá»ƒ cÃ³ giáº£i thÃ­ch chi tiáº¿t hÆ¡n (trÆ°á»›c ``TorchConnector``) vá» neural network cá»• Ä‘iá»ƒn-lÆ°á»£ng tá»­ lai, báº¡n cÃ³ thá»ƒ xem pháº§n tÆ°Æ¡ng á»©ng trong `Qiskit Textbook <https://qiskit.org/textbook/ch-machine-learning/machine-learning-qiskit-pytorch.html>`__."

#: ../../tutorials/05_torch_connector.ipynb:1042
msgid "Step 1: Defining Data-loaders for train and test"
msgstr "BÆ°á»›c 1: XÃ¡c Ä‘á»‹nh Bá»™ táº£i dá»¯ liá»‡u cho Ä‘Ã o táº¡o vÃ  kiá»ƒm tra"

#: ../../tutorials/05_torch_connector.ipynb:1053
msgid "We take advantage of the ``torchvision`` `API <https://pytorch.org/vision/stable/datasets.html>`__ to directly load a subset of the `MNIST dataset <https://en.wikipedia.org/wiki/MNIST_database>`__ and define torch ``DataLoader``\\ s (`link <https://pytorch.org/docs/stable/data.html>`__) for train and test."
msgstr "ChÃºng tÃ´i táº­n dá»¥ng ``torchvision`` `API <https://pytorch.org/vision/stable/datasets.html>`__ Ä‘á»ƒ táº£i trá»±c tiáº¿p má»™t táº­p con cá»§a `Táº­p dá»¯ liá»‡u MNIST <https://en.wikipedia.org/wiki/MNIST_database>`__ vÃ  xÃ¡c Ä‘á»‹nh torch ``DataLoader``\\ s (`link <https://pytorch.org/docs/stable/data.html>`__) Ä‘á»ƒ Ä‘Ã o táº¡o vÃ  kiá»ƒm tra."

#: ../../tutorials/05_torch_connector.ipynb:1398
msgid "If we perform a quick visualization we can see that the train dataset consists of images of handwritten 0s and 1s."
msgstr "Náº¿u chÃºng ta thá»±c hiá»‡n má»™t hiá»ƒn thá»‹ nhanh, chÃºng ta cÃ³ thá»ƒ tháº¥y ráº±ng táº­p dá»¯ liá»‡u Ä‘Ã o táº¡o bao gá»“m cÃ¡c hÃ¬nh áº£nh cá»§a cÃ¡c sá»‘ 0 vÃ  1 Ä‘Æ°á»£c viáº¿t tay."

#: ../../tutorials/05_torch_connector.ipynb:1472
msgid "Step 2: Defining the QNN and Hybrid Model"
msgstr "BÆ°á»›c 2: XÃ¡c Ä‘á»‹nh QNN vÃ  Hybrid Model (mÃ´ hÃ¬nh lai)"

#: ../../tutorials/05_torch_connector.ipynb:1483
msgid "This second step shows the power of the ``TorchConnector``. After defining our quantum neural network layer (in this case, a ``EstimatorQNN``), we can embed it into a layer in our torch ``Module`` by initializing a torch connector as ``TorchConnector(qnn)``."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:1485
msgid "**âš ï¸ Attention:** In order to have an adequate gradient backpropagation in hybrid models, we MUST set the initial parameter ``input_gradients`` to TRUE during the qnn initialization."
msgstr "**âš ï¸ ChÃº Ã½:** Äá»ƒ cÃ³ sá»± lan truyá»n ngÆ°á»£c gradient Ä‘áº§y Ä‘á»§ trong cÃ¡c mÃ´ hÃ¬nh káº¿t há»£p, chÃºng ta PHáº¢I Ä‘áº·t tham sá»‘ ban Ä‘áº§u ``input_gradients`` thÃ nh TRUE trong quÃ¡ trÃ¬nh khá»Ÿi táº¡o qnn."

#: ../../tutorials/05_torch_connector.ipynb:1564
msgid "Step 3: Training"
msgstr "BÆ°á»›c 3: ÄÃ o táº¡o"

#: ../../tutorials/05_torch_connector.ipynb:1678
msgid "Now weâ€™ll save the trained model, just to show how a hybrid model can be saved and re-used later for inference. To save and load hybrid models, when using the TorchConnector, follow the PyTorch recommendations of saving and loading the models."
msgstr "BÃ¢y giÆ¡Ì€, chuÌng ta seÌƒ lÆ°u laÌ£i mÃ´ hiÌ€nh Ä‘aÌƒ Ä‘aÌ€o taÌ£o, chiÌ‰ Ä‘ÃªÌ‰ cho biÃªÌt caÌch mÃ´Ì£t mÃ´ hiÌ€nh lai coÌ thÃªÌ‰ lÆ°u vaÌ€ sÆ°Ì‰ duÌ£ng laÌ£i cho nhÆ°Ìƒng suy luÃ¢Ì£n sau naÌ€y. ÄÃªÌ‰ lÆ°u vaÌ€ taÌ‰i caÌc mÃ´ hiÌ€nh lai, khi sÆ°Ì‰ duÌ£ng TorchConnector, nÃªn tuÃ¢n theo caÌc khuyÃªÌn nghiÌ£ PyTorch cuÌ‰a viÃªÌ£c lÆ°u vaÌ€ taÌ‰i caÌc mÃ´ hiÌ€nh."

#: ../../tutorials/05_torch_connector.ipynb:1700
msgid "Step 4: Evaluation"
msgstr "BÆ°á»›c 4: ÄÃ¡nh giÃ¡"

#: ../../tutorials/05_torch_connector.ipynb:1711
msgid "We start from recreating the model and loading the state from the previously saved file. You create a QNN layer using another simulator or a real hardware. So, you can train a model on real hardware available on the cloud and then for inference use a simulator or vice verse. For a sake of simplicity we create a new quantum neural network in the same way as above."
msgstr "ChuÌng ta bÄƒÌt Ä‘Ã¢Ì€u taÌ£o laÌ£i mÃ´ hiÌ€nh vaÌ€ taÌ‰i caÌc traÌ£ng thaÌi tÆ°Ì€ tÃ¢Ì£p tin Ä‘aÌƒ lÆ°u trÆ°Æ¡Ìc Ä‘oÌ. BaÌ£n cÃ¢Ì€n taÌ£o mÃ´Ì£t lÆ¡Ìp QNN sÆ°Ì‰ duÌ£ng mÃ´Ì£t triÌ€nh giaÌ‰ lÃ¢Ì£p khaÌc hoÄƒÌ£c phÃ¢Ì€n cÆ°Ìng thÆ°Ì£c. ViÌ€ vÃ¢Ì£y, baÌ£n coÌ thÃªÌ‰ thÆ°Ì£c tÃ¢Ì£p mÃ´Ì£t mÃ´ hiÌ€nh trÃªn phÃ¢Ì€n cÆ°Ìng thÆ°Ì£c coÌ sÄƒÌƒn trÃªn Ä‘aÌm mÃ¢y vaÌ€ sau Ä‘oÌ sÆ°Ì‰ duÌ£ng cho viÃªÌ£c suy luÃ¢Ì£n hoÄƒÌ£c mÃ´ phoÌ‰ng vaÌ€ ngÆ°Æ¡Ì£c laÌ£i. ÄÃªÌ‰ Ä‘Æ¡n giaÌ‰n hÆ¡n, chuÌng ta taÌ£o mÃ´Ì£t maÌ£ng lÆ°Æ¡Ìi thÃ¢Ì€n kinh lÆ°Æ¡Ì£ng tÆ°Ì‰ mÆ¡Ìi theo caÌch tÆ°Æ¡ng tÆ°Ì£ nhÆ° trÃªn."

#: ../../tutorials/05_torch_connector.ipynb:1859
msgid "ğŸ‰ğŸ‰ğŸ‰ğŸ‰ **You are now able to experiment with your own hybrid datasets and architectures using Qiskit Machine Learning.** **Good Luck!**"
msgstr "ğŸ‰ğŸ‰ğŸ‰ğŸ‰ **BÃ¢y giá» báº¡n cÃ³ thÃªÌ‰ thá»­ nghiÃªÌ£m vá»›i kiÃªÌn trÃºc vÃ  bÃ´Ì£ dá»¯ liá»‡u lai cá»§a riÃªng báº¡n bÄƒÌ€ng cÃ¡ch sÆ°Ì‰ dá»¥ng Qiskit Machine Learning.** **ChÃºc may máº¯n!**"

