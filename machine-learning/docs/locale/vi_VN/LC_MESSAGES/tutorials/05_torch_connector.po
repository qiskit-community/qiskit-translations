msgid ""
msgstr ""
"Project-Id-Version: qiskit-docs\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-02-17 23:23+0000\n"
"PO-Revision-Date: 2022-03-22 13:47\n"
"Last-Translator: \n"
"Language-Team: Vietnamese\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"X-Crowdin-Project: qiskit-docs\n"
"X-Crowdin-Project-ID: 369271\n"
"X-Crowdin-Language: vi\n"
"X-Crowdin-File: /master/machine-learning/docs/locale/en/LC_MESSAGES/tutorials/05_torch_connector.po\n"
"X-Crowdin-File-ID: 9636\n"
"Language: vi_VN\n"

#: ../../tutorials/05_torch_connector.ipynb:9
msgid "This page was generated from `docs/tutorials/05_torch_connector.ipynb`__."
msgstr "Trang n√†y ƒë∆∞∆°Ã£c t·∫°o t·ª´ `docs/tutorials/05_torch_connector.ipynb`__."

#: ../../tutorials/05_torch_connector.ipynb:9
msgid "Torch Connector and Hybrid QNNs"
msgstr "Torch Connector v√† Hybrid QNNs"

#: ../../tutorials/05_torch_connector.ipynb:11
msgid "This tutorial introduces Qiskit‚Äôs ``TorchConnector`` class, and demonstrates how the ``TorchConnector`` allows for a natural integration of any ``NeuralNetwork`` from Qiskit Machine Learning into a PyTorch workflow. ``TorchConnector`` takes a Qiskit ``NeuralNetwork`` and makes it available as a PyTorch ``Module``. The resulting module can be seamlessly incorporated into PyTorch classical architectures and trained jointly without additional considerations, enabling the development and testing of novel **hybrid quantum-classical** machine learning architectures."
msgstr "H∆∞·ªõng d·∫´n n√†y gi·ªõi thi·ªáu l·ªõp ``TorchConnector`` c·ªßa Qiskit v√† tr√¨nh b√†y c√°ch ``TorchConnector`` cho ph√©p t√≠ch h·ª£p t·ª± nhi√™n b·∫•t k·ª≥ ``NeuralNetwork`` n√†o t·ª´ Qiskit Machine Learning v√†o m·ªôt quy tr√¨nh l√†m vi·ªác PyTorch. ``TorchConnector`` s·ª≠ d·ª•ng Qiskit ``NeuralNetwork`` v√† cung c·∫•p n√≥ d∆∞·ªõi d·∫°ng ``Module`` PyTorch. Module k·∫øt qu·∫£ c√≥ th·ªÉ ƒë∆∞·ª£c t√≠ch h·ª£p li·ªÅn m·∫°ch v√†o c√°c ki·∫øn tr√∫c c·ªï ƒëi·ªÉn PyTorch v√† ƒë∆∞·ª£c ƒë√†o t·∫°o chung m√† kh√¥ng c·∫ßn c√¢n nh·∫Øc th√™m, cho ph√©p ph√°t tri·ªÉn v√† th·ª≠ nghi·ªám c√°c ki·∫øn tr√∫c h·ªçc m√°y **lai l∆∞·ª£ng t·ª≠-c·ªï ƒëi·ªÉn** m·ªõi."

#: ../../tutorials/05_torch_connector.ipynb:15
msgid "Content:"
msgstr "N·ªôi dung:"

#: ../../tutorials/05_torch_connector.ipynb:17
msgid "`Part 1: Simple Classification & Regression <#Part-1:-Simple-Classification-&-Regression>`__"
msgstr "`Ph·∫ßn 1: Ph√¢n lo·∫°i ƒë∆°n gi·∫£n & H·ªìi quy <#Part-1:-Simple-Classification-&-Regression>`__"

#: ../../tutorials/05_torch_connector.ipynb:19
msgid "The first part of this tutorial shows how quantum neural networks can be trained using PyTorch‚Äôs automatic differentiation engine (``torch.autograd``, `link <https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html>`__) for simple classification and regression tasks."
msgstr "Ph·∫ßn ƒë·∫ßu ti√™n c·ªßa h∆∞·ªõng d·∫´n n√†y cho th·∫•y c√°ch m·∫°ng neural l∆∞·ª£ng t·ª≠ c√≥ th·ªÉ ƒë∆∞·ª£c ƒë√†o t·∫°o b·∫±ng c√°ch s·ª≠ d·ª•ng c√¥ng c·ª• ph√¢n bi·ªát t·ª± ƒë·ªông c·ªßa PyTorch (``torch.autograd``, `link <https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html>`__) cho c√°c nhi·ªám v·ª• ph√¢n lo·∫°i v√† h·ªìi quy ƒë∆°n gi·∫£n."

#: ../../tutorials/05_torch_connector.ipynb:21
msgid "`Classification <#1.-Classification>`__"
msgstr "`Ph√¢n lo·∫°i <#1.-Classification>`__"

#: ../../tutorials/05_torch_connector.ipynb:23
msgid "Classification with PyTorch and ``OpflowQNN``"
msgstr "Ph√¢n lo·∫°i v∆°ÃÅi PyTorch v√† ``OpflowQNN``"

#: ../../tutorials/05_torch_connector.ipynb:24
msgid "Classification with PyTorch and ``CircuitQNN``"
msgstr "Ph√¢n lo·∫°i v·ªõi PyTorch v√† ``CircuitQNN``"

#: ../../tutorials/05_torch_connector.ipynb:26
msgid "`Regression <#2.-Regression>`__"
msgstr "`H·ªìi quy <#2.-Regression>`__"

#: ../../tutorials/05_torch_connector.ipynb:28
msgid "Regression with PyTorch and ``OpflowQNN``"
msgstr "H·ªìi quy v·ªõi PyTorch v√† ``OpflowQNN``"

#: ../../tutorials/05_torch_connector.ipynb:30
msgid "`Part 2: MNIST Classification, Hybrid QNNs <#Part-2:-MNIST-Classification,-Hybrid-QNNs>`__"
msgstr "`Part 2: Ph√¢n lo·∫°i MNIST, Hybrid QNNs <#Part-2:-MNIST-Classification,-Hybrid-QNNs>`__"

#: ../../tutorials/05_torch_connector.ipynb:32
msgid "The second part of this tutorial illustrates how to embed a (Quantum) ``NeuralNetwork`` into a target PyTorch workflow (in this case, a typical CNN architecture) to classify MNIST data in a hybrid quantum-classical manner."
msgstr "Ph·∫ßn th·ª© hai c·ªßa h∆∞·ªõng d·∫´n n√†y minh h·ªça c√°ch nh√∫ng m·ªôt ``NeuralNetwork`` (L∆∞·ª£ng t·ª≠) v√†o m·ªôt quy tr√¨nh l√†m vi·ªác PyTorch m·ª•c ti√™u (trong tr∆∞·ªùng h·ª£p n√†y l√† ki·∫øn tr√∫c CNN ƒëi·ªÉn h√¨nh) ƒë·ªÉ ph√¢n lo·∫°i d·ªØ li·ªáu MNIST theo c√°ch lai c·ªï ƒëi·ªÉn-l∆∞·ª£ng t·ª≠."

#: ../../tutorials/05_torch_connector.ipynb:85
msgid "Part 1: Simple Classification & Regression"
msgstr "Ph·∫ßn 1: Ph√¢n lo·∫°i ƒë∆°n gi·∫£n v√† H·ªìi quy"

#: ../../tutorials/05_torch_connector.ipynb:97
msgid "1. Classification"
msgstr "1. Ph√¢n lo·∫°i"

#: ../../tutorials/05_torch_connector.ipynb:99
msgid "First, we show how ``TorchConnector`` allows to train a Quantum ``NeuralNetwork`` to solve a classification tasks using PyTorch‚Äôs automatic differentiation engine. In order to illustrate this, we will perform **binary classification** on a randomly generated dataset."
msgstr "ƒê·∫ßu ti√™n, ch√∫ng t√¥i ch·ªâ ra c√°ch m√† ``TorchConnector`` cho ph√©p ƒë√†o t·∫°o m·ªôt ``NeuralNetwork`` l∆∞·ª£ng t·ª≠ ƒë·ªÉ gi·∫£i quy·∫øt c√°c nhi·ªám v·ª• ph√¢n lo·∫°i b·∫±ng c√°ch s·ª≠ d·ª•ng c√¥ng c·ª• ph√¢n bi·ªát t·ª± ƒë·ªông c·ªßa PyTorch. ƒê·ªÉ minh h·ªça cho ƒëi·ªÅu n√†y, ch√∫ng t√¥i s·∫Ω th·ª±c hi·ªán **ph√¢n lo·∫°i nh·ªã ph√¢n** tr√™n m·ªôt t·∫≠p d·ªØ li·ªáu ƒë∆∞·ª£c t·∫°o ng·∫´u nhi√™n."

#: ../../tutorials/05_torch_connector.ipynb:152
msgid "A. Classification with PyTorch and ``OpflowQNN``"
msgstr "A. Ph√¢n lo·∫°i v∆°ÃÅi PyTorch v√† ``OpflowQNN``"

#: ../../tutorials/05_torch_connector.ipynb:154
msgid "Linking an ``OpflowQNN`` to PyTorch is relatively straightforward. Here we illustrate this using the ``TwoLayerQNN``, a sub-case of ``OpflowQNN`` introduced in previous tutorials."
msgstr "Li√™n k·∫øt m·ªôt ``OpflowQNN`` v·ªõi PyTorch t∆∞∆°ng ƒë·ªëi ƒë∆°n gi·∫£n. ·ªû ƒë√¢y ch√∫ng t√¥i minh h·ªça ƒëi·ªÅu n√†y b·∫±ng c√°ch s·ª≠ d·ª•ng ``TwoLayerQNN``, m·ªôt tr∆∞·ªùng h·ª£p con c·ªßa `` OpflowQNN '' ƒë√£ ƒë∆∞·ª£c gi·ªõi thi·ªáu trong c√°c h∆∞·ªõng d·∫´n tr∆∞·ªõc."

#: ../../tutorials/05_torch_connector.ipynb:295
msgid "Optimizer"
msgstr "Tr√¨nh t·ªëi ∆∞u"

#: ../../tutorials/05_torch_connector.ipynb:297
msgid "The choice of optimizer for training any machine learning model can be crucial in determining the success of our training‚Äôs outcome. When using ``TorchConnector``, we get access to all of the optimizer algorithms defined in the [``torch.optim``] package (`link <https://pytorch.org/docs/stable/optim.html>`__). Some of the most famous algorithms used in popular machine learning architectures include *Adam*, *SGD*, or *Adagrad*. However, for this tutorial we will be using the L-BFGS algorithm (``torch.optim.LBFGS``), one of the most well know second-order optimization algorithms for numerical optimization."
msgstr "Vi·ªác l·ª±a ch·ªçn tr√¨nh t·ªëi ∆∞u h√≥a ƒë·ªÉ ƒë√†o t·∫°o b·∫•t k·ª≥ m√¥ h√¨nh h·ªçc m√°y n√†o c√≥ th·ªÉ r·∫•t quan tr·ªçng trong vi·ªác x√°c ƒë·ªãnh s·ª± th√†nh c√¥ng c·ªßa k·∫øt qu·∫£ ƒë√†o t·∫°o. Khi s·ª≠ d·ª•ng ``TorchConnector``, ch√∫ng t√¥i c√≥ quy·ªÅn truy c·∫≠p v√†o t·∫•t c·∫£ c√°c thu·∫≠t to√°n c·ªßa tr√¨nh t·ªëi ∆∞u h√≥a ƒë∆∞·ª£c x√°c ƒë·ªãnh trong g√≥i [``torch.optim``] (`link <https://pytorch.org/docs/stable/optim.html>`__). M·ªôt s·ªë thu·∫≠t to√°n n·ªïi ti·∫øng nh·∫•t ƒë∆∞·ª£c s·ª≠ d·ª•ng trong ki·∫øn tr√∫c h·ªçc m√°y th√¥ng d·ª•ng bao g·ªìm *Adam*, *SGD*, ho·∫∑c *Adagrad*. Tuy nhi√™n, ƒë·ªëi v·ªõi h∆∞·ªõng d·∫´n n√†y, ch√∫ng t√¥i s·∫Ω s·ª≠ d·ª•ng thu·∫≠t to√°n L-BFGS (``torch.optim.LBFGS``), m·ªôt trong nh·ªØng thu·∫≠t to√°n t·ªëi ∆∞u h√≥a b·∫≠c hai ƒë∆∞·ª£c bi·∫øt ƒë·∫øn nhi·ªÅu nh·∫•t ƒë·ªÉ t·ªëi ∆∞u h√≥a s·ªë."

#: ../../tutorials/05_torch_connector.ipynb:301
msgid "Loss Function"
msgstr "H√†m m·∫•t m√°t"

#: ../../tutorials/05_torch_connector.ipynb:303
msgid "As for the loss function, we can also take advantage of PyTorch‚Äôs pre-defined modules from ``torch.nn``, such as the `Cross-Entropy <https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html>`__ or `Mean Squared Error <https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html>`__ losses."
msgstr "ƒê·ªëi v·ªõi loss function (h√†m m·∫•t m√°t), ch√∫ng t√¥i c≈©ng c√≥ th·ªÉ t·∫≠n d·ª•ng c√°c module ƒë∆∞·ª£c x√°c ƒë·ªãnh tr∆∞·ªõc c·ªßa PyTorch t·ª´ ``torch.nn``, ch·∫≥ng h·∫°n nh∆∞ `Cross-Entropy <https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html>`__ ho·∫∑c `Mean Squared Error <https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html>`__."

#: ../../tutorials/05_torch_connector.ipynb:305
msgid "**üí° Clarification :** In classical machine learning, the general rule of thumb is to apply a Cross-Entropy loss to classification tasks, and MSE loss to regression tasks. However, this recommendation is given under the assumption that the output of the classification network is a class probability value in the [0,1] range (usually this is achieved through a Softmax layer). Because the following example for ``TwoLayerQNN`` does not include such layer, and we don‚Äôt apply any mapping to the output (the following section shows an example of application of parity mapping with ``CircuitQNNs``), the QNN‚Äôs output can take any value in the range [-1,1]. In case you were wondering, this is the reason why this particular example uses MSELoss for classification despite it not being the norm (but we encourage you to experiment with different loss functions and see how they can impact training results)."
msgstr "** üí° L√†m r√µ: ** Trong h·ªçc m√°y c·ªï ƒëi·ªÉn, nguy√™n t·∫Øc chung l√† √°p d·ª•ng t·ªïn th·∫•t Cross-Entropy cho c√°c t√°c v·ª• ph√¢n lo·∫°i v√† t·ªïn th·∫•t MSE cho c√°c t√°c v·ª• h·ªìi quy. Tuy nhi√™n, l·ªùi khuy√™n n√†y ƒë∆∞·ª£c ƒë∆∞a ra v·ªõi gi·∫£ ƒë·ªãnh r·∫±ng ƒë·∫ßu ra c·ªßa m·∫°ng ph√¢n lo·∫°i l√† m·ªôt gi√° tr·ªã x√°c su·∫•t l·ªõp trong ph·∫°m vi [0,1] (th∆∞·ªùng ƒëi·ªÅu n√†y ƒë·∫°t ƒë∆∞·ª£c th√¥ng qua m·ªôt l·ªõp Softmax). V√¨ v√≠ d·ª• sau cho ``TwoLayerQNN`` kh√¥ng bao g·ªìm l·ªõp nh∆∞ v·∫≠y v√† ch√∫ng t√¥i kh√¥ng √°p d·ª•ng b·∫•t k·ª≥ √°nh x·∫° n√†o cho ƒë·∫ßu ra (ph·∫ßn sau tr√¨nh b√†y m·ªôt v√≠ d·ª• v·ªÅ ·ª©ng d·ª•ng c·ªßa √°nh x·∫° ch·∫µn l·∫ª v·ªõi ``CircuitQNNs``), ƒë·∫ßu ra c·ªßa QNN c√≥ th·ªÉ nh·∫≠n b·∫•t k·ª≥ gi√° tr·ªã n√†o trong kho·∫£ng [-1,1]. Trong tr∆∞·ªùng h·ª£p b·∫°n th·∫Øc m·∫Øc, ƒë√¢y l√† l√Ω do t·∫°i sao v√≠ d·ª• c·ª• th·ªÉ n√†y s·ª≠ d·ª•ng MSELoss ƒë·ªÉ ph√¢n lo·∫°i m·∫∑c d√π n√≥ kh√¥ng ph·∫£i l√† chu·∫©n (nh∆∞ng ch√∫ng t√¥i khuy·∫øn kh√≠ch b·∫°n th·ª≠ nghi·ªám v·ªõi c√°c h√†m m·∫•t m√°t kh√°c nhau v√† xem ch√∫ng c√≥ th·ªÉ t√°c ƒë·ªông nh∆∞ th·∫ø n√†o ƒë·∫øn k·∫øt qu·∫£ ƒë√†o t·∫°o)."

#: ../../tutorials/05_torch_connector.ipynb:512
#: ../../tutorials/05_torch_connector.ipynb:781
msgid "The red circles indicate wrongly classified data points."
msgstr "V√≤ng tr√≤n m√†u ƒë·ªè bi·ªÉu th·ªã c√°c ƒëi·ªÉm d·ªØ li·ªáu b·ªã ph√¢n lo·∫°i sai."

#: ../../tutorials/05_torch_connector.ipynb:524
msgid "B. Classification with PyTorch and ``CircuitQNN``"
msgstr "B. Ph√¢n lo·∫°i v·ªõi PyTorch v√† ``CircuitQNN``"

#: ../../tutorials/05_torch_connector.ipynb:526
msgid "Linking an ``CircuitQNN`` to PyTorch requires a bit more attention than ``OpflowQNN``. Without the correct setup, backpropagation is not possible."
msgstr "Vi·ªác li√™n k·∫øt m·ªôt ``CircuitQNN`` v·ªõi PyTorch c·∫ßn ch√∫ √Ω h∆°n m·ªôt ch√∫t so v·ªõi ``OpflowQNN``. N·∫øu kh√¥ng c√≥ thi·∫øt l·∫≠p ch√≠nh x√°c, gi·∫£i thu·∫≠t lan truy·ªÅn ng∆∞·ª£c l√† kh√¥ng th·ªÉ."

#: ../../tutorials/05_torch_connector.ipynb:528
msgid "In particular, we must make sure that we are returning a dense array of probabilities in the network‚Äôs forward pass (``sparse=False``). This parameter is set up to ``False`` by default, so we just have to make sure that it has not been changed."
msgstr "ƒê·∫∑c bi·ªát, ch√∫ng t√¥i ph·∫£i ƒë·∫£m b·∫£o r·∫±ng ch√∫ng t√¥i ƒëang tr·∫£ v·ªÅ m·ªôt m·∫£ng d√†y ƒë·∫∑c c√°c x√°c su·∫•t trong t√≠nh to√°n tr·ª±c ti·∫øp c·ªßa m·∫°ng (``sparse=False``). Tham s·ªë n√†y ƒë∆∞·ª£c thi·∫øt l·∫≠p m·∫∑c ƒë·ªãnh th√†nh `False``, v√¨ v·∫≠y ch√∫ng t√¥i ch·ªâ c·∫ßn ƒë·∫£m b·∫£o r·∫±ng n√≥ kh√¥ng b·ªã thay ƒë·ªïi."

#: ../../tutorials/05_torch_connector.ipynb:530
msgid "**‚ö†Ô∏è Attention:** If we define a custom interpret function ( in the example: ``parity``), we must remember to explicitly provide the desired output shape ( in the example: ``2``). For more info on the initial parameter setup for ``CircuitQNN``, please check out the `official qiskit documentation <https://qiskit.org/documentation/machine-learning/stubs/qiskit_machine_learning.neural_networks.CircuitQNN.html>`__."
msgstr "**‚ö†Ô∏è Ch√∫ √Ω:** N·∫øu ch√∫ng ta x√°c ƒë·ªãnh m·ªôt h√†m th√¥ng d·ªãch t√πy ch·ªânh (trong v√≠ d·ª•: ``parity``), ch√∫ng ta ph·∫£i nh·ªõ cung c·∫•p r√µ r√†ng h√¨nh d·∫°ng ƒë·∫ßu ra mong mu·ªën (trong v√≠ d·ª•: ``2``). ƒê·ªÉ bi·∫øt th√™m th√¥ng tin v·ªÅ thi·∫øt l·∫≠p tham s·ªë ban ƒë·∫ßu cho ``CircuitQNN``, vui l√≤ng xem `t√†i li·ªáu qiskit ch√≠nh th·ª©c <https://qiskit.org/documentation/machine-learning/stubs/qiskit_machine_learning.neural_networks.CircuitQNN.html>`__."

#: ../../tutorials/05_torch_connector.ipynb:602
#: ../../tutorials/05_torch_connector.ipynb:930
msgid "For a reminder on optimizer and loss function choices, you can go back to `this section <#Optimizer>`__."
msgstr "ƒê·ªÉ c√≥ l·ªùi nh·∫Øc v·ªÅ c√°c l·ª±a ch·ªçn tr√¨nh t·ªëi ∆∞u h√≥a v√† h√†m m·∫•t m√°t, b·∫°n c√≥ th·ªÉ quay l·∫°i `ph·∫ßn n√†y <#Optimizer>`__."

#: ../../tutorials/05_torch_connector.ipynb:793
msgid "2. Regression"
msgstr "2. H·ªìi quy"

#: ../../tutorials/05_torch_connector.ipynb:795
msgid "We use a model based on the ``TwoLayerQNN`` to also illustrate how to perform a regression task. The chosen dataset in this case is randomly generated following a sine wave."
msgstr "Ch√∫ng t√¥i s·ª≠ d·ª•ng m√¥ h√¨nh d·ª±a tr√™n ``TwoLayerQNN`` ƒë·ªÉ minh h·ªça c√°ch th·ª±c hi·ªán t√°c v·ª• h·ªìi quy. T·∫≠p d·ªØ li·ªáu ƒë√£ ch·ªçn trong tr∆∞·ªùng h·ª£p n√†y ƒë∆∞·ª£c t·∫°o ng·∫´u nhi√™n theo s√≥ng sin."

#: ../../tutorials/05_torch_connector.ipynb:836
msgid "A. Regression with PyTorch and ``OpflowQNN``"
msgstr "A. H·ªìi quy v·ªõi PyTorch v√† ``OpflowQNN``"

#: ../../tutorials/05_torch_connector.ipynb:847
msgid "The network definition and training loop will be analogous to those of the classification task using ``TwoLayerQNN``. In this case, we define our own feature map and ansatz, instead of using the default values."
msgstr "ƒê·ªãnh nghƒ©a m·∫°ng v√† v√≤ng l·∫∑p ƒë√†o t·∫°o s·∫Ω t∆∞∆°ng t·ª± v·ªõi c√°c ƒë·ªãnh nghƒ©a c·ªßa t√°c v·ª• ph√¢n lo·∫°i b·∫±ng s·ª≠ d·ª•ng ``TwoLayerQNN``. Trong tr∆∞·ªùng h·ª£p n√†y, ch√∫ng t√¥i x√°c ƒë·ªãnh b·∫£n ƒë·ªì t√≠nh nƒÉng v√† ansatz c·ªßa ri√™ng m√¨nh, thay v√¨ s·ª≠ d·ª•ng c√°c gi√° tr·ªã m·∫∑c ƒë·ªãnh."

#: ../../tutorials/05_torch_connector.ipynb:1067
msgid "Part 2: MNIST Classification, Hybrid QNNs"
msgstr "Ph·∫ßn 2: Ph√¢n lo·∫°i MNIST, c√°c QNN lai"

#: ../../tutorials/05_torch_connector.ipynb:1069
msgid "In this second part, we show how to leverage a hybrid quantum-classical neural network using ``TorchConnector``, to perform a more complex image classification task on the MNIST handwritten digits dataset."
msgstr "Trong ph·∫ßn th·ª© hai n√†y, ch√∫ng t√¥i ch·ªâ ra c√°ch t·∫≠n d·ª•ng m·ªôt neural network c·ªï ƒëi·ªÉn-l∆∞·ª£ng t·ª≠ lai b·∫±ng c√°ch s·ª≠ d·ª•ng ``TorchConnector``, ƒë·ªÉ th·ª±c hi·ªán nhi·ªám v·ª• ph√¢n lo·∫°i h√¨nh ·∫£nh ph·ª©c t·∫°p h∆°n tr√™n t·∫≠p d·ªØ li·ªáu ch·ªØ s·ªë vi·∫øt tay MNIST."

#: ../../tutorials/05_torch_connector.ipynb:1071
msgid "For a more detailed (pre-``TorchConnector``) explanation on hybrid quantum-classical neural networks, you can check out the corresponding section in the `Qiskit Textbook <https://qiskit.org/textbook/ch-machine-learning/machine-learning-qiskit-pytorch.html>`__."
msgstr "ƒê·ªÉ c√≥ gi·∫£i th√≠ch chi ti·∫øt h∆°n (tr∆∞·ªõc ``TorchConnector``) v·ªÅ neural network c·ªï ƒëi·ªÉn-l∆∞·ª£ng t·ª≠ lai, b·∫°n c√≥ th·ªÉ xem ph·∫ßn t∆∞∆°ng ·ª©ng trong `Qiskit Textbook <https://qiskit.org/textbook/ch-machine-learning/machine-learning-qiskit-pytorch.html>`__."

#: ../../tutorials/05_torch_connector.ipynb:1109
msgid "Step 1: Defining Data-loaders for train and test"
msgstr "B∆∞·ªõc 1: X√°c ƒë·ªãnh B·ªô t·∫£i d·ªØ li·ªáu cho ƒë√†o t·∫°o v√† ki·ªÉm tra"

#: ../../tutorials/05_torch_connector.ipynb:1120
msgid "We take advantage of the ``torchvision`` `API <https://pytorch.org/vision/stable/datasets.html>`__ to directly load a subset of the `MNIST dataset <https://en.wikipedia.org/wiki/MNIST_database>`__ and define torch ``DataLoader``\\ s (`link <https://pytorch.org/docs/stable/data.html>`__) for train and test."
msgstr "Ch√∫ng t√¥i t·∫≠n d·ª•ng ``torchvision`` `API <https://pytorch.org/vision/stable/datasets.html>`__ ƒë·ªÉ t·∫£i tr·ª±c ti·∫øp m·ªôt t·∫≠p con c·ªßa `T·∫≠p d·ªØ li·ªáu MNIST <https://en.wikipedia.org/wiki/MNIST_database>`__ v√† x√°c ƒë·ªãnh torch ``DataLoader``\\ s (`link <https://pytorch.org/docs/stable/data.html>`__) ƒë·ªÉ ƒë√†o t·∫°o v√† ki·ªÉm tra."

#: ../../tutorials/05_torch_connector.ipynb:1163
msgid "If we perform a quick visualization we can see that the train dataset consists of images of handwritten 0s and 1s."
msgstr "N·∫øu ch√∫ng ta th·ª±c hi·ªán m·ªôt hi·ªÉn th·ªã nhanh, ch√∫ng ta c√≥ th·ªÉ th·∫•y r·∫±ng t·∫≠p d·ªØ li·ªáu ƒë√†o t·∫°o bao g·ªìm c√°c h√¨nh ·∫£nh c·ªßa c√°c s·ªë 0 v√† 1 ƒë∆∞·ª£c vi·∫øt tay."

#: ../../tutorials/05_torch_connector.ipynb:1237
msgid "Step 2: Defining the QNN and Hybrid Model"
msgstr "B∆∞·ªõc 2: X√°c ƒë·ªãnh QNN v√† Hybrid Model (m√¥ h√¨nh lai)"

#: ../../tutorials/05_torch_connector.ipynb:1248
msgid "This second step shows the power of the ``TorchConnector``. After defining our quantum neural network layer (in this case, a ``TwoLayerQNN``), we can embed it into a layer in our torch ``Module`` by initializing a torch connector as ``TorchConnector(qnn)``."
msgstr "B∆∞·ªõc th·ª© hai n√†y cho th·∫•y s·ª©c m·∫°nh c·ªßa ``TorchConnector``. Sau khi x√°c ƒë·ªãnh l·ªõp neural network l∆∞·ª£ng t·ª≠ c·ªßa ch√∫ng t√¥i (trong tr∆∞·ªùng h·ª£p n√†y l√† m·ªôt ``TwoLayerQNN``), ch√∫ng t√¥i c√≥ th·ªÉ nh√∫ng n√≥ v√†o m·ªôt l·ªõp trong ``Module`` torch b·∫±ng c√°ch kh·ªüi t·∫°o m·ªôt ƒë·∫ßu n·ªëi torch ``TorchConnector(qnn)``."

#: ../../tutorials/05_torch_connector.ipynb:1250
msgid "**‚ö†Ô∏è Attention:** In order to have an adequate gradient backpropagation in hybrid models, we MUST set the initial parameter ``input_gradients`` to TRUE during the qnn initialization."
msgstr "**‚ö†Ô∏è Ch√∫ √Ω:** ƒê·ªÉ c√≥ s·ª± lan truy·ªÅn ng∆∞·ª£c gradient ƒë·∫ßy ƒë·ªß trong c√°c m√¥ h√¨nh k·∫øt h·ª£p, ch√∫ng ta PH·∫¢I ƒë·∫∑t tham s·ªë ban ƒë·∫ßu ``input_gradients`` th√†nh TRUE trong qu√° tr√¨nh kh·ªüi t·∫°o qnn."

#: ../../tutorials/05_torch_connector.ipynb:1368
msgid "Step 3: Training"
msgstr "B∆∞·ªõc 3: ƒê√†o t·∫°o"

#: ../../tutorials/05_torch_connector.ipynb:1482
msgid "Step 4: Evaluation"
msgstr "B∆∞·ªõc 4: ƒê√°nh gi√°"

#: ../../tutorials/05_torch_connector.ipynb:1592
msgid "üéâüéâüéâüéâ **You are now able to experiment with your own hybrid datasets and architectures using Qiskit Machine Learning.** **Good Luck!**"
msgstr "üéâüéâüéâüéâ **B√¢y gi·ªù b·∫°n c√≥ th√™Ãâ th·ª≠ nghi√™Ã£m v·ªõi ki√™ÃÅn tr√∫c v√† b√¥Ã£ d·ªØ li·ªáu lai c·ªßa ri√™ng b·∫°n bƒÉÃÄng c√°ch s∆∞Ãâ d·ª•ng Qiskit Machine Learning.** **Ch√∫c may m·∫Øn!**"

