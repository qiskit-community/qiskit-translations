msgid ""
msgstr ""
"Project-Id-Version: qiskit-docs\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-06-01 19:11+0000\n"
"PO-Revision-Date: 2023-08-14 19:37\n"
"Last-Translator: \n"
"Language: bn_BN\n"
"Language-Team: Bengali Language\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"
"X-Crowdin-Project: qiskit-docs\n"
"X-Crowdin-Project-ID: 369271\n"
"X-Crowdin-Language: bengali\n"
"X-Crowdin-File: /main/machine-learning/docs/locale/en/LC_MESSAGES/tutorials/04_torch_qgan.po\n"
"X-Crowdin-File-ID: 9885\n"

#: ../../tutorials/04_torch_qgan.ipynb:9
msgid "This page was generated from `docs/tutorials/04_torch_qgan.ipynb`__."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:9
msgid "PyTorch qGAN Implementation"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:12
msgid "Overview"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:14
msgid "This tutorial introduces step-by-step how to build a PyTorch-based Quantum Generative Adversarial Network algorithm."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:16
msgid "The tutorial is structured as follows:"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:18
msgid "`Introduction <#1.-Introduction>`__"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:19
msgid "`Data and Represtation <#2.-Data-and-Representation>`__"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:20
msgid "`Definitions of the Neural Networks <#3.-Definitions-of-the-Neural-Networks>`__"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:21
msgid "`Setting up Parameters for the Training Loop <#4.-Setting-up-the-Training-Loop>`__"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:22
msgid "`Model Training <#5.-Model-Training>`__"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:23
msgid "`Results: Cumulative Density Functions <#6.-Results:-Cumulative-Density-Functions>`__"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:24
msgid "`Conclusion <#7.-Conclusion>`__"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:27
msgid "1. Introduction"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:29
msgid "The qGAN [1] is a hybrid quantum-classical algorithm used for generative modeling tasks. The algorithm uses the interplay of a quantum generator :math:`G_{\\theta}`, i.e., an ansatz (parametrized quantum circuit), and a classical discriminator :math:`D_{\\phi}`, a neural network, to learn the underlying probability distribution given training data."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:31
msgid "The generator and discriminator are trained in alternating optimization steps, where the generator aims at generating probabilities that will be classified by the discriminator as training data values (i.e, probabilities from the real training distribution), and the discriminator tries to differentiate between original distribution and probabilities from the generator (in other words, telling apart the real and generated distributions). The final goal is for the quantum generator to learn a representation for the target probability distribution. The trained quantum generator can, thus, be used to load a quantum state which is an approximate model of the target distribution."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:34
msgid "**References:**"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:36
msgid "[1] Zoufal et al., `Quantum Generative Adversarial Networks for learning and loading random distributions <https://www.nature.com/articles/s41534-019-0223-2>`__"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:39
msgid "1.1. qGANs for Loading Random Distributions"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:41
msgid "Given :math:`k`-dimensional data samples, we employ a quantum Generative Adversarial Network (qGAN) to learn a random distribution and to load it directly into a quantum state:"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:43
msgid "\\big| g_{\\theta}\\rangle = \\sum_{j=0}^{2^n-1} \\sqrt{p_{\\theta}^{j}}\\big| j \\rangle\n\n"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:45
msgid "where :math:`p_{\\theta}^{j}` describe the occurrence probabilities of the basis states :math:`\\big| j\\rangle`."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:47
msgid "The aim of the qGAN training is to generate a state :math:`\\big| g_{\\theta}\\rangle` where :math:`p_{\\theta}^{j}`, for :math:`j\\in \\left\\{0, \\ldots, {2^n-1} \\right\\}`, describe a probability distribution that is close to the distribution underlying the training data :math:`X=\\left\\{x^0, \\ldots, x^{k-1} \\right\\}`."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:49
msgid "For further details please refer to `Quantum Generative Adversarial Networks for Learning and Loading Random Distributions <https://arxiv.org/abs/1904.00043>`__ *Zoufal, Lucchi, Woerner* [2019]."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:51
msgid "For an example of how to use a trained qGAN in an application, the pricing of financial derivatives, please see the `Option Pricing with qGANs <https://github.com/Qiskit/qiskit-finance/tree/main/docs/tutorials/10_qgan_option_pricing.ipynb>`__ tutorial."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:54
msgid "2. Data and Representation"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:56
msgid "First, we need to load our training data :math:`X`."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:58
msgid "In this tutorial, the training data is given by a 2D multivariate normal distribution."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:60
msgid "The goal of the generator is to learn how to represent such distribution, and the trained generator should correspond to an :math:`n`-qubit quantum state :nbsphinx-math:`\\begin{equation} |g_{\\text{trained}}\\rangle=\\sum\\limits_{j=0}^{k-1}\\sqrt{p_{j}}|x_{j}\\rangle, \\end{equation}` where the basis states :math:`|x_{j}\\rangle` represent the data items in the training data set :math:`X={x_0, \\ldots, x_{k-1}}` with :math:`k\\leq 2^n` and :math:`p_j` refers to the sampling probability of :math:`|x_{j}\\rangle`."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:64
msgid "To facilitate this representation, we need to map the samples from the multivariate normal distribution to discrete values. The number of values that can be represented depends on the number of qubits used for the mapping. Hence, the data resolution is defined by the number of qubits. If we use :math:`3` qubits to represent one feature, we have :math:`2^3 = 8` discrete values."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:66
msgid "We first begin by fixing seeds in the random number generators for reproducibility of the outcome in this tutorial."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:91
msgid "We fix the number of dimensions, the discretization number and compute the number of qubits required as :math:`2^3 = 8`."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:116
msgid "Then, we prepare a discrete distribution from the continuous 2D normal distribution. We evaluate the continuous probability density function (PDF) on the grid :math:`(-2, 2)^2` with a discretization of :math:`8` values per feature. Thus, we have :math:`64` values of the PDF. Since this will be a discrete distribution we normalize the obtained probabilities."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:143
msgid "Let's visualize our distribution. It is a nice bell-shaped bivariate normal distribution on a discrete grid."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:183
msgid "3. Definitions of the Neural Networks"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:185
msgid "In this section we define two neural networks as described above:"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:187
msgid "A quantum generator as a quantum neural network."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:188
msgid "A classical discriminator as a PyTorch-based neural network."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:191
msgid "3.1. Definition of the quantum neural network ansatz"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:193
msgid "Now, we define the parameterized quantum circuit :math:`G\\left(\\boldsymbol{\\theta}\\right)` with :math:`\\boldsymbol{\\theta} = {\\theta_1, ..., \\theta_k}` which will be used in our quantum generator."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:195
msgid "To implement the quantum generator, we choose a hardware efficient ansatz with :math:`6` repetitions. The ansatz implements :math:`R_Y`, :math:`R_Z` rotations and :math:`CX` gates which takes a uniform distribution as an input state. Notably, for :math:`k>1` the generator's parameters must be chosen carefully. For example, the circuit depth should be more than :math:`1` because higher circuit depths enable the representation of more complex structures. Here, we construct quite a deep circuit with a large number of parameters to be able to adequately capture and represent the distribution."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:224
msgid "Let's draw our circuit and see what it looks like. On the plot we may notice a pattern that appears :math:`6` times."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:254
msgid "Let's print the number of trainable parameters."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:301
msgid "3.2. Definition of the quantum generator"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:303
msgid "We start defining the generator by creating a sampler for the ansatz. The reference implementation is a statevector-based implementation, thus it returns exact probabilities as a result of circuit execution. We add the ``shots`` parameter to add some noise to the results. In this case the implementation samples probabilities from the multinomial distribution constructed from the measured quasi probabilities. And as usual we fix the seed for reproducibility purposes."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:327
msgid "Next, we define a function that creates the quantum generator from a given parameterized quantum circuit. Inside this function we create a neural network that returns the quasi probability distribution evaluated by the underlying Sampler. We fix ``initial_weights`` for reproducibility purposes. In the end we wrap the created quantum neural network in ``TorchConnector`` to make use of PyTorch-based training."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:363
msgid "3.3. Definition of the classical discriminator"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:365
msgid "Next, we define a PyTorch-based classical neural network that represents the classical discriminator. The underlying gradients can be automatically computed with PyTorch."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:404
msgid "3.4. Create a generator and a discriminator"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:406
msgid "Now we create a generator and a discriminator."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:429
msgid "4. Setting up the Training Loop"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:431
msgid "In this section we set up:"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:433
msgid "A loss function for the generator and discriminator."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:434
msgid "Optimizers for both."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:435
msgid "A utility plotting function to visualize training process."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:438
msgid "4.1. Definition of the loss functions"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:440
msgid "We want to train the generator and the discriminator with binary cross entropy as the loss function:"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:442
msgid "L\\left(\\boldsymbol{\\theta}\\right)=\\sum_jp_j\\left(\\boldsymbol{\\theta}\\right)\\left[y_j\\log(x_j) + (1-y_j)\\log(1-x_j)\\right],\n\n"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:444
msgid "where :math:`x_j` refers to a data sample and :math:`y_j` to the corresponding label."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:446
msgid "Since PyTorch's ``binary_cross_entropy`` is not differentiable with respect to weights, we implement the loss function manually to be able to evaluate gradients."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:472
msgid "4.2. Definition of the optimizers"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:474
msgid "In order to train the generator and discriminator, we need to define optimization schemes. In the following, we employ a momentum based optimizer called Adam, see `Kingma et al., Adam: A method for stochastic optimization <https://arxiv.org/abs/1412.6980>`__ for more details."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:505
msgid "4.3. Visualization of the training process"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:507
msgid "We will visualize what is happening during the training by plotting the evolution of the generator's and the discriminator's loss functions during the training, as well as the progress in the relative entropy between the trained and the target distribution. We define a function that plots the loss functions and relative entropy. We call this function once an epoch of training is complete."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:509
msgid "Visualization of the training process begins when training data is collected across two epochs."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:558
msgid "5. Model Training"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:560
msgid "In the training loop we monitor not only loss functions, but relative entropy as well. The relative entropy describes a distance metric for distributions. Hence, we can use it to benchmark how close/far away the trained distribution is from the target distribution."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:562
msgid "Now, we are ready to train our model. It may take some time to train the model so be patient."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:673
msgid "6. Results: Cumulative Density Functions"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:675
msgid "In this section we compare the cumulative distribution function (CDF) of the trained distribution to the CDF of the target distribution."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:677
msgid "First, we generate a new probability distribution with PyTorch autograd turned off as we are not going to train the model anymore."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:699
msgid "And then, we plot the cumulative distribution functions of the generated distribution, original distribution, and the difference between them. Please, be careful, the scale on the third plot **is not the same** as on the first and second plot, and the actual difference between the two plotted CDFs is pretty small."
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:754
msgid "7. Conclusion"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:756
msgid "Quantum generative adversarial networks employ the interplay of a generator and discriminator to map an approximate representation of a probability distribution underlying given data samples into a quantum channel. This tutorial presents a self-standing PyTorch-based qGAN implementation where the generator is given by a quantum channel, i.e., a variational quantum circuit, and the discriminator by a classical neural network, and discusses the application of efficient learning and loading of generic probability distributions into quantum states. The loading requires :math:`\\mathscr{O}\\left(poly\\left(n\\right)\\right)` gates and can thus enable the use of potentially advantageous quantum algorithms."
msgstr ""

