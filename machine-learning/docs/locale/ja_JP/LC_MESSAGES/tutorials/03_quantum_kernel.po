msgid ""
msgstr ""
"Project-Id-Version: qiskit-docs\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-05-09 10:21+0000\n"
"PO-Revision-Date: 2023-08-26 12:49\n"
"Last-Translator: \n"
"Language: ja\n"
"Language-Team: Japanese\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"
"X-Crowdin-Project: qiskit-docs\n"
"X-Crowdin-Project-ID: 369271\n"
"X-Crowdin-Language: ja\n"
"X-Crowdin-File: /main/machine-learning/docs/locale/en/LC_MESSAGES/tutorials/03_quantum_kernel.po\n"
"X-Crowdin-File-ID: 9632\n"

#: ../../tutorials/03_quantum_kernel.ipynb:9
msgid "This page was generated from `docs/tutorials/03_quantum_kernel.ipynb`__."
msgstr "このページは `docs/tutorials/03_quantum_kernel.ipynb`__ から生成されました。"

#: ../../tutorials/03_quantum_kernel.ipynb:9
msgid "Quantum Kernel Machine Learning"
msgstr "量子カーネル法機械学習"

#: ../../tutorials/03_quantum_kernel.ipynb:12
msgid "Overview"
msgstr "概要"

#: ../../tutorials/03_quantum_kernel.ipynb:14
msgid "The general task of machine learning is to find and study patterns in data. For many datasets, the datapoints are better understood in a higher dimensional feature space. This is the fundamental principle behind a series of machine learning algorithms known as *kernel methods*."
msgstr "機械学習の一般的なタスクは、データのパターンを見つけて学習することです。 多くのデータセットでは、データポイントは高次元の特徴空間でよりよく理解されます。 これは、*カーネル・メソッド*として知られる一連の機械学習アルゴリズムの背後にある基本原理です。"

#: ../../tutorials/03_quantum_kernel.ipynb:16
msgid "In this notebook, you will learn how to define quantum kernels using ``qiskit-machine-learning`` and how these can be plugged into different algorithms to solve classification and clustering problems."
msgstr "このノートブックでは、 ``qiskit-machine-learning`` を使用して量子カーネルを定義する方法と、これらをさまざまなアルゴリズムにプラグインして分類とクラスタリングの問題を解決する方法を学習します。"

#: ../../tutorials/03_quantum_kernel.ipynb:18
msgid "All examples used in this tutorial are based on this reference paper: `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__."
msgstr "このチュートリアルで使用されているすべての例は、次のリファレンス ペーパーに基づいています。 `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__"

#: ../../tutorials/03_quantum_kernel.ipynb:20
msgid "The content is structured as follows:"
msgstr "このコンテンツの構造は以下のとおりです。"

#: ../../tutorials/03_quantum_kernel.ipynb:22
msgid "`Introduction <#1.-Introduction>`__"
msgstr "`はじめに <#1.-Introduction>`__"

#: ../../tutorials/03_quantum_kernel.ipynb:23
msgid "`Classification <#2.-Classification>`__"
msgstr "`分類 <#2.-Classification>`__"

#: ../../tutorials/03_quantum_kernel.ipynb:24
msgid "`Clustering <#3.-Clustering>`__"
msgstr "`クラスタリング <#3.-Clustering>`__"

#: ../../tutorials/03_quantum_kernel.ipynb:25
msgid "`Kernel Principal Components Analysis <#4.-Kernel-Principal-Component-Analysis>`__"
msgstr "`Kernel Principal Components Analysis <#4.-Kernel-Principal-Component-Analysis>`__"

#: ../../tutorials/03_quantum_kernel.ipynb:26
msgid "`Conclusion <#5.-Conclusion>`__"
msgstr "`結論 <#5.-Conclusion>`__"

#: ../../tutorials/03_quantum_kernel.ipynb:38
msgid "1. Introduction"
msgstr "1. はじめに"

#: ../../tutorials/03_quantum_kernel.ipynb:41
msgid "1.1. Kernel Methods for Machine Learning"
msgstr "1.1. 機械学習のカーネル・メソッド"

#: ../../tutorials/03_quantum_kernel.ipynb:43
msgid "Kernel methods are a collection of pattern analysis algorithms that use kernel functions to operate in a high-dimensional feature space. The best-known application of kernel methods is in **Support Vector Machines (SVMs)**, supervised learning algorithms commonly used for classification tasks. The main goal of SVMs is to find decision boundaries to separate a given set of data points into classes. When these data spaces are not linearly separable, SVMs can benefit from the use of kernels to find these boundaries."
msgstr "カーネル法は、高次元特徴量空間で動作するカーネル関数を使用するパターン分析アルゴリズムのコレクションです。カーネル法の最もよく知られた応用は **サポートベクターマシン（SVM）** で、分類タスクによく使われる教師あり学習アルゴリズムです。SVMの主な目的は、与えられたデータ・ポイントをクラスに分離するための決定境界を見つけることです。このデータ空間が線形分離可能でない場合、SVMはこれらの境界を見つけるためにカーネルを使用することで利益を得ることができます。"

#: ../../tutorials/03_quantum_kernel.ipynb:46
msgid "Formally, decision boundaries are hyperplanes in a high dimensional space. The kernel function implicitly maps input data into this higher dimensional space, where it can be easier to solve the initial problem. In other words, kernels may allow data distributions that were originally non-linearly separable to become a linearly separable problem. This is an effect known as the \"kernel trick\"."
msgstr "形式的には、決定境界は高次元空間の超平面です。カーネル関数は、入力データをこの高次元空間に暗黙的にマッピングし、与えられた問題をより簡単に解くことがでます。言い換えれば、カーネルは元々非線形分離可能であったデータ分布を線形分離可能な問題にすることができます。これは「カーネル・トリック」として知られる効果です。"

#: ../../tutorials/03_quantum_kernel.ipynb:48
msgid "There are use-cases for kernel-based unsupervised algorithms too, for example, in the context of clustering. **Spectral Clustering** is a technique where data points are treated as nodes of a graph, and the clustering task is viewed as a graph partitioning problem where nodes are mapped to a space where they can be easily segregated to form clusters."
msgstr "カーネルベースの教師なしアルゴリズムにも、例えばクラスタリングの文脈で使われるケースがあります。 **スペクトル・クラスタリング** は、データ点をグラフのノードとして扱う手法であり、クラスタリング・タスクは、ノードがクラスタを形成するために容易に分離できる空間にマッピングされるグラフ分割問題として見なされます。"

#: ../../tutorials/03_quantum_kernel.ipynb:51
msgid "1.2. Kernel Functions"
msgstr "1.2 . カーネル関数"

#: ../../tutorials/03_quantum_kernel.ipynb:53
msgid "Mathematically, kernel functions follow:"
msgstr "数学的には、カーネル関数は次のとおりです："

#: ../../tutorials/03_quantum_kernel.ipynb:55
msgid ":math:`k(\\vec{x}_i, \\vec{x}_j) = \\langle f(\\vec{x}_i), f(\\vec{x}_j) \\rangle`"
msgstr ":math:`k(\\vec{x}_i, \\vec{x}_j) = \\langle f(\\vec{x}_i), f(\\vec{x}_j) \\rangle`"

#: ../../tutorials/03_quantum_kernel.ipynb:57
msgid "where \\* :math:`k` is the kernel function \\* :math:`\\vec{x}_i, \\vec{x}_j` are :math:`n` dimensional inputs \\* :math:`f` is a map from :math:`n`-dimension to :math:`m`-dimension space and \\* :math:`\\langle a,b \\rangle` denotes the inner product"
msgstr "ここで、 \\* :math:`k` はカーネル関数、 \\* :math:`\\vec{x}_i, \\vec{x}_j` は :math:`n` 次元の入力、 \\* :math:`f` は :math:`n` 次元から :math:`m` 次元の空間への写像、 \\* :math:`\\langle a,b \\rangle` は内積を表します。"

#: ../../tutorials/03_quantum_kernel.ipynb:59
msgid "When considering finite data, a kernel function can be represented as a matrix:"
msgstr "有限データを考慮する場合、カーネル関数は次のような行列として表現することができます。"

#: ../../tutorials/03_quantum_kernel.ipynb:61
msgid ":math:`K_{ij} = k(\\vec{x}_i,\\vec{x}_j)`."
msgstr ":math:`K_{ij} = k(\\vec{x}_i,\\vec{x}_j)`."

#: ../../tutorials/03_quantum_kernel.ipynb:64
msgid "1.3. Quantum Kernels"
msgstr "1.3 量子カーネル"

#: ../../tutorials/03_quantum_kernel.ipynb:66
msgid "The main idea behind quantum kernel machine learning is to leverage quantum feature maps to perform the kernel trick. In this case, the quantum kernel is created by mapping a classical feature vector :math:`\\vec{x}` to a Hilbert space using a quantum feature map :math:`\\phi(\\vec{x})`. Mathematically:"
msgstr "量子カーネル機械学習の背後にある主なアイデアは、量子特徴量マップを活用してカーネル トリックを実行することです。 この場合、量子カーネルは、量子特徴量マップ :math:`\\phi(\\vec{x})` を使用して古典的特徴量ベクトル :math:`\\vec{x}` をヒルベルト空間にマッピングすることによって作成されます。 数学的には以下です:"

#: ../../tutorials/03_quantum_kernel.ipynb:68
msgid ":math:`K_{ij} = \\left| \\langle \\phi(\\vec{x}_i)| \\phi(\\vec{x}_j) \\rangle \\right|^{2}`"
msgstr ":math:`K_{ij} = \\left| \\langle \\phi(\\vec{x}_i)| \\phi(\\vec{x}_j) \\rangle \\right|^{2}`"

#: ../../tutorials/03_quantum_kernel.ipynb:70
msgid "where \\* :math:`K_{ij}` is the kernel matrix \\* :math:`\\vec{x}_i, \\vec{x}_j` are :math:`n` dimensional inputs \\* :math:`\\phi(\\vec{x})` is the quantum feature map \\* :math:`\\left| \\langle a|b \\rangle \\right|^{2}` denotes the overlap of two quantum states :math:`a` and :math:`b`"
msgstr "ここで、 \\* :math:`K_{ij}` はカーネル行列、 \\* :math:`\\vec{x}_i, \\vec{x}_j` は :math:`n` 次元の入力、 \\* :math:`\\phi(\\vec{x})` は量子特徴量マップ、 \\* :math:`\\left| \\langle a|b \\rangle \\right|^{2}` は 2 つの量子状態 :math:`a` と :math:`b` の重なりを表します。"

#: ../../tutorials/03_quantum_kernel.ipynb:72
msgid "Quantum kernels can be plugged into common classical kernel learning algorithms such as SVMs or clustering algorithms, as you will see in the examples below. They can also be leveraged in new quantum kernel methods like ``QSVC`` `class <https://qiskit.org/documentation/machine-learning/stubs/qiskit_machine_learning.algorithms.QSVC.html>`__ provided by ``qiskit-machine-learning`` which is explored in this tutorial, and other methods as shown in later tutorials on `Pegasos QSVC <https://github.com/Qiskit/qiskit-machine-learning/blob/main/docs/tutorials/07_pegasos_qsvc.ipynb>`__ and `QKA <https://github.com/Qiskit/qiskit-machine-learning/blob/main/docs/tutorials/08_quantum_kernel_trainer.ipynb>`__."
msgstr "以下の例でわかるように、量子カーネルは、SVM やクラスタリング アルゴリズムなどの一般的な古典的なカーネル学習アルゴリズムに接続できます。 これらは、このチュートリアルで説明する ``qiskit-machine-learning`` によって提供される ``QSVC`` `class <https://qiskit.org/documentation/machine-learning/stubs/qiskit_machine_learning.algorithms.QSVC.html>`__ のような新しい量子カーネル メソッドや、後の `Pegasos QSVC <https://github.com/Qiskit/qiskit-machine-learning/blob/main/docs/tutorials/07_pegasos_qsvc.ipynb>`__ および量`QKA <https://github.com/Qiskit/qiskit-machine-learning/blob/main/docs/tutorials/08_quantum_kernel_trainer.ipynb>`__ で示される他のメソッドでも利用できます。"

#: ../../tutorials/03_quantum_kernel.ipynb:77
msgid "Before introducing any example, we set up the global seed to ensure reproducibility:"
msgstr "例を紹介する前に、再現性を確保するためにグローバル・シードをセットアップします。"

#: ../../tutorials/03_quantum_kernel.ipynb:101
msgid "2. Classification"
msgstr "2.分類"

#: ../../tutorials/03_quantum_kernel.ipynb:103
msgid "This section illustrates a quantum kernel classification workflow using ``qiskit-machine-learning``."
msgstr "このセクションでは、 ``qiskit-machine-learning`` を使用した量子カーネル分類ワークフローについて説明します。"

#: ../../tutorials/03_quantum_kernel.ipynb:115
msgid "2.1. Defining the dataset"
msgstr "2.1. データセットの定義"

#: ../../tutorials/03_quantum_kernel.ipynb:126
msgid "For this example, we will use the *ad hoc dataset* as described in the reference `paper <https://arxiv.org/pdf/1804.11326.pdf>`__."
msgstr "この例では、 `paper <https://arxiv.org/pdf/1804.11326.pdf>`__ の中で説明されているように、 *ad hoc dataset* を使用します。"

#: ../../tutorials/03_quantum_kernel.ipynb:128
msgid "We can define the dataset dimension and get our train and test subsets:"
msgstr "データセットの次元を定義し、訓練とテストのサブセットを取得します。"

#: ../../tutorials/03_quantum_kernel.ipynb:160
msgid "This dataset is two-dimensional, the two features are represented by the :math:`x` and :math:`y` coordinates, and it has two class labels: A and B. We can plot it and see what the distribution looks like. We define utility functions to plot the dataset."
msgstr "このデータセットは2次元で、 :math:`x` と :math:`y` 座標で表されます。 AとBの2つのクラスラベルがあります。データセットをプロットするためのユーティリティ関数を定義します。"

#: ../../tutorials/03_quantum_kernel.ipynb:227
msgid "Now we actually plot the dataset for classification:"
msgstr "さて分類のためのデータセットを実際にプロットします。"

#: ../../tutorials/03_quantum_kernel.ipynb:257
msgid "2.2. Defining the quantum kernel"
msgstr "2.2. 量子カーネルの定義"

#: ../../tutorials/03_quantum_kernel.ipynb:259
msgid "The next step is to create a quantum kernel instance that will help classify this data."
msgstr "次のステップは、このデータを分類するのに役立つ量子カーネルインスタンスを作成することです。"

#: ../../tutorials/03_quantum_kernel.ipynb:261
msgid "We use the ``FidelityQuantumKernel`` `class <https://github.com/Qiskit/qiskit-machine-learning/blob/main/qiskit_machine_learning/kernels/fidelity_quantum_kernel.py>`__, and pass two input arguments to its constructor:"
msgstr "``FidelityQuantumKernel`` `class <https://github.com/Qiskit/qiskit-machine-learning/blob/main/qiskit_machine_learning/kernels/fidelity_quantum_kernel.py>`__ を使用し、2つの入力引数をコンストラクターに渡します:"

#: ../../tutorials/03_quantum_kernel.ipynb:263
msgid "``feature_map``: in this case, a two-qubit ``ZZFeatureMap`` `instance <https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZZFeatureMap.html>`__."
msgstr "``feature_map``: この場合、2 qubit の ``ZZFeatureMap`` `instance <https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZZFeatureMap.html>`__ です。"

#: ../../tutorials/03_quantum_kernel.ipynb:265
msgid "``fidelity``: in this case, the ``ComputeUncompute`` `fidelity <https://qiskit.org/documentation/stubs/qiskit.algorithms.state_fidelities.ComputeUncompute.html>`__ subroutine that leverages the ``Sampler`` `primitive <https://qiskit.org/documentation/stubs/qiskit.primitives.Sampler.html>`__."
msgstr "``fidelity``: この場合、``ComputeUncompute`` `fidelity <https://qiskit.org/documentation/stubs/qiskit.algorithms.state_fidelities.ComputeUncompute.html>`__ サブルーチンは、``Sampler`` `primitive <https://qiskit.org/documentation/stubs/qiskit.primitives.Sampler.html>`__ を利用します。"

#: ../../tutorials/03_quantum_kernel.ipynb:267
msgid "**NOTE:** If you don't pass a ``Sampler`` or ``Fidelity`` instance, then the instances of the reference ``Sampler`` and ``ComputeUncompute`` classes (found in ``qiskit.primitives``) will be created by default."
msgstr "**注意:** ``Sampler`` または ``Fidelity`` インスタンスを渡さない場合は、``Sampler`` と ``ComputeUncompute`` クラス (``qiskit.primitives`` にあります) はデフォルトで作成されます。"

#: ../../tutorials/03_quantum_kernel.ipynb:300
msgid "2.3. Classification with SVC"
msgstr "2.3. SVCによる分類"

#: ../../tutorials/03_quantum_kernel.ipynb:302
msgid "The quantum kernel can now be plugged into classical kernel methods, such as the ``SVC`` `algorithm <https://scikit-learn.org/stable/modules/svm.html>`__ from ``scikit-learn``. This algorithm allows us to define a `custom kernel <https://scikit-learn.org/stable/modules/svm.html#custom-kernels>`__ in two ways: 1. by providing the kernel as a **callable function** 2. by precomputing the **kernel matrix**"
msgstr "量子カーネルは、 ``scikit-learn`` の ``SVC`` `algorithm <https://scikit-learn.org/stable/modules/svm.html>`__ のような古典的なカーネルメソッドに接続することができます。このアルゴリズムでは、2つの方法で `カスタムカーネル <https://scikit-learn.org/stable/modules/svm.html#custom-kernels>`__ を定義することができます： 1. カーネルを **呼び出し可能な関数** として提供する 2. **カーネル行列** を事前に計算する"

#: ../../tutorials/03_quantum_kernel.ipynb:314
msgid "Kernel as a callable function"
msgstr "呼び出し可能な関数としてのカーネル"

#: ../../tutorials/03_quantum_kernel.ipynb:316
msgid "We define a SVC model and directly pass the ``evaluate`` function of the quantum kernel as a callable. Once the model is created, we train it by calling the ``fit`` method on the training dataset and evaluate the model for accuracy with ``score``."
msgstr "SVC モデルを定義し、量子カーネルの ``evaluate`` 関数を呼び出し可能として直接渡します。 モデルが作成されたら、トレーニング データセットに対して ``fit`` メソッドを呼び出してモデルをトレーニングし、モデルの精度を ``score`` で評価します。"

#: ../../tutorials/03_quantum_kernel.ipynb:370
msgid "Precomputed kernel matrix"
msgstr "事前計算されたカーネル行列"

#: ../../tutorials/03_quantum_kernel.ipynb:372
msgid "Instead of passing a function of the quantum kernel as a callable, we can also precompute training and testing kernel matrices before passing them to the ``scikit-learn`` ``SVC`` algorithm."
msgstr "量子カーネルの関数を呼び出し可能として渡す代わりに、 ``scikit-learn`` ``SVC`` アルゴリズムに渡す前に、トレーニングとテストのカーネル行列を事前計算することもできます。"

#: ../../tutorials/03_quantum_kernel.ipynb:374
msgid "To extract the train and test matrices, we can call ``evaluate`` on the previously defined kernel and visualize them graphically as follows:"
msgstr "トレインとテスト行列を抽出するには、以前に定義したカーネルの ``evaluate`` を呼び出し、次のようにグラフィカルに視覚化します。"

#: ../../tutorials/03_quantum_kernel.ipynb:416
msgid "To use these matrices, we set the ``kernel`` parameter of a new ``SVC`` instance to ``\"precomputed\"``. We train the classifier by calling ``fit`` with the training matrix and training dataset. Once the model is trained, we evaluate it using the test matrix on the test dataset."
msgstr "これらの行列を使用するには、新しい ``SVC`` インスタンスの ``kernel`` パラメーターを ``\"precomputed\"`` に設定します。 トレーニング行列とトレーニング データセットを使用して ``fit`` を呼び出して分類器をトレーニングします。 モデルがトレーニングされたら、テスト データセットのテスト行列を使用してモデルを評価します。"

#: ../../tutorials/03_quantum_kernel.ipynb:468
msgid "2.4. Classification with QSVC"
msgstr "2.4. SVCによる分類"

#: ../../tutorials/03_quantum_kernel.ipynb:470
msgid "``QSVC`` is an alternative training algorithm provided by ``qiskit-machine-learning`` for convenience. It is an extension of ``SVC`` that takes in a quantum kernel instead of the ``kernel.evaluate`` method shown before."
msgstr "``QSVC`` は、便宜上 ``qiskit-machine-learning`` によって提供される代替トレーニング アルゴリズムです。 これは、前に示した ``kernel.evaluate`` メソッドの代わりに量子カーネルを取り込む ``SVC`` の拡張機能です。"

#: ../../tutorials/03_quantum_kernel.ipynb:524
msgid "2.5. Evaluation of models used for classification"
msgstr "2.5. 分類に使用されるモデルの評価"

#: ../../tutorials/03_quantum_kernel.ipynb:585
msgid "As the classification dataset is small, we find that the three models achieve 100% accuracy."
msgstr "分類データセットが小さいため、3つのモデルが100%の精度を達成していることがわかります。"

#: ../../tutorials/03_quantum_kernel.ipynb:597
msgid "3. Clustering"
msgstr "3. クラスタリング"

#: ../../tutorials/03_quantum_kernel.ipynb:599
msgid "The second workflow in this tutorial focuses on a clustering task using ``qiskit-machine-learning`` and the spectral clustering algorithm from ``scikit-learn``."
msgstr "このチュートリアルの 2 番目のワークフローは、``qiskit-machine-learning`` を使用したクラスタリングタスクと、``scikit-learn`` からのスペクトルクラスタリングアルゴリズムに焦点を当てています。"

#: ../../tutorials/03_quantum_kernel.ipynb:611
msgid "3.1. Defining the dataset"
msgstr "3.1. データセットの定義"

#: ../../tutorials/03_quantum_kernel.ipynb:613
msgid "We will once again use the *ad hoc dataset*, but now generated with a higher gap of ``0.6`` (previous example: ``0.3``) between the two classes."
msgstr "再び *ad hoc dataset* を使用しますが、2つのクラス間で ``0.6`` (以前の例では ``0.3` `) のギャップで生成されます。"

#: ../../tutorials/03_quantum_kernel.ipynb:615
msgid "Note that clustering falls under the category of unsupervised machine learning, so a test dataset is not required."
msgstr "クラスタリングは教師なし機械学習のカテゴリに属するため、テストデータセットは必要ありません。"

#: ../../tutorials/03_quantum_kernel.ipynb:645
msgid "We plot the clustering dataset below:"
msgstr "クラスタリングデータセットを以下にプロットします。"

#: ../../tutorials/03_quantum_kernel.ipynb:695
msgid "3.2. Defining the Quantum Kernel"
msgstr "3.2. 量子カーネルの定義"

#: ../../tutorials/03_quantum_kernel.ipynb:697
msgid "We use an identical setup as in the classification example. We create another instance of the ``FidelityQuantumKernel`` class with a ``ZZFeatureMap``, but you might notice that in this case we do not provide a ``fidelity`` instance. This is because the ``ComputeUncompute`` method provided in the previous case is instantiated by default when the fidelity instance is not provided explicitly."
msgstr "分類例と同じセットアップを使用します。 ``ZZFeatureMap`` を使用した ``FidelityQuantumKernel`` クラスの別のインスタンスを作成します。 しかし、この場合、``fidelity`` インスタンスは提供されません。 これは、fidelityインスタンスが明示的に提供されていない場合、前のケースで提供されている``ComputeUncompute`` メソッドがデフォルトでインスタンス化されるためです。"

#: ../../tutorials/03_quantum_kernel.ipynb:721
msgid "3.3. Clustering with the Spectral Clustering Model"
msgstr "3.3. スペクトルクラスタリングモデルとクラスタリング"

#: ../../tutorials/03_quantum_kernel.ipynb:723
msgid "The ``scikit-learn`` spectral clustering algorithm allows us to define a custom kernel in two ways (just like ``SVC``): 1. by providing the kernel as a **callable function** 2. by precomputing the **kernel matrix**."
msgstr "``scikit-learn`` のスペクトラル・クラスタリング・アルゴリズムでは、(``SVC`` のように) カスタム・カーネルを2つの方法で定義することができます: 1. カーネルを **呼び出し可能な関数** として提供するか、2. カーネル行列を事前計算するかです。"

#: ../../tutorials/03_quantum_kernel.ipynb:725
msgid "With the current ``FidelityQuantumKernel`` class in ``qiskit-machine-learning``, we can only use the latter option, so we precompute the kernel matrix by calling ``evaluate`` and visualize it as follows:"
msgstr "``qiskit-machine-learning`` の現在の ``FidelityQuantumKernel`` クラスでは、後者のオプションのみを使用できます。 カーネル行列を事前に計算し、``evaluate`` を呼び出し、次のように視覚化します。"

#: ../../tutorials/03_quantum_kernel.ipynb:759
msgid "Next, we define a spectral clustering model and fit it using the precomputed kernel. Further, we score the labels using normalized mutual information, since we know the class labels a priori (before hand)."
msgstr "次に、スペクトルクラスタリングモデルを定義し、あらかじめ計算されたカーネルを使用してそれを適合させます。 さらに、正規化された相互情報を使ってラベルをスコアします。クラスのラベルはプリオリ(手前) であることが分かっています。"

#: ../../tutorials/03_quantum_kernel.ipynb:814
msgid "4. Kernel Principal Component Analysis"
msgstr "4. カーネル主成分解析"

#: ../../tutorials/03_quantum_kernel.ipynb:816
msgid "This section focuses on a Principal Component Analysis task using a kernel PCA algorithm. We calculate a kernel matrix using a ``ZZFeatureMap`` and show that this approach translates the original features into a new space, where axes are chosen along principal components. In this space the classification task can be performed with a simpler model rather than an SVM."
msgstr "このセクションでは、カーネルPCAアルゴリズムを使用した主成分分析タスクに焦点を当てます。 ``ZZFeatureMap`` を使用してカーネルマトリックスを計算し、このアプローチが元の機能を新しい空間に変換することを示します。 軸を主成分に沿って選びます。この分野では、SVMではなく単純なモデルで分類作業を行うことができます。"

#: ../../tutorials/03_quantum_kernel.ipynb:828
msgid "4.1. Defining the dataset"
msgstr "4.1. データセットの定義"

#: ../../tutorials/03_quantum_kernel.ipynb:830
msgid "We again use the *ad hoc dataset* with a gap of ``0.6`` between the two classes. This dataset resembles the dataset we had in the clustering section, the difference is that in this case ``test_size`` is not zero."
msgstr "2 つのクラス間の ``0.6`` のギャップを持つ *ad hoc dataset* を使用します。 このデータセットは、クラスタリングセクションにあるデータセットに似ています。この場合、``test_size`` はゼロではありません。"

#: ../../tutorials/03_quantum_kernel.ipynb:860
msgid "We plot the training and test datasets below. Our ultimate goal in this section is to construct new coordinates where the two classes can be linearly separated."
msgstr "トレーニングとテストのデータセットを以下に示します。 このセクションの最終目標は、二つのクラスを直線的に分離できる新しい座標を構築することです。"

#: ../../tutorials/03_quantum_kernel.ipynb:890
msgid "4.2. Defining the Quantum Kernel"
msgstr "4.2. 量子カーネルの定義"

#: ../../tutorials/03_quantum_kernel.ipynb:892
msgid "We proceed with the same kernel setup as it was in the classification task, namely a ``ZZFeatureMap`` circuit as a feature map and an instance of ``FidelityQuantumKernel``."
msgstr "分類作業と同じカーネルセットアップを進めます。 つまり、特徴量マップとしての ``ZZFeatureMap`` 回路と、 ``FidelityQuantumKernel`` のインスタンスです。"

#: ../../tutorials/03_quantum_kernel.ipynb:914
msgid "Then, we evaluate kernel matrices for the training and test features."
msgstr "次に、トレーニングとテスト機能のカーネル行列を評価します。"

#: ../../tutorials/03_quantum_kernel.ipynb:937
msgid "4.3. Comparison of Kernel PCA on gaussian and quantum kernel"
msgstr "4.3. ガウシアンのカーネルPCAと量子カーネルの比較"

#: ../../tutorials/03_quantum_kernel.ipynb:939
msgid "In this section we use the ``KernelPCA`` implementation from ``scikit-learn``, with the ``kernel`` parameter set to \"rbf\" for a gaussian kernel and \"precomputed\" for a quantum kernel. The former is very popular in classical machine learning models, whereas the latter allows using a quantum kernel defined as ``qpca_kernel``."
msgstr "このセクションでは、``scikit-learn`` から ``KernelPCA`` 実装を使用します。 gaussian カーネルの ``kernel`` パラメータを \"rbf\" に、量子カーネルの \"precomputed\" に設定します。 前者は古典的な機械学習モデルで非常に人気がありますが、後者は ``qpca_kernel`` として定義された量子カーネルを使用できます。"

#: ../../tutorials/03_quantum_kernel.ipynb:941
msgid "One can observe that the gaussian kernel based Kernel PCA model fails to make the dataset linearly separable, while the quantum kernel succeeds."
msgstr "ガウス系カーネルベースのKernel PCAモデルは、データセットを線形に分離可能にできず、量子カーネルは成功していることがわかります。"

#: ../../tutorials/03_quantum_kernel.ipynb:943
msgid "While usually PCA is used to reduce the number of features in a dataset, or in other words to reduce dimensionality of a dataset, we don't do that here. Rather we keep the number of dimensions and employ the kernel PCA, mostly for visualization purposes, to show that classification on the transformed dataset becomes easily tractable by linear methods, like logistic regression. We use this method to separate two classes in the principal component space with a ``LogisticRegression`` model from ``scikit-learn``. As usual we train it by calling the ``fit`` method on the training dataset and evaluate the model for accuracy with ``score``."
msgstr "通常、PCAはデータセットの特徴量の数を減らすために使用されます。 言い換えればデータセットの次元数を減らすために使われますが、ここではそのようには使いません。次元の数を保ち、カーネルPCAを使用するのは、主に可視化目的のためです。 変換されたデータセットの分類がロジスティック回帰のような線形手法で容易に扱えることを示します。このメソッドを使用して、主要コンポーネント空間内の 2 つのクラスを ``scikit-learn`` の ``LogisticRegression`` モデルを使って分離します。 いつものように、トレーニングデータセットに ``fit`` メソッドを呼び出し、モデルの正確性を ``score`` で評価することで訓練します。"

#: ../../tutorials/03_quantum_kernel.ipynb:974
msgid "Here we train and score a model."
msgstr "モデルを訓練して得点をつけます。"

#: ../../tutorials/03_quantum_kernel.ipynb:1025
msgid "Let's plot the results. First, we plot the transformed dataset we get with the quantum kernel. On the same plot we also add model results. Then, we plot the transformed dataset we get with the gaussian kernel."
msgstr "結果をプロットしてみましょう。まず、量子カーネルで得られる変換されたデータセットをプロットします。 同じプロットでは、モデルの結果も追加します。そして、ガウスカーネルで得られる変換されたデータセットをプロットします。"

#: ../../tutorials/03_quantum_kernel.ipynb:1089
msgid "As we can see, the data points on the right figure are not separable, but they are on the left figure, hence in case of quantum kernel we can apply linear models on the transformed dataset and this is why SVM classifier works perfectly well on the *ad hoc* dataset as we saw in the `classification section <#2.-Classification>`__."
msgstr "ご覧のとおり、右の図のデータ ポイントは分離可能ではありませんが、左の図では分離可能です。したがって、量子カーネルの場合、変換されたデータセットに線形モデルを適用できます。これが、 `分類セクション <#2.-Classification>`__ で見た *ad hoc* データセットでSVM 分類器で分離可能である理由です。"

#: ../../tutorials/03_quantum_kernel.ipynb:1101
msgid "5. Conclusion"
msgstr "5. 結論"

#: ../../tutorials/03_quantum_kernel.ipynb:1103
msgid "In this tutorial:"
msgstr "このチュートリアルでは:"

#: ../../tutorials/03_quantum_kernel.ipynb:1105
msgid "We reviewed the fundamentals of quantum kernel learning"
msgstr ""

#: ../../tutorials/03_quantum_kernel.ipynb:1106
msgid "We understood how to define quantum kernels as instances of ``FidelityQuantumKernel``"
msgstr ""

#: ../../tutorials/03_quantum_kernel.ipynb:1107
msgid "We learned how to use the ``scikit-learn`` ``SVC`` algorithm with a custom quantum kernel as a callable function vs precomputed quantum kernel matrix for classification"
msgstr ""

#: ../../tutorials/03_quantum_kernel.ipynb:1108
msgid "We learned how to train classifiers with the ``QSVC`` algorithm from ``qiskit-machine-learning``"
msgstr ""

#: ../../tutorials/03_quantum_kernel.ipynb:1109
msgid "We learned how to use the ``scikit-learn`` ``SpectralClustering`` algorithms with a precomputed quantum kernel matrix for clustering"
msgstr ""

#: ../../tutorials/03_quantum_kernel.ipynb:1110
msgid "We investigated how to plug in a quantum kernel into ``scikit-learn``'s ``KernelPCA`` algorithm and transform the ad-hoc dataset into a new one that can be tackled by a linear model."
msgstr ""

#: ../../tutorials/03_quantum_kernel.ipynb:1121
msgid "For further reference, ``scikit-learn`` has other algorithms that can use a precomputed kernel matrix, such as:"
msgstr ""

#: ../../tutorials/03_quantum_kernel.ipynb:1123
msgid "`Agglomerative clustering <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html>`__"
msgstr "`Agglomerative clustering <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html>`__"

#: ../../tutorials/03_quantum_kernel.ipynb:1124
msgid "`Support vector regression <https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html>`__"
msgstr "`Support vector regression <https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html>`__"

#: ../../tutorials/03_quantum_kernel.ipynb:1125
msgid "`Ridge regression <https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html>`__"
msgstr "`Ridge regression <https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html>`__"

#: ../../tutorials/03_quantum_kernel.ipynb:1126
msgid "`Gaussian process regression <https://scikit-learn.org/stable/modules/gaussian_process.html>`__"
msgstr "`Gaussian process regression <https://scikit-learn.org/stable/modules/gaussian_process.html>`__"

