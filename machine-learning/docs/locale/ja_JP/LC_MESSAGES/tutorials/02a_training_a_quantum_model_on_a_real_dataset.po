msgid ""
msgstr ""
"Project-Id-Version: qiskit-docs\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-09-01 14:17+0000\n"
"PO-Revision-Date: 2023-09-01 15:30\n"
"Last-Translator: \n"
"Language: ja\n"
"Language-Team: Japanese\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"
"X-Crowdin-Project: qiskit-docs\n"
"X-Crowdin-Project-ID: 369271\n"
"X-Crowdin-Language: ja\n"
"X-Crowdin-File: /main/machine-learning/docs/locale/en/LC_MESSAGES/tutorials/02a_training_a_quantum_model_on_a_real_dataset.po\n"
"X-Crowdin-File-ID: 9883\n"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:9
msgid "This page was generated from `docs/tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb`__."
msgstr "このページは `docs/tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb`__ から生成されました。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:9
msgid "Training a Quantum Model on a Real Dataset"
msgstr "実際のデータセットでの量子モデルの学習"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:11
msgid "This tutorial will demonstrate how to train a quantum machine learning model to tackle a classification problem. Previous tutorials have featured small, artificial datasets. Here we will increase the problem complexity by considering a real-life classical dataset. We decided to pick a very well-known – albeit still relatively small – problem: the Iris flower dataset. This dataset even has its own Wikipedia `page <https://en.wikipedia.org/wiki/Iris_flower_data_set>`__. Although the Iris dataset is well known to data scientists, we will briefly introduce it to refresh our memories. For comparison, we'll first train a classical counterpart to the quantum model."
msgstr "このチュートリアルでは、分類問題に取り組むために、量子機械学習モデルをどのように学習させるかを説明します。これまでのチュートリアルでは、小規模で人工的なデータセットを取り上げてきました。今回は、現実の古典的なデータセットを用いて、問題の複雑性を高めることにします。我々は、非常に有名ですが、まだ比較的小さな問題であるアイリスの花のデータセットを選ぶことにしました。このデータセットにはWikipediaの `ページ <https://en.wikipedia.org/wiki/Iris_flower_data_set>`__ があるくらいです。アイリスデータセットはデータサイエンティストにはよく知られていますが、記憶を呼び覚ますために簡単に紹介します。比較のために、まず量子モデルに対応する古典的なモデルを学習します。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:14
msgid "So, let's get started:"
msgstr "では、さっそく始めましょう："

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:16
msgid "First, we'll load the dataset and explore what it looks like."
msgstr "まず、データセットをロードし、それがどのように見えるかを探ります。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:17
msgid "Next, we'll train a classical model using `SVC <https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html>`__ from `scikit-learn <https://scikit-learn.org/>`__ to see how well the classification problem can be solved using classical methods."
msgstr "次に、`scikit-learn <https://scikit-learn.org/>`__ の `SVC <https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html>`__ を使って古典的なモデルを学習し、古典的な手法で分類問題がどの程度解決できるかを見てみましょう。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:18
msgid "After that, we'll introduce the Variational Quantum Classifier (VQC)."
msgstr "その後、Variational Quantum Classifier(VQC) を紹介します。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:19
msgid "To conclude, we'll compare the results obtained from both models."
msgstr "最後に、両方のモデルから得られた結果を比較します。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:22
msgid "1. Exploratory Data Analysis"
msgstr "1. データ分析の探索"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:24
msgid "First, let us explore the Iris dataset this tutorial will use and see what it contains. For our convenience, this `dataset <https://scikit-learn.org/stable/datasets/toy_dataset.html#iris-dataset>`__ is available in scikit-learn and can be loaded easily."
msgstr "まず、このチュートリアルで使用するアイリスのデータセットを調べて、何が含まれているかを見てみましょう。便宜上、この `データセット <https://scikit-learn.org/stable/datasets/toy_dataset.html#iris-dataset>`__ は scikit-learn で利用可能で、簡単にロードすることができます。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:47
msgid "If no parameters are specified in the ``load_iris`` function, then a dictionary-like object is returned by scikit-learn. Let's print the description of the dataset and see what is inside."
msgstr "``load_iris`` 関数でパラメーターを指定しなかった場合、辞書のようなオブジェクトが scikit-learn によって返されます。データセットの記述を出力して、中身を見てみましょう。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:278
msgid "There are a few interesting observations we can find from this dataset description:"
msgstr "このデータセットの説明から、いくつかの興味深い点を見出すことができます。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:280
msgid "There are 150 samples (instances) in the dataset."
msgstr "データセットには150のサンプル(インスタンス) があります。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:281
msgid "There are four features (attributes) in each sample."
msgstr "各サンプルには4つの特徴量 (属性) があります。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:282
msgid "There are three labels (classes) in the dataset."
msgstr "データセットには3つのラベル(クラス) があります。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:283
msgid "The dataset is perfectly balanced, as there are the same number of samples (50) in each class."
msgstr "各クラスに同じ数のサンプル(50) があるため、データセットは完全にバランスが取れています。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:284
msgid "We can see features are not normalized, and their value ranges are different, e.g., :math:`[4.3, 7.9]` and :math:`[0.1, 2.5]` for sepal length and petal width, respectively. So, transforming the features to the same scale may be helpful."
msgstr "例えば、がくの長さ(sepal width)は :math:`[4.3, 7.9]` 、花弁の幅(petal width)は :math:`[0.1, 2.5]` というように、特徴量が正規化されておらず、値の範囲も異なっていることが分かります。そこで、特徴量を同じスケールに変換することが有効な場合があります。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:285
msgid "As stated in the table above, feature-to-class correlation in some cases is very high; this may lead us to think that our model should cope well with the dataset."
msgstr "上の表にあるように、いくつかのケースでは特徴量とクラスの相関が非常に高く、このことから、我々のモデルはデータセットにうまく対応できるはずだと思われるかもしれません。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:287
msgid "We only examined the dataset description, but additional properties are available in the ``iris_data`` object. Now we are going to work with features and labels from the dataset."
msgstr "今回はデータセットの説明だけを調べましたが、 ``iris_data`` オブジェクトにはその他のプロパティも用意されています。それでは、データセットに含まれる特徴量とラベルを操作してみましょう。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:309
msgid "Firstly, we'll normalize the features. Namely, we will apply a simple transformation to represent all features on the same scale. In our case, we squeeze all features onto the interval :math:`[0, 1]`. Normalization is a common technique in machine learning and often leads to better numerical stability and convergence of an algorithm."
msgstr "まず、特徴量を正規化します。つまり、すべての特徴量を同じ尺度で表現するために、簡単な変換を行います。この例では、すべての特徴量を区間 :math:`[0, 1]` に絞り込みます。正規化は機械学習において一般的な手法であり、しばしばアルゴリズムの数値的安定性や収束性を向上させることにつながります。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:311
msgid "We can use ``MinMaxScaler`` from scikit-learn to perform this. Without specifying parameters, this does exactly what is required: maps data onto :math:`[0, 1]`."
msgstr "Scikit-learn の ``MinMaxScaler`` を使用して、この処理を行うことができます。これはパラメーターを指定せずに、必要なことだけを行うもので、データを :math:`[0, 1]` にマップします。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:334
msgid "Let's see how our data looks. We plot the features pair-wise to see if there's an observable correlation between them."
msgstr "では、データがどのように見えるか見てみましょう。特徴量を対にしてプロットし、それらの間に観測可能な相関があるかどうかを確認します。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:424
msgid "From the plots, we see that class ``0`` is easily separable from the other two classes, while classes ``1`` and ``2`` are sometimes intertwined, especially regarding the \"sepal width\" feature."
msgstr "プロットから、クラス ``0`` と他の2つのクラスは容易に分離可能であることがわかりますが、クラス ``1`` と ``2`` は、特に「がくの幅（sepal width）」の特徴に関して、時々絡み合っていることがわかります。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:426
msgid "Next, let's see how classical machine learning handles this dataset."
msgstr "次に、古典的な機械学習がこのデータセットをどのように扱うか見てみましょう。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:429
msgid "2. Training a Classical Machine Learning Model"
msgstr "2. 古典的な機械学習モデルの学習"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:431
msgid "Before we train a model, we should split the dataset into two parts: a training dataset and a test dataset. We'll use the former to train the model and the latter to verify how well our models perform on unseen data."
msgstr "モデルを学習する前に、データセットをトレーニングデータセットとテストデータセットの2つに分ける必要があります。前者はモデルの学習に、後者は未知のデータに対してモデルがどの程度の性能を発揮するかを検証するために使います。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:433
msgid "As usual, we'll ask scikit-learn to do the boring job for us. We'll also fix the seed to ensure the results are reproducible."
msgstr "いつものように、退屈な仕事はscikit-learnにお願いすることにします。また、結果が再現できるようにseedを固定します。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:460
msgid "We train a classical Support Vector Classifier from scikit-learn. For the sake of simplicity, we don't tweak any parameters and rely on the default values."
msgstr "Scikit-learnの古典的なサポートベクター分類器を学習させます。簡単のため、パラメーターはいじらず、デフォルトの値を使用します。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:484
msgid "Now we check out how well our classical model performs. We will analyze the scores in the conclusion section."
msgstr "ここで、我々の古典的なモデルがどの程度のパフォーマンスを示すかを確認します。結論の部分でそのスコアを分析します。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:536
msgid "As can be seen from the scores, the classical SVC algorithm performs very well. Next up, it's time to look at quantum machine learning models."
msgstr "スコアからわかるように、古典的なSVCアルゴリズムは非常に良いパフォーマンスを見せています。次は、量子機械学習モデルについてです。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:539
msgid "3. Training a Quantum Machine Learning Model"
msgstr "2. 量子機械学習モデルの学習"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:541
msgid "As an example of a quantum model, we'll train a variational quantum classifier (VQC). The VQC is the simplest classifier available in Qiskit Machine Learning and is a good starting point for newcomers to quantum machine learning who have a background in classical machine learning."
msgstr "量子モデルの例として、変分量子分類器(VQC) を学習します。VQCは Qiskit Machine Learning で利用できる最もシンプルな分類器であり、古典的な機械学習のバックグラウンドを持つ量子機械学習初心者に適した出発点です。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:543
msgid "But before we train a model, let's examine what comprises the ``VQC`` class. Two of its central elements are the feature map and ansatz. What these are will now be explained."
msgstr "しかし、モデルを学習する前に、``VQC`` クラスを構成するものを調べてみましょう。その中心的な要素は特徴量マップとansatzです。これらが何かを説明します。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:545
msgid "Our data is classical, meaning it consists of a set of bits, not qubits. We need a way to encode the data as qubits. This process is crucial if we want to obtain an effective quantum model. We usually refer to this mapping as data encoding, data embedding, or data loading and this is the role of the feature map. While feature mapping is a common ML mechanism, this process of loading data into quantum states does not appear in classical machine learning as that only operates in the classical world."
msgstr "私たちのデータは古典的なもので、量子ビットではなくビットの集合で構成されています。データを量子ビットとして符号化する方法が必要なのです。このプロセスは、有効な量子モデルを得るために非常に重要です。我々は通常、このマッピングをデータエンコーディング、データエンベッディング、データローディングと呼び、これが特徴量マップの役割となります。特徴量マップは一般的なMLメカニズムですが、この量子状態へのデータロードのプロセスは、古典的な世界でのみ作用するため、古典的な機械学習には登場しません。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:548
msgid "Once the data is loaded, we must immediately apply a parameterized quantum circuit. This circuit is a direct analog to the layers in classical neural networks. It has a set of tunable parameters or weights. The weights are optimized such that they minimize an objective function. This objective function characterizes the distance between the predictions and known labeled data. A parameterized quantum circuit is also called a parameterized trial state, variational form, or ansatz. Perhaps, the latter is the most widely used term."
msgstr "データを読み込んだら、すぐにパラメーター化された量子回路を適用しなければなりません。この回路は、古典的なニューラルネットワークの層と直接的に類似しています。この回路は、調整可能なパラメーター（重み）を持っています。重みは、ある目的関数を最小化するように最適化されます。この目的関数は、予測値と既知のラベル付けされたデータとの距離を特徴づけます。パラメーター化された量子回路は、パラメーター化された試行状態、変分形式、ansatzとも呼ばれます。おそらく、後者が最も広く用いられている用語でしょう。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:551
msgid "For more information, we direct the reader to the `Quantum Machine Learning Course <https://learn.qiskit.org/course/machine-learning>`__."
msgstr "詳しくは、 `量子機械学習のコース <https://learn.qiskit.org/course/machine-learning>`__ をご覧ください。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:553
msgid "Our choice of feature map will be the ``ZZFeatureMap``. The ``ZZFeatureMap`` is one of the standard feature maps in the Qiskit circuit library. We pass ``num_features`` as ``feature_dimension``, meaning the feature map will have ``num_features`` or ``4`` qubits."
msgstr "ここでは ``ZZFeatureMap`` を使用することにします。 ``ZZFeatureMap`` はQiskitのcircuit ライブラリーに含まれる標準的な特徴量マップの一つです。 ``num_features`` を ``feature_dimension`` として渡すと、特徴量マップは ``num_features`` または ``4`` 個の量子ビットを持つことになります。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:555
msgid "We decompose the feature map into its constituent gates to give the reader a flavor of how feature maps may look."
msgstr "特徴量マップをゲートに分解し、特徴量マップがどのように見えるかを伝えます。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:590
msgid "If you look closely at the feature map diagram, you will notice parameters ``x[0], ..., x[3]``. These are placeholders for our features."
msgstr "特徴量ダイアグラムをよく見ると、パラメーター ``x[0], ..., x[3]`` が見えます。これらは、特徴量のプレースホルダーです。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:592
msgid "Now we create and plot our ansatz. Pay attention to the repetitive structure of the ansatz circuit. We define the number of these repetitions using the ``reps`` parameter."
msgstr "Ansatzを作成してプロットします。Ansatz回路の反復構造に注意してください。 これらの繰り返しの数は ``reps`` パラメーターを使用して定義します。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:625
msgid "This circuit has 16 parameters named ``θ[0], ..., θ[15]``. These are the trainable weights of the classifier."
msgstr "この回路には16個のパラメーターがあり、そのパラメーターには ``θ[0], ..., θ[15]`` があります。これらは分類器の訓練可能な重みです。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:627
msgid "We then choose an optimization algorithm to use in the training process. This step is similar to what you may find in classical deep learning frameworks. To make the training process faster, we choose a gradient-free optimizer. You may explore other optimizers available in Qiskit."
msgstr "次に、学習プロセスで使用する最適化アルゴリズムを選択します。このステップは、古典的な深層学習フレームワークで見られるものと似ています。学習プロセスを高速化するために、勾配なしオプティマイザーを選択します。Qiskitで利用可能な他のオプティマイザーを探索することもできます。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:678
msgid "In the next step, we define where to train our classifier. We can train on a simulator or a real quantum computer. Here, we will use a simulator. We create an instance of the ``Sampler`` primitive. This is the reference implementation that is statevector based. Using qiskit runtime services you can create a sampler that is backed by a quantum computer."
msgstr "次のステップでは、分類器をどこで学習させるかを定義します。学習にはシミュレーターと実物の量子コンピューターがあります。ここでは、シミュレーターを使用します。 ``Sampler`` primitiveのインスタンスを作成します。これは状態ベクトルベースのリファレンス実装です。qiskitのruntimeサービスを使用すると、量子コンピューターを使ったsamplerを作成することができます。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:701
msgid "We will add a callback function called ``callback_graph``. ``VQC`` will call this function for each evaluation of the objective function with two parameters: the current weights and the value of the objective function at those weights. Our callback will append the value of the objective function to an array so we can plot the iteration versus the objective function value. The callback will update the plot at each iteration. Note that you can do whatever you want inside a callback function, so long as it has the two-parameter signature we mentioned above."
msgstr "ここでは、``callback_graph`` というコールバック関数を追加します。VQCは目的関数を評価するたびに、現在の重みとその重みにおける目的関数の値という2つのパラメーターを指定して、この関数を呼び出します。コールバックは目的関数の値を配列に追加し、反復計算を目的関数の値に対してプロットできるようにします。コールバックは各反復ごとにプロットを更新します。コールバック関数の内部では、先ほど説明した2パラメーターのシグネチャーを持つ限り、どんなことでもできることに注意してください。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:737
msgid "Now we are ready to construct the classifier and fit it."
msgstr "これで、分類器を構築し、適合させる準備が整いました。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:739
msgid "``VQC`` stands for \"variational quantum classifier.\" It takes a feature map and an ansatz and constructs a quantum neural network automatically. In the simplest case it is enough to pass the number of qubits and a quantum instance to construct a valid classifier. You may omit the ``sampler`` parameter, in this case a ``Sampler`` instance will be created for you in the way we created it earlier. We created it manually for illustrative purposes only."
msgstr "``VQC`` は、「変分量子分類器」の略称です。特徴量とansatzを受け取り、量子ニューラルネットワークを自動的に構築します。最も単純なケースでは、量子ビットの数と量子インスタンスを渡すだけで、有効な分類器を構築することができます。 ``Sampler`` パラメーターを省略することも可能です。この場合、先ほど作成した方法で ``Sampler`` インスタンスが作成されます。ここでは、説明のために手動で作成しました。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:741
msgid "Training may take some time. Please, be patient."
msgstr "学習には時間がかかる場合があります。少しお待ちください。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:813
msgid "Let's see how the quantum model performs on the real-life dataset."
msgstr "それでは、量子モデルが実際のデータセットでどのようなパフォーマンスを発揮するのか見てみましょう。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:865
msgid "As we can see, the scores are high, and the model can be used to predict labels on unseen data."
msgstr "見ての通り、スコアは高く、このモデルを使って未見のデータに対するラベルを予測することが可能です。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:867
msgid "Now let's see what we can tune to get even better models."
msgstr "では、さらに良いモデルを得るために、どのようなチューニングをすればよいのかを見ていきましょう。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:869
msgid "The key components are the feature map and the ansatz. You can tweak parameters. In our case, you may change the ``reps`` parameter that specifies how repetitions of a gate pattern we add to the circuit. Larger values lead to more entanglement operations and more parameters. Thus, the model can be more flexible, but the higher number of parameters also adds complexity, and training such a model usually takes more time. Furthermore, we may end up overfitting the model. You can try the other feature maps and ansatzes available in the `Qiskit circuit library <https://qiskit.org/documentation/apidoc/circuit_library.html#n-local-circuits>`__, or you can come up with custom circuits."
msgstr "重要な構成要素は、特徴量マップとansatzです。パラメーターを調整することができます。この例では、回路に追加するゲートパターンの繰り返しを指定する ``reps`` パラメーターを変更することができます。この値を大きくすると、より多くのエンタングルメント演算が行われるようになり、パラメーターも多くなります。このように、モデルはより柔軟になりますが、パラメーターが多くなることで複雑さが増し、モデルのトレーニングに時間がかかるのが一般的です。さらに、モデルをオーバーフィットさせてしまう可能性もあります。 `Qiskit circuit library <https://qiskit.org/documentation/apidoc/circuit_library.html#n-local-circuits>`__ で利用可能な他の特徴量マップやansatzを試すこともできますし、カスタム回路を考え出すこともできます。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:871
msgid "You may try other optimizers. Qiskit contains a bunch of them. Some of them are gradient-free, others not. If you choose a gradient-based optimizer, e.g., ``L_BFGS_B``, expect the training time to increase. Additionally to the objective function, these optimizers must evaluate the gradient with respect to the training parameters, which leads to an increased number of circuit executions per iteration."
msgstr "他のオプティマイザーを試してみるのもよいでしょう。Qiskitには色々なオプティマイザーがあります。その中には勾配がないものもあれば、そうでないものもあります。例えば ``L_BFGS_B`` のような勾配ベースのオプティマイザーを選択した場合、トレーニング時間が増加することが予想されます。目的関数に加えて、これらのオプティマイザーは学習パラメーターに関して勾配を評価する必要があり、これは反復あたりの回路実行数の増加につながります。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:872
msgid "Another option is to randomly (or deterministically) sample ``initial_point`` and fit the model several times."
msgstr "もう一つの方法は、ランダムに（または決定論的に） ``initial_point`` をサンプリングし、何度もモデルを適合させることです。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:874
msgid "But what if a dataset contains more features than a modern quantum computer can handle? Recall, in this example, we had the same number of qubits as the number of features in the dataset, but this may not always be the case."
msgstr "しかし、データセットに含まれる素性が、最新の量子コンピューターが処理できる量よりも多い場合はどうでしょうか。この例では、データセットの特徴量と同じ数の量子ビットを用意しましたが、必ずしもそうではないかもしれません。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:877
msgid "4. Reducing the Number of Features"
msgstr "4. 特徴量の数の削減"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:879
msgid "In this section, we reduce the number of features in our dataset and train our models again. We'll move through faster this time as the steps are the same except for the first, where we apply a PCA transformation."
msgstr "このセクションでは、データセットの特徴量数を減らし、再度モデルを学習します。今回は、最初のPCA変換を行う以外は同じステップなので、より速く進めます。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:881
msgid "We transform our four features into two features only. This dimensionality reduction is for educational purposes only. As you saw in the previous section, we can train a quantum model using all four features from the dataset."
msgstr "4つの特徴量を2つの特徴量のみに変換します。この次元削減は教育的な目的のために行うのであり、前節で見たように、データセットの4つの特徴量を全て使って量子モデルを学習することができます。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:883
msgid "Now, we can easily plot these two features on a single figure."
msgstr "さて、この2つの特徴量を1つの図に簡単にプロットすることができます。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:943
msgid "As usual, we split the dataset first, then fit a classical model."
msgstr "いつものように、まずデータセットを分割し、次に古典的なモデルを当てはめます。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:1001
msgid "The results are still good but slightly worse compared to the initial version. Let's see how a quantum model deals with them. As we now have two qubits, we must recreate the feature map and ansatz."
msgstr "結果はまだ良いのですが、最初のバージョンと比べると若干悪くなっています。量子モデルがそれらをどのように扱うか見てみましょう。量子ビットが2つになったので、特徴量マップとAnsatzを作り直さなければなりません。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:1025
msgid "We also reduce the maximum number of iterations we run the optimization process for, as we expect it to converge faster because we now have fewer qubits."
msgstr "また、最適化プロセスを実行する反復回数の最大値を減らしました。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:1046
msgid "Now we construct a quantum classifier from the new parameters and train it."
msgstr "ここで、新しいパラメーターから量子分類器を構築し、学習させます。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:1160
msgid "Well, the scores are higher than a fair coin toss but could be better. The objective function is almost flat towards the end, meaning increasing the number of iterations won't help, and model performance will stay the same. Let's see what we can do with another ansatz."
msgstr "さて、スコアは公平なコイントスよりも高いのですが、もっと良くなる可能性があります。目的関数は最後に向かってほぼフラットです。つまり、反復回数を増やしても、モデルの性能は変わらないということです。別のansatzで何ができるかを見てみましょう。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:1275
msgid "The scores are better than in the previous setup. Perhaps if we had used more iterations, we could do even better."
msgstr "スコアは前回の設定よりも良くなっています。おそらく、もっと反復回数を増やせば、さらに良い結果が得られると思います。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:1287
msgid "5. Conclusion"
msgstr "5. 結論"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:1289
msgid "In this tutorial, we have built two classical and three quantum machine learning models. Let's print an overall table with our results."
msgstr "このチュートリアルでは、2つの古典的なモデルと3つの量子機械学習モデルを構築しました。結果とともに全体的な表を出力してみます。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:1358
msgid "Unsurprisingly, the classical models perform better than their quantum counterparts, but classical ML has come a long way, and quantum ML has yet to reach that level of maturity. As we can see, we achieved the best results using a classical support vector machine. But the quantum model trained on four features was also quite good. When we reduced the number of features, the performance of all models went down as expected. So, if resources permit training a model on a full-featured dataset without any reduction, you should train such a model. If not, you may expect to compromise between dataset size, training time, and score."
msgstr "当然ながら、古典的なモデルの方が量子的なモデルよりも性能が良いです。古典的なMLは長い道のりを歩んできており、量子MLはまだそのレベルに到達していないのです。見ての通り、我々は古典的なサポートベクターマシンを使って最高の結果を得ました。しかし、4つの特徴量で学習させた量子モデルも非常に良い結果でした。しかし、特徴量の数を減らすと、予想通り全てのモデルの性能が落ちました。ですから、もしリソースが許すのであれば、全特徴を持つデータセットでモデルを学習させるべきでしょう。そうでない場合は、データセットサイズ、学習時間、スコアの間で妥協することになるかもしれません。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:1361
msgid "Another observation is that even a simple ansatz change can lead to better results. The two-feature model with the ``EfficientSU2`` ansatz performs better than the one with ``RealAmplitudes``. That means the choice of hyperparameters plays the same critical role in quantum ML as in classical ML, and searching for optimal hyperparameters may take a long time. You may apply the same techniques we use in classical ML, such as random/grid or more sophisticated approaches."
msgstr "もう一つの観察は、単純なAnsatzの変更でさえ、より良い結果につながるということです。 ``EfficientSU2`` ansatz を用いた2成分モデルは、 ``RealAmplitudes`` を用いたモデルよりも良好な結果を得ることができました。つまり、量子MLでも古典MLと同様にハイパーパラメーターの選択が重要であり、最適なハイパーパラメーターを探索するには長い時間がかかる可能性があります。古典的なMLと同じ手法、例えばランダム/グリッドや、より洗練されたアプローチを適用することができます。"

#: ../../tutorials/02a_training_a_quantum_model_on_a_real_dataset.ipynb:1363
msgid "We hope this brief tutorial helps you to take the leap from classical to quantum ML."
msgstr "この簡単なチュートリアルで、あなたが古典から量子MLへと飛躍することを期待しています。"

