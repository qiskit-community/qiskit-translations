msgid ""
msgstr ""
"Project-Id-Version: qiskit-docs\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-05-09 10:21+0000\n"
"PO-Revision-Date: 2023-08-21 14:31\n"
"Last-Translator: \n"
"Language: ja\n"
"Language-Team: Japanese\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"
"X-Crowdin-Project: qiskit-docs\n"
"X-Crowdin-Project-ID: 369271\n"
"X-Crowdin-Language: ja\n"
"X-Crowdin-File: /main/machine-learning/docs/locale/en/LC_MESSAGES/tutorials/01_neural_networks.po\n"
"X-Crowdin-File-ID: 9628\n"

#: ../../tutorials/01_neural_networks.ipynb:9
msgid "This page was generated from `docs/tutorials/01_neural_networks.ipynb`__."
msgstr "このページは `docs/tutorials/01_neural_networks.ipynb`__ から生成されました。"

#: ../../tutorials/01_neural_networks.ipynb:9
msgid "Quantum Neural Networks"
msgstr "量子ニューラル・ネットワーク"

#: ../../tutorials/01_neural_networks.ipynb:12
msgid "Overview"
msgstr "概要"

#: ../../tutorials/01_neural_networks.ipynb:14
msgid "This notebook demonstrates different quantum neural network (QNN) implementations provided in ``qiskit-machine-learning``, and how they can be integrated into basic quantum machine learning (QML) workflows."
msgstr "このノートブックでは、 ``qiskit-machine-learning`` で提供されるさまざまな量子ニューラルネットワーク (QNN) の実装と、それらを基本的な量子機械学習 (QML) ワークフローに統合する方法について説明します。"

#: ../../tutorials/01_neural_networks.ipynb:16
msgid "The tutorial is structured as follows:"
msgstr "このチュートリアルの構造は以下のとおりです。"

#: ../../tutorials/01_neural_networks.ipynb:18
msgid "`Introduction <#1.-Introduction>`__"
msgstr "`はじめに <#1.-Introduction>`__"

#: ../../tutorials/01_neural_networks.ipynb:19
msgid "`How to Instantiate QNNs <#2.-How-to-Instantiate-QNNs>`__"
msgstr "`QNNをインスタンス化する方法 <#2. - How-to-Instantiate-QNNs>`__"

#: ../../tutorials/01_neural_networks.ipynb:20
msgid "`How to Run a Forward Pass <#3.-How-to-Run-a-Forward-Pass>`__"
msgstr "`フォワード・パスの実行方法 <#3.-How-to-Run-a-Forward-Pass>`__"

#: ../../tutorials/01_neural_networks.ipynb:21
msgid "`How to Run a Backward Pass <#4.-How-to-Run-a-Backward-Pass>`__"
msgstr "`バックワード・パスの実行方法 <#4.-How-to-Run-a-Backward-Pass>`__"

#: ../../tutorials/01_neural_networks.ipynb:22
msgid "`Advanced Functionality <#5.-Advanced-Functionality>`__"
msgstr "`拡張機能 <#5.-Advanced-Functionality>`__"

#: ../../tutorials/01_neural_networks.ipynb:23
msgid "`Conclusion <#6.-Conclusion>`__"
msgstr "`結論 <#6.-Conclusion>`__"

#: ../../tutorials/01_neural_networks.ipynb:35
msgid "1. Introduction"
msgstr "1. はじめに"

#: ../../tutorials/01_neural_networks.ipynb:38
msgid "1.1. Quantum vs. Classical Neural Networks"
msgstr "1.1. 量子対古典的ニューラルネットワーク"

#: ../../tutorials/01_neural_networks.ipynb:40
msgid "Classical neural networks are algorithmic models inspired by the human brain that can be trained to recognize patterns in data and learn to solve complex problems. They are based on a series of interconnected nodes, or *neurons*, organized in a layered structure, with parameters that can be learned by applying machine or deep learning training strategies."
msgstr "古典的なニューラルネットワークは、人間の脳に触発されたアルゴリズム・モデルで、データのパターンを認識し、複雑な問題の解決を学習できます。 一連の相互接続されたノード (*ニューロン*) が基礎となり、層構造で構成され、機械学習や深層学習の戦略を適用することで学習できるパラメーターから成ります。"

#: ../../tutorials/01_neural_networks.ipynb:42
msgid "The motivation behind quantum machine learning (QML) is to integrate notions from quantum computing and classical machine learning to open the way for new and improved learning schemes. QNNs apply this generic principle by combining classical neural networks and parametrized quantum circuits. Because they lie at an intersection between two fields, QNNs can be viewed from two perspectives:"
msgstr "量子機械学習 (QML) の背後にある動機は、量子コンピューティングと古典的機械学習からの概念を統合し、新しくかつ改良された学習スキームの方法を開くことです。QNN は、古典的なニューラルネットワークとパラメーター化された量子回路を結合することにより、この一般的な原理を適用します。2 つの分野間の交差点に位置するため、 QNN は次の 2 つの観点から見ることができます。"

#: ../../tutorials/01_neural_networks.ipynb:44
msgid "From a **machine learning perspective**, QNNs are, once again, algorithmic models that can be trained to find hidden patterns in data in a similar manner to their classical counterparts. These models can **load** classical data (**inputs**) into a quantum state, and later **process** it with quantum gates parametrized by **trainable weights**. Figure 1 shows a generic QNN example including the data loading and processing steps. The output from measuring this state can then be plugged into a loss function to train the weights through backpropagation."
msgstr "**機械学習の観点**から言うと、 QNN は、改めて、古典的なものと同様にデータの隠れたパターンを見つけるために訓練できるアルゴリズム・モデルです。 これらのモデルは、古典データ (**入力**) を量子状態に **読み込む** ことができ、**訓練可能な重み** によりパラメーター化された量子ゲートとともに **処理** されます。 図1は、データの読み込みおよび処理のステップを含む、汎用 QNN の例を示しています。 この状態を測定した出力を損失関数に差し込んで、誤差逆伝播法を通して重みを訓練することができます。"

#: ../../tutorials/01_neural_networks.ipynb:47
msgid "From a **quantum computing perspective**, QNNs are quantum algorithms based on parametrized quantum circuits that can be trained in a variational manner using classical optimizers. These circuits contain a **feature map** (with input parameters) and an **ansatz** (with trainable weights), as seen in Figure 1."
msgstr "**量子コンピューティングの観点** から言うと、QNNはパラメーター化された量子回路に基づいた量子アルゴリズムで、古典的なオプティマイザーを使用して変分法で訓練することができます。 これらの回路には、図1に示すように **フィーチャーマップ** (入力パラメーター付き) と **ansatz** (訓練可能な重み付き) が含まれています。"

#: ../../tutorials/01_neural_networks.ipynb:49
msgid "|new_qnn-3.jpg|"
msgstr "|new_qnn-3.jpg|"

#: ../../tutorials/01_neural_networks.ipynb:53
msgid "new_qnn-3.jpg"
msgstr "new_qnn-3.jpg"

#: ../../tutorials/01_neural_networks.ipynb:51
msgid "*Figure 1. Generic quantum neural network (QNN) structure.*"
msgstr "*図1. 一般的な量子ニューラルネットワーク(QNN) の構造*"

#: ../../tutorials/01_neural_networks.ipynb:64
msgid "As you can see, these two perspectives are complementary, and do not necessarily rely on strict definitions of concepts such as \"quantum neuron\" or what constitutes a QNN's \"layer\"."
msgstr "ご覧の通り、これらの 2 つの視点は補完なものであり、必ずしも「量子ニューロン」や QNNの「層」といった概念の厳密な定義に依存するわけではありません。"

#: ../../tutorials/01_neural_networks.ipynb:67
msgid "1.2. Implementation in ``qiskit-machine-learning``"
msgstr "1.2. ``qiskit-machine-learning`` での実装"

#: ../../tutorials/01_neural_networks.ipynb:69
msgid "The QNNs in ``qiskit-machine-learning`` are meant as application-agnostic computational units that can be used for different use cases, and their setup will depend on the application they are needed for. The module contains an interface for the QNNs and two specific implementations:"
msgstr "``qiskit-machine-learning`` の QNN は、さまざまなユースケースで使用できるアプリケーションに依存しない計算ユニットとして使用されます。 それらの設定は必要なアプリケーションによって異なります このモジュールには、QNNのインターフェースと 2 つの特定の実装が含まれています。"

#: ../../tutorials/01_neural_networks.ipynb:71
msgid "`NeuralNetwork <https://qiskit.org/documentation/machine-learning/stubs/qiskit_machine_learning.neural_networks.NeuralNetwork.html>`__: The interface for neural networks. This is an abstract class all QNNs inherit from."
msgstr "`NeuralNetwork <https://qiskit.org/documentation/machine-learning/stubs/qiskit_machine_learning.neural_networks.NeuralNetwork.html>`__: ニューラルネットワークのインターフェイス。これは全ての QNN が継承する抽象クラスです。"

#: ../../tutorials/01_neural_networks.ipynb:72
msgid "`EstimatorQNN <https://qiskit.org/documentation/machine-learning/stubs/qiskit_machine_learning.neural_networks.EstimatorQNN.html>`__: A network based on the evaluation of quantum mechanical observables."
msgstr "`EstimatorQNN <https://qiskit.org/documentation/machine-learning/stubs/qiskit_machine_learning.neural_networks.EstimatorQNN.html>`__: 量子力学的観測可能量の評価に基づくネットワーク。"

#: ../../tutorials/01_neural_networks.ipynb:73
msgid "`SamplerQNN <https://qiskit.org/documentation/machine-learning/locale/fr_FR/stubs/qiskit_machine_learning.neural_networks.SamplerQNN.html>`__: A network based on the samples resulting from measuring a quantum circuit."
msgstr "`SamplerQNN <https://qiskit.org/documentation/machine-learning/locale/fr_FR/stubs/qiskit_machine_learning.neural_networks.SamplerQNN.html>`__: 量子回路の測定から得られたサンプルを基にしたネットワーク。"

#: ../../tutorials/01_neural_networks.ipynb:75
msgid "These implementations are based on the `qiskit primitives <https://qiskit.org/documentation/apidoc/primitives.html>`__. The primitives are the entry point to run QNNs on either a simulator or real quantum hardware. Each implementation, ``EstimatorQNN`` and ``SamplerQNN``, takes in an optional instance of its corresponding primitive, which can be any subclass of ``BaseEstimator`` and ``BaseSampler``, respectively."
msgstr "これらの実装は `qiskit primitive <https://qiskit.org/documentation/apidoc/primitives.html>`__ に基づいています。 primitiveは、シミュレーターまたは実際の量子ハードウェア上で QNN を実行するエントリー・ポイントです。 それぞれの実装である ``EstimatorQNN`` と ``SamplerQNN`` は、対応するprimitive (``BaseEstimator`` と ``BaseSampler`` の任意のサブクラス) のオプション・インスタンスを取り込みます。"

#: ../../tutorials/01_neural_networks.ipynb:77
msgid "The ``qiskit.primitives`` module provides a reference implementation for the ``Sampler`` and ``Estimator`` classes to run statevector simulations. By default, if no instance is passed to a QNN class, an instance of the corresponding reference primitive (``Sampler`` or ``Estimator``) is created automatically by the network. For more information about primitives please refer to the `primitives documentation <https://qiskit.org/documentation/apidoc/primitives.html>`__."
msgstr "``qiskit.primitives`` モジュールは、状態ベクトル・シミュレーションを実行するための ``Sampler`` と ``Estimator`` クラスの参照実装を提供します。 デフォルトでは、QNNクラスにインスタンスが渡されていない場合、対応する参照primitiveのインスタンス (``Sampler`` または ``Estimator``) は、ネットワークによって自動的に作成されます。 primitiveについての詳細は、 `primitiveドキュメント <https://qiskit.org/documentation/apidoc/primitives.html>`__ を参照してください。"

#: ../../tutorials/01_neural_networks.ipynb:79
msgid "The ``NeuralNetwork`` class is the interface for all QNNs available in ``qiskit-machine-learning``. It exposes a forward and a backward pass that take data samples and trainable weights as input."
msgstr "``NeuralNetwork`` クラスは、``qiskit-machine-learning`` で利用可能なすべての QNN のインターフェイスです。 これは、データ・サンプルと訓練可能な重みを入力として取るフォワードおよびバックワード・パスを公開します。"

#: ../../tutorials/01_neural_networks.ipynb:81
msgid "It's important to note that ``NeuralNetwork``\\ s are \"stateless\". They do not contain any training capabilities (these are pushed to the actual algorithms or applications: `classifiers <https://qiskit.org/documentation/machine-learning/apidocs/qiskit_machine_learning.algorithms.html#classifiers>`__, `regressors <https://qiskit.org/documentation/machine-learning/apidocs/qiskit_machine_learning.algorithms.html#regressors>`__, etc), nor do they store the values for trainable weights."
msgstr "``NeuralNetwork`` は「ステートレス」であることに注意する必要があります。学習能力を有さず (学習能力は実際のアルゴリズムやアプリケーションに任されます: `分類器 <https://qiskit.org/documentation/machine-learning/apidocs/qiskit_machine_learning.algorithms.html#classifiers>`__, `回帰器 <https://qiskit.org/documentation/machine-learning/apidocs/qiskit_machine_learning.algorithms.html#regressors>`__, など) 、訓練可能ウェイトの値を保持しません。"

#: ../../tutorials/01_neural_networks.ipynb:94
msgid "Let's now look into specific examples for the two ``NeuralNetwork`` implementations. But first, let's set the algorithmic seed to ensure that the results don't change between runs."
msgstr "では、2 つの ``NeuralNetwork`` 実装の具体的な例を見てみましょう。 しかし、まず、実行間で結果が変わらないようにアルゴリズムのシードを設定しましょう。"

#: ../../tutorials/01_neural_networks.ipynb:118
msgid "2. How to Instantiate QNNs"
msgstr "2. QNNをインスタンス化する方法"

#: ../../tutorials/01_neural_networks.ipynb:121
msgid "2.1. ``EstimatorQNN``"
msgstr "2.1. ``EstimatorQNN``"

#: ../../tutorials/01_neural_networks.ipynb:123
msgid "The ``EstimatorQNN`` takes in a parametrized quantum circuit as input, as well as an optional quantum mechanical observable, and outputs expectation value computations for the forward pass. The ``EstimatorQNN`` also accepts lists of observables to construct more complex QNNs."
msgstr "``EstimatorQNN`` は、オプションの量子力学的観測可能量と同様に、パラメーター化された量子回路を入力として取り込み、フォワード・パスの期待値計算を出力します。 ``EstimatorQNN`` は、より複雑な QNN を構築するために観測量のリストも受け入れます。"

#: ../../tutorials/01_neural_networks.ipynb:125
msgid "Let's see an ``EstimatorQNN`` in action with a simple example. We start by constructing the parametrized circuit. This quantum circuit has two parameters, one represents a QNN input and the other represents a trainable weight:"
msgstr "簡単な例を使って、 ``EstimatorQNN`` の動きを見てみましょう。パラメーター化回路の構築から始めます。この量子回路には2つのパラメータがあります。1つはQNNの入力を表し、もう1つは訓練可能な重みを表します。"

#: ../../tutorials/01_neural_networks.ipynb:163
msgid "We can now create an observable to define the expectation value computation. If not set, then the ``EstimatorQNN`` will automatically create the default observable :math:`Z^{\\otimes n}`. Here, :math:`n` is the number of qubits of the quantum circuit."
msgstr "これで、期待値計算を定義するための観測量を作成できるようになりました。 設定していない場合、 ``EstimatorQNN`` は自動的にデフォルトの観測量 :math:`Z^{\\otimes n}` :math:を作成します。 ここで、:math:`n` は量子回路の量子ビット数です。"

#: ../../tutorials/01_neural_networks.ipynb:165
msgid "In this example, we will change things up and use the :math:`Y^{\\otimes n}` observable:"
msgstr "この例では、一転して :math:`Y^{\\otimes n}` 観測量を使用します。"

#: ../../tutorials/01_neural_networks.ipynb:188
msgid "Together with the quantum circuit defined above, and the observable we have created, the ``EstimatorQNN`` constructor takes in the following keyword arguments:"
msgstr "上記で定義された量子回路と作成した観測量と共に、 ``EstimatorQNN`` のコンストラクタは次のキーワード引数を取ります。"

#: ../../tutorials/01_neural_networks.ipynb:190
msgid "``estimator``: optional primitive instance"
msgstr "``estimator``: オプションのprimitiveインスタンス"

#: ../../tutorials/01_neural_networks.ipynb:191
msgid "``input_params``: list of quantum circuit parameters that should be treated as \"network inputs\""
msgstr "``input_params``: 「ネットワーク入力」として扱うべき量子回路パラメーターのリスト"

#: ../../tutorials/01_neural_networks.ipynb:192
msgid "``weight_params``: list of quantum circuit parameters that should be treated as \"network weights\""
msgstr "``weight_params``:「ネットワークの重み」として扱われるべき量子回路パラメーターのリスト"

#: ../../tutorials/01_neural_networks.ipynb:194
msgid "In this example, we previously decided that the first parameter of ``params1`` should be the input, while the second should be the weight. As we are performing a local statevector simulation, we will not set the ``estimator`` parameter; the network will create an instance of the reference ``Estimator`` primitive for us. If we needed to access cloud resources or ``Aer`` simulators, we would have to define the respective ``Estimator`` instances and pass them to the ``EstimatorQNN``."
msgstr "この例では、``params1`` の最初のパラメーターを入力とし、2番目のパラメーターを重みとしました。 ローカルの状態ベクトル・シミュレーションを実行しているので、``estimator`` パラメーターを設定しません。ネットワークは、参照 ``Estimator`` primitiveのインスタンスを作成します。 クラウド・リソースや ``Aer`` シミュレーターにアクセスする必要がある場合は、それぞれの ``Estimator`` インスタンスを定義し、それらを ``EstimatorQNN`` に渡す必要があります。"

#: ../../tutorials/01_neural_networks.ipynb:245
msgid "We'll see how to use the QNN in the following sections, but before that, let's check out the ``SamplerQNN`` class."
msgstr "次のセクションでQNNの使い方を見ていきますが、その前に ``SamplerQNN`` クラスを見てみましょう。"

#: ../../tutorials/01_neural_networks.ipynb:257
msgid "2.2. ``SamplerQNN``"
msgstr "2.2. ``SamplerQNN``"

#: ../../tutorials/01_neural_networks.ipynb:259
msgid "The ``SamplerQNN`` is instantiated in a similar way to the ``EstimatorQNN``, but because it directly consumes samples from measuring the quantum circuit, it does not require a custom observable."
msgstr "``SamplerQNN`` は、``EstimatorQNN`` と同様にインスタンス化されます。 しかし、量子回路の測定から直接サンプルを吸収するので、カスタム観測量を必要としません。"

#: ../../tutorials/01_neural_networks.ipynb:261
msgid "These output samples are interpreted by default as the probabilities of measuring the integer index corresponding to a bitstring. However, the ``SamplerQNN`` also allows us to specify an ``interpret`` function to post-process the samples. This function should be defined so that it takes a measured integer (from a bitstring) and maps it to a new value, i.e. non-negative integer."
msgstr "これらの出力サンプルは、デフォルトでは、ビット文字列に対応する整数インデックスを測定する確率として解釈されます。 しかし、``SamplerQNN`` ではサンプルを後処理するための ``interpret`` 関数を指定することもできます。 この関数は (ビット文字列から) 測定された整数を取って、それを新しい値、すなわち非負の整数にマップするように定義する必要があります。"

#: ../../tutorials/01_neural_networks.ipynb:263
msgid "**(!)** It's important to note that if a custom ``interpret`` function is defined, the ``output_shape`` cannot be inferred by the network, and **needs to be provided explicitly**."
msgstr "**(!)** カスタムの ``interpret`` 関数が定義されている場合、 ``output_shape`` はネットワークでは推論できないので、 **明示的に提供する必要がある** ことに注意してください。"

#: ../../tutorials/01_neural_networks.ipynb:265
msgid "**(!)** It's also important to keep in mind that if no ``interpret`` function is used, the dimension of the probability vector will scale exponentially with the number of qubits. With a custom ``interpret`` function, this scaling can change. If, for instance, an index is mapped to the parity of the corresponding bitstring, i.e., to 0 or 1, the result will be a probability vector of length 2 independently of the number of qubits."
msgstr "**(!)** ``interpret`` 関数を使用しない場合、確率ベクトルの次元は量子ビット数に応じて指数関数的に増加することにも注意してください。 カスタム ``interpret`` 関数を使用する場合、この増加を変更できます。例えば、インデックスが対応するビット文字列のパリティにマッピングされる場合、つまり0または1にマッピングされる場合、結果は量子ビット数に関係なく長さ2の確率ベクトルになります。"

#: ../../tutorials/01_neural_networks.ipynb:276
msgid "Let's create a different quantum circuit for the ``SamplerQNN``. In this case, we will have two input parameters and four trainable weights that parametrize a two-local circuit."
msgstr "``SamplerQNN`` 用に別の量子回路を作成しましょう。 この場合、2 つの入力パラメーターと 4 つの訓練可能な重みを持ち、2 つのローカル回路をパラメーター化します。"

#: ../../tutorials/01_neural_networks.ipynb:351
msgid "Similarly to the ``EstimatorQNN``, we must specify inputs and weights when instantiating the ``SamplerQNN``. In this case, the keyword arguments will be: - ``sampler``: optional primitive instance - ``input_params``: list of quantum circuit parameters that should be treated as \"network inputs\" - ``weight_params``: list of quantum circuit parameters that should be treated as \"network weights\""
msgstr "``EstimatorQNN`` と同様に、``SamplerQNN`` をインスタンス化する際に入力と重みを指定する必要があります。 この場合、キーワード引数は以下のようになります: - ``sampler``: オプションのprimitiveインスタンス - ``input_params``: 「ネットワーク入力」として扱われるべき量子回路パラメーターのリスト - ``weight_params``: 「ネットワーク重み」として扱われるべき量子回路パラメーターのリスト"

#: ../../tutorials/01_neural_networks.ipynb:353
msgid "Please note that, once again, we are choosing not to set the ``Sampler`` instance to the QNN and relying on the default."
msgstr "再び、``Sampler`` インスタンスを QNN に設定しないことを選択し、デフォルトに依存していることに注意してください。"

#: ../../tutorials/01_neural_networks.ipynb:402
msgid "In addition to the basic arguments shown above, the ``SamplerQNN`` accepts three more settings: ``input_gradients``, ``interpret``, and ``output_shape``. These will be introduced in sections 4 and 5."
msgstr "上記の基本的な引数に加えて、``SamplerQNN`` は ``input_gradients`` と ``interpret`` と ``output_shape`` の 3 つの設定を受け付けます。 これらはセクション4とセクション5で紹介されます。"

#: ../../tutorials/01_neural_networks.ipynb:414
msgid "3. How to Run a Forward Pass"
msgstr "3. フォワード・パスの実行方法"

#: ../../tutorials/01_neural_networks.ipynb:426
msgid "3.1. Set-Up"
msgstr "3.1. セットアップ"

#: ../../tutorials/01_neural_networks.ipynb:428
msgid "In a real setting, the inputs would be defined by the dataset, and the weights would be defined by the training algorithm or as part of a pre-trained model. However, for the sake of this tutorial, we will specify random sets of input and weights of the right dimension:"
msgstr "実際の設定では、入力はデータセットによって定義され、重みは学習アルゴリズムや学習済みモデルの一部として定義されます。しかし、このチュートリアルのために、適切な次元の入力と重みのランダムなセットを指定します。"

#: ../../tutorials/01_neural_networks.ipynb:440
msgid "3.1.1. ``EstimatorQNN`` Example"
msgstr "3.1.1. ``EstimatorQNN`` の例"

#: ../../tutorials/01_neural_networks.ipynb:511
msgid "3.1.2. ``SamplerQNN`` Example"
msgstr "3.1.2. ``SamplerQNN`` の例"

#: ../../tutorials/01_neural_networks.ipynb:581
msgid "Once we have the inputs and the weights, let's see the results for batched and non-batched passes."
msgstr "入力と重みを設定したら、バッチパスと非バッチパスの結果を確認しましょう。"

#: ../../tutorials/01_neural_networks.ipynb:593
msgid "3.2. Non-batched Forward Pass"
msgstr "3.2. 非バッチ・フォワード・パス"

#: ../../tutorials/01_neural_networks.ipynb:605
msgid "3.2.1. ``EstimatorQNN`` Example"
msgstr "3.2.1. ``EstimatorQNN`` の例"

#: ../../tutorials/01_neural_networks.ipynb:616
msgid "For the ``EstimatorQNN``, the expected output shape for the forward pass is ``(1, num_qubits * num_observables)`` where ``1`` in our case is the number of samples:"
msgstr "``EstimatorQNN`` では、フォワード・パスの期待される出力形状は ``(1, num_qubits * num_observables)`` です。ここで ``1`` はサンプル数です:"

#: ../../tutorials/01_neural_networks.ipynb:669
msgid "3.2.2. ``SamplerQNN`` Example"
msgstr "3.2.2. ``SamplerQNN`` の例"

#: ../../tutorials/01_neural_networks.ipynb:680
msgid "For the ``SamplerQNN`` (without a custom interpret function), the expected output shape for the forward pass is ``(1, 2**num_qubits)``. With a custom interpret function, the output shape will be ``(1, output_shape)``, where ``1`` in our case is the number of samples:"
msgstr "``SamplerQNN`` (カスタム interpret 関数なし) では、フォワード・パスの期待される出力形状は ``(1, 2**num_qubits)`` になります。 カスタム interpret 関数を使用すると、出力形状は ``(1, output_shape)`` になります。ここで、``1`` はサンプル数です。"

#: ../../tutorials/01_neural_networks.ipynb:733
msgid "3.3. Batched Forward Pass"
msgstr "3.3. バッチ・フォワード・パス"

#: ../../tutorials/01_neural_networks.ipynb:745
msgid "3.3.1. ``EstimatorQNN`` Example"
msgstr "3.3.1. ``EstimatorQNN`` の例"

#: ../../tutorials/01_neural_networks.ipynb:756
msgid "For the ``EstimatorQNN``, the expected output shape for the forward pass is ``(batch_size, num_qubits * num_observables)``:"
msgstr "``EstimatorQNN`` では、フォワード・パスの期待される出力形状は``(batch_size, num_qubits * num_observables)`` になります。"

#: ../../tutorials/01_neural_networks.ipynb:814
msgid "3.3.2. ``SamplerQNN`` Example"
msgstr "3.3.2. ``SamplerQNN`` の例"

#: ../../tutorials/01_neural_networks.ipynb:825
msgid "For the ``SamplerQNN`` (without custom interpret function), the expected output shape for the forward pass is ``(batch_size, 2**num_qubits)``. With a custom interpret function, the output shape will be ``(batch_size, output_shape)``."
msgstr "``SamplerQNN`` (カスタム interpret 関数なし) では、フォワード・パスの期待される出力形状は ``(batch_size, 2**num_qubits)`` になります。 カスタム interpret 関数を使用すると、出力形状は ``(batch_size, output_shape)`` になります。"

#: ../../tutorials/01_neural_networks.ipynb:883
msgid "4. How to Run a Backward Pass"
msgstr "4. バックワード・パスの実行方法"

#: ../../tutorials/01_neural_networks.ipynb:885
msgid "Let's take advantage of the inputs and weights defined above to show how the backward pass works. This pass returns a tuple ``(input_gradients, weight_gradients)``. By default, the backward pass will only calculate gradients with respect to the weight parameters."
msgstr "上記で定義した入力と重みを利用して、バックワード・パスの動作を示しましょう。このパスはタプル ``(input_gradients, weight_gradients)`` を返します。 デフォルトでは、バックワード・パスは重みパラメーターに対する勾配のみを計算します。"

#: ../../tutorials/01_neural_networks.ipynb:887
msgid "If you want to enable gradients with respect to the input parameters, you should set the following flag during the QNN instantiation:"
msgstr "入力パラメーターに対して勾配を有効にする場合は、QNN インスタンスを作成するときに次のフラグを設定する必要があります。"

#: ../../tutorials/01_neural_networks.ipynb:893
msgid "Please remember that input gradients are **required** for the use of ``TorchConnector`` for PyTorch integration."
msgstr "PyTorchとの統合に ``TorchConnector`` を使用するには、入力勾配が **必要** であることを忘れないでください。"

#: ../../tutorials/01_neural_networks.ipynb:905
msgid "4.1. Backward Pass without Input Gradients"
msgstr "4.1. 入力勾配のないバックワード・パス"

#: ../../tutorials/01_neural_networks.ipynb:917
msgid "4.1.1. ``EstimatorQNN`` Example"
msgstr "4.1.1. ``EstimatorQNN`` の例"

#: ../../tutorials/01_neural_networks.ipynb:928
msgid "For the ``EstimatorQNN``, the expected output shape for the weight gradients is ``(batch_size, num_qubits * num_observables, num_weights)``:"
msgstr "``EstimatorQNN`` では、重み勾配の期待される出力形状は ``(batch_size, num_qubits * num_observables, num_weights)`` になります。"

#: ../../tutorials/01_neural_networks.ipynb:992
msgid "4.1.2. ``SamplerQNN`` Example"
msgstr "4.1.2. ``SamplerQNN`` の例"

#: ../../tutorials/01_neural_networks.ipynb:1003
msgid "For the ``SamplerQNN`` (without custom interpret function), the expected output shape for the forward pass is ``(batch_size, 2**num_qubits, num_weights)``. With a custom interpret function, the output shape will be ``(batch_size, output_shape, num_weights)``.:"
msgstr "``SamplerQNN`` (カスタム interpret 関数なし) では、フォワード・パスの期待される出力形状は ``(batch_size, 2**num_qubits, num_weights)`` になります。 カスタム interpret 関数を使用すると、出力形状は ``(batch_size, output_shape, num_weights)`` になります。"

#: ../../tutorials/01_neural_networks.ipynb:1076
msgid "4.2. Backward Pass with Input Gradients"
msgstr "4.2. 入力勾配のあるバックワード・パス"

#: ../../tutorials/01_neural_networks.ipynb:1078
msgid "Let's enable the ``input_gradients`` to show what the expected output sizes are for this option."
msgstr "``input_gradients`` を有効にして、このオプションで期待される出力サイズを表示しましょう。"

#: ../../tutorials/01_neural_networks.ipynb:1101
msgid "4.2.1. ``EstimatorQNN`` Example"
msgstr "4.2.1. ``EstimatorQNN`` の例"

#: ../../tutorials/01_neural_networks.ipynb:1112
msgid "For the ``EstimatorQNN``, the expected output shape for the input gradients is ``(batch_size, num_qubits * num_observables, num_inputs)``:"
msgstr "``EstimatorQNN`` では、重み勾配の期待される出力形状は ``(batch_size, num_qubits * num_observables, num_inputs)`` になります。"

#: ../../tutorials/01_neural_networks.ipynb:1176
msgid "4.2.2. ``SamplerQNN`` Example"
msgstr "4.2.2. ``SamplerQNN`` の例"

#: ../../tutorials/01_neural_networks.ipynb:1187
msgid "For the ``SamplerQNN`` (without custom interpret function), the expected output shape for the input gradients is ``(batch_size, 2**num_qubits, num_inputs)``. With a custom interpret function, the output shape will be ``(batch_size, output_shape, num_inputs)``."
msgstr "``SamplerQNN`` (カスタム interpret 関数なし) では、入力勾配に対する期待される出力形状は ``(batch_size, 2**num_qubits, num_inputs)`` になります。 カスタム interpret 関数を使用すると、出力形状は ``(batch_size, output_shape, num_inputs)`` になります。"

#: ../../tutorials/01_neural_networks.ipynb:1269
msgid "5. Advanced Functionality"
msgstr "5. 拡張機能"

#: ../../tutorials/01_neural_networks.ipynb:1281
msgid "5.1. ``EstimatorQNN`` with Multiple Observables"
msgstr "5.1. 複数の観測量を持つ ``EstimatorQNN``"

#: ../../tutorials/01_neural_networks.ipynb:1292
msgid "The ``EstimatorQNN`` allows to pass lists of observables for more complex QNN architectures. For example (note the change in output shape):"
msgstr "``EstimatorQNN`` では、より複雑な QNN アーキテクチャーのために観測量のリストを渡すことができます。例えば (出力形状の変化に注意してください)"

#: ../../tutorials/01_neural_networks.ipynb:1372
msgid "5.2. ``SamplerQNN`` with custom ``interpret``"
msgstr "5.2. カスタム ``interpret`` 付き ``SamplerQNN``"

#: ../../tutorials/01_neural_networks.ipynb:1383
msgid "One common ``interpret`` method for ``SamplerQNN`` is the ``parity`` function, which allows it to perform binary classification. As explained in the instantiation section, using interpret functions will modify the output shape of the forward and backward passes. In the case of the parity interpret function, ``output_shape`` is fixed to ``2``. Therefore, the expected forward and weight gradient shapes are ``(batch_size, 2)`` and ``(batch_size, 2, num_weights)``, respectively:"
msgstr "``SamplerQNN`` の一般的な ``interpret`` メソッドの 1 つは ``parity`` 関数で、バイナリ分類を行うことができます。 インスタンス作成のセクションで説明したように、interpret 関数を使うことで、フォワードとバックワード・パスの出力形状が変更されます。 パリティinterpret関数の場合、``output_shape`` は ``2`` に固定されます。 そこで期待される前方および重み勾配の形状は、おそれぞれ ``(batch_size, 2)`` と ``(batch_size, 2, num_weights)`` です。"

#: ../../tutorials/01_neural_networks.ipynb:1465
msgid "6. Conclusion"
msgstr "6. 結論"

#: ../../tutorials/01_neural_networks.ipynb:1467
msgid "In this tutorial, we introduced the two neural networks classes provided by ``qiskit-machine-learning``, namely the ``EstimatorQNN`` and ``SamplerQNN``, which extend the base ``NeuralNetwork`` class. We provided some theoretical background, the key steps for QNN initialization, basic use in forward and backward passes, and advanced functionality."
msgstr "このチュートリアルでは、``qiskit-machine-learning`` によって提供される2つのニューラルネットワーク・クラス、つまり ``EstimatorQNN`` と ``SamplerQNN`` について紹介しました。 QNN初期化のための重要なステップ、フォワードおよびバックワード・パスでの基本的な使用法、および高度な機能のいくつかの理論的背景を提供しました。"

#: ../../tutorials/01_neural_networks.ipynb:1469
msgid "We now encourage you to play around with the problem setup and see how different circuit sizes, input, and weight parameter lengths influence the output shapes."
msgstr "この問題設定を扱って、さまざまな回路のサイズ、入力、重みパラメーターの長さが、出力形状にどのように影響を与えるか確認することをお勧めします。"

