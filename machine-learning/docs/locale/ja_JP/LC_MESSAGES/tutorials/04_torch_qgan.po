msgid ""
msgstr ""
"Project-Id-Version: qiskit-docs\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-02-07 03:44+0000\n"
"PO-Revision-Date: 2023-02-16 05:48\n"
"Last-Translator: \n"
"Language: ja\n"
"Language-Team: Japanese\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.11.0\n"
"X-Crowdin-Project: qiskit-docs\n"
"X-Crowdin-Project-ID: 369271\n"
"X-Crowdin-Language: ja\n"
"X-Crowdin-File: /master/machine-learning/docs/locale/en/LC_MESSAGES/tutorials/04_torch_qgan.po\n"
"X-Crowdin-File-ID: 9885\n"

#: ../../tutorials/04_torch_qgan.ipynb:9
msgid "This page was generated from `docs/tutorials/04_torch_qgan.ipynb`__."
msgstr "このページは `docs/tutorials/04_torch_qgan.ipynb`__ から生成されました。"

#: ../../tutorials/04_torch_qgan.ipynb:9
msgid "PyTorch qGAN Implementation"
msgstr "PyTorchによるqGANの実装"

#: ../../tutorials/04_torch_qgan.ipynb:12
msgid "Overview"
msgstr "概要"

#: ../../tutorials/04_torch_qgan.ipynb:14
msgid "This tutorial introduces step-by-step how to build a PyTorch-based Quantum Generative Adversarial Network algorithm."
msgstr "このチュートリアルでは、PyTorchベースの量子敵対的生成ネットワークのアルゴリズムを構築する方法をステップバイステップで紹介します。"

#: ../../tutorials/04_torch_qgan.ipynb:17
msgid "Context"
msgstr "説明"

#: ../../tutorials/04_torch_qgan.ipynb:19
msgid "The qGAN [1] is a hybrid quantum-classical algorithm used for generative modeling tasks. The algorithm uses the interplay of a quantum generator :math:`G_{\\theta}`, i.e., an ansatz, and a classical discriminator :math:`D_{\\phi}`, a neural network, to learn the underlying probability distribution given training data."
msgstr "QGAN [1]は、生成的なモデリングを行うために使用されるハイブリッド量子古典アルゴリズムです。このアルゴリズムでは、量子生成器 :math:`G_{theta}` (ansatz) と古典識別器 :math:`D_{phi}` (ニューラルネットワーク) の相互作用により、学習データから基本的な確率分布の学習を行います。"

#: ../../tutorials/04_torch_qgan.ipynb:21
msgid "The generator and discriminator are trained in alternating optimization steps, where the generator aims at generating samples that will be classified by the discriminator as training data samples (i.e, samples extracted from the real training distribution), and the discriminator tries to differentiate between original training data samples and data samples from the generator (in other words, telling apart the real and generated distributions). The final goal is for the quantum generator to learn a representation for the training data’s underlying probability distribution. The trained quantum generator can, thus, be used to load a quantum state which is an approximate model of the target distribution."
msgstr "生成器と識別器は交互に最適化され、生成器は識別器が学習データのサンプルとして分類するサンプル（実際の学習分布から抽出したサンプル）の生成を目指し、識別器は元の学習データサンプルと生成器のデータサンプルを区別する（つまり、実際の分布と生成した分布を区別する）ことを目指します。最終的な目標は、量子生成器が学習データの確率分布の表現を学習することです。この学習により、量子生成器は、学習データの確率分布を近似的に表現した量子状態を読み込むことができるようになります。"

#: ../../tutorials/04_torch_qgan.ipynb:24
msgid "**References:**"
msgstr "**参考文献: **"

#: ../../tutorials/04_torch_qgan.ipynb:26
msgid "[1] Zoufal et al., `Quantum Generative Adversarial Networks for learning and loading random distributions <https://www.nature.com/articles/s41534-019-0223-2>`__"
msgstr "[1] Zoufal et al., `Quantum Generative Adversarial Networks for learning and loading random distributions <https://www.nature.com/articles/s41534-019-0223-2>`__"

#: ../../tutorials/04_torch_qgan.ipynb:29
msgid "Application: qGANs for Loading Random Distributions"
msgstr "応用例：qGANによるランダム分布の読み込み"

#: ../../tutorials/04_torch_qgan.ipynb:31
msgid "Given :math:`k`-dimensional data samples, we employ a quantum Generative Adversarial Network (qGAN) to learn the data’s underlying random distribution and to load it directly into a quantum state:"
msgstr ":math:`k` 次元のデータサンプルが与えられた場合、量子敵対的生成ネットワーク(qGAN) を使用して、データの基礎となるランダム分布を学習し、それを量子状態に直接ロードします。"

#: ../../tutorials/04_torch_qgan.ipynb:33
msgid "\\big| g_{\\theta}\\rangle = \\sum_{j=0}^{2^n-1} \\sqrt{p_{\\theta}^{j}}\\big| j \\rangle\n\n"
msgstr "\\big| g_{\\theta}\\rangle = \\sum_{j=0}^{2^n-1} \\sqrt{p_{\\theta}^{j}}\\big| j \\rangle\n\n"

#: ../../tutorials/04_torch_qgan.ipynb:35
msgid "where :math:`p_{\\theta}^{j}` describe the occurrence probabilities of the basis states :math:`\\big| j\\rangle`."
msgstr "ここで :math:`p_{\\theta}^{j}` は、基底状態 :math:`\\big| j\\rangle` の発生確率を表します。"

#: ../../tutorials/04_torch_qgan.ipynb:37
msgid "The aim of the qGAN training is to generate a state :math:`\\big| g_{\\theta}\\rangle` where :math:`p_{\\theta}^{j}`, for :math:`j\\in \\left\\{0, \\ldots, {2^n-1} \\right\\}`, describe a probability distribution that is close to the distribution underlying the training data :math:`X=\\left\\{x^0, \\ldots, x^{k-1} \\right\\}`."
msgstr "qGAN学習の目的は、状態 :math:`\\big| g_{\\theta}\\rangle` を生成することです。ここで、 :math:`p_{\\theta}^{j}` は、 :math:`j\\in \\left\\{0, \\ldots, {2^n-1} \\right\\}` の場合、 学習データ :math:`X=\\left\\{x^0, \\ldots, x^{k-1} \\right\\}` の基礎となる分布に近い確率分布を表します。"

#: ../../tutorials/04_torch_qgan.ipynb:39
msgid "For further details please refer to `Quantum Generative Adversarial Networks for Learning and Loading Random Distributions <https://arxiv.org/abs/1904.00043>`__ *Zoufal, Lucchi, Woerner* [2019]."
msgstr "詳細については、 `Quantum Generative Adversarial Networks for Learning and Loading Random Distributions <https://arxiv.org/abs/1904.00043>`__ *Zoufal, Lucchi, Woerner* [2019] を参照してください。"

#: ../../tutorials/04_torch_qgan.ipynb:41
msgid "For an example of how to use a trained qGAN in an application, the pricing of financial derivatives, please see the `Option Pricing with qGANs <https://github.com/Qiskit/qiskit-finance/tree/main/docs/tutorials/10_qgan_option_pricing.ipynb>`__ tutorial."
msgstr "学習済みの qGAN の実用例として、金融デリバティブの価格設定については、チュートリアルの `qGANs によるオプション価格推定 <https://github.com/Qiskit/qiskit-finance/tree/main/docs/tutorials/10_qgan_option_pricing.ipynb>`__ を参照してください。"

#: ../../tutorials/04_torch_qgan.ipynb:44
msgid "Tutorial"
msgstr "チュートリアル"

#: ../../tutorials/04_torch_qgan.ipynb:47
msgid "Data and Representation"
msgstr "データと表現"

#: ../../tutorials/04_torch_qgan.ipynb:49
msgid "First, we need to load our training data :math:`X`."
msgstr "まず、学習データ :math:`X` をロードする必要があります。"

#: ../../tutorials/04_torch_qgan.ipynb:51
msgid "In this tutorial, the training data is given by samples from a 2D multivariate normal distribution."
msgstr "このチュートリアルでは、学習データは2次元多変量正規分布からのサンプルで与えられます。"

#: ../../tutorials/04_torch_qgan.ipynb:53
msgid "The goal of the generator is to learn how to represent such distribution, and the trained generator should correspond to an :math:`n`-qubit quantum state :nbsphinx-math:`\\begin{equation} |g_{\\text{trained}}\\rangle=\\sum\\limits_{j=0}^{k-1}\\sqrt{p_{j}}|x_{j}\\rangle, \\end{equation}` where the basis states :math:`|x_{j}\\rangle` represent the data items in the training data set :math:`X={x_0, \\ldots, x_{k-1}}` with :math:`k\\leq 2^n` and :math:`p_j` refers to the sampling probability of :math:`|x_{j}\\rangle`."
msgstr "生成器の目標は、そのような分布を表す方法を学習することであり、トレーニングされた生成器は :math:`n`-qubit 量子状態に対応する必要があります :nbsphinx-math:`\\begin{equation} |g_{\\text{trained }}\\rangle=\\sum\\limits_{j=0}^{k-1}\\sqrt{p_{j}}|x_{j}\\rangle, \\end{equation}` ここで、基底状態 :math:`|x_{j}\\rangle` はトレーニング データ セット内のデータ項目を表します :math:`X={x_0, \\ldots, x_{k-1}}` with :math:` k\\leq 2^n` と :math:`p_j` は :math:`|x_{j}\\rangle` のサンプリング確率を指します。"

#: ../../tutorials/04_torch_qgan.ipynb:57
msgid "To facilitate this representation, we need to map the samples from the multivariate normal distribution to discrete values. The number of values that can be represented depends on the number of qubits used for the mapping. Hence, the data resolution is defined by the number of qubits. If we use :math:`3` qubits to represent one feature, we have :math:`2^3 = 8` discrete values."
msgstr "この表現を容易にするために、多変量正規分布からのサンプルを離散的な値にマッピングする必要があります。表現できる値の数はマッピングに使用する量子ビットの数に依存します。したがって、データの分解能は量子ビットの数によって定義されます。math:`3` 個の量子ビットを使って 1 つの特徴を表現すると、 :math:`2^3 = 8` 個の離散値を持つことになります。"

#: ../../tutorials/04_torch_qgan.ipynb:59
msgid "We first begin by fixing seeds in the random number generators, then we will import the libraries and packages we will need for this tutorial."
msgstr "まず、乱数発生器のシードを固定することから始め、このチュートリアルで必要なライブラリーやパッケージをインポートします。"

#: ../../tutorials/04_torch_qgan.ipynb:84
msgid "Then, we sample some data from a 2D multivariate normal distribution as described above."
msgstr "次に、上述したように2次元の多変量正規分布からいくつかのデータをサンプリングします。"

#: ../../tutorials/04_torch_qgan.ipynb:132
msgid "Let’s visualize our distribution and samples. The ``discretize_and_truncate`` function discretized the classical data we got from the multivariate normal so, a plot of the probability density function looks very coarse. But it is still a bell-shaped bivariate normal distribution."
msgstr "分布とサンプルを可視化しましょう。離散化関数 ``discretize_and_truncate`` は、多変量正規分布から得られた古典的なデータを離散化するので、確率密度関数のプロットは非常に粗く見えます。しかし、それはベル型の2変量正規分布のままです。"

#: ../../tutorials/04_torch_qgan.ipynb:194
msgid "On the next figure we prove that the training data was discretized. We passed data resolution as an array ``[3, 3]``, This array defined the number of qubits used to represent each data dimension. Thus, we can define :math:`2^3 = 8` discrete values and all training samples should fall under one of these discrete values. On the next figure there is a histogram for each random variable and to see actual discretization we increased the number of bins. Some of the bins are empty and there are 8 non-empty bins for each variable."
msgstr "次の図では、学習データが離散化されていることを証明しています。データの分解能は配列 ``[3, 3]`` として渡され、この配列は各データ次元を表現するために使用される量子ビットの数を定義しています。したがって、 :math:`2^3 = 8` の離散値を定義することができ、すべてのトレーニングサンプルはこれらの離散値のいずれかに該当するはずです。次の図は、各ランダム変数のヒストグラムです。実際の離散化を見るために、棒グラフの棒の数を増やしました。いくつかの棒は空で、各変数には空でない棒が8つあります。"

#: ../../tutorials/04_torch_qgan.ipynb:260
msgid "We move to PyTorch modeling and start from converting data arrays into tensors and create a data loader from our training data."
msgstr "PyTorchのモデリングに移り、データ配列をテンソルに変換するところから始め、学習データからデータローダーを作成します。"

#: ../../tutorials/04_torch_qgan.ipynb:289
msgid "Backend Configurations"
msgstr "バックエンドの構成"

#: ../../tutorials/04_torch_qgan.ipynb:291
msgid "Next, we need to choose a backend that is used to run the quantum generator. The presented method is compatible with all shot-based backends (qasm, fake hardware, real hardware) provided by Qiskit."
msgstr "次に、量子生成器を実行するために使用するバックエンドを選択する必要があります。提示した方法は、Qiskitが提供するすべてのショットベースのバックエンド（qasm、fake hardware、real hardware）と互換性があります。"

#: ../../tutorials/04_torch_qgan.ipynb:293
msgid "First, we create a quantum instance for the training where the batch size defines the number of shots."
msgstr "まず、学習用の量子インスタンスを作成し、バッチサイズによってショット数を定義します。"

#: ../../tutorials/04_torch_qgan.ipynb:318
msgid "Then we create a quantum instance for the evaluation purposes, we choose a higher number of shots to get better insights."
msgstr "次に、評価用の量子インスタンスを作成し、より良い知見を得るためにショット数を多くすることにしました。"

#: ../../tutorials/04_torch_qgan.ipynb:340
msgid "Initialize the quantum neural network ansatz"
msgstr "量子ニューラルネットワークのansatzの初期化"

#: ../../tutorials/04_torch_qgan.ipynb:342
msgid "Now, we define the parameterized quantum circuit :math:`G\\left(\\boldsymbol{\\theta}\\right)` with :math:`\\boldsymbol{\\theta} = {\\theta_1, ..., \\theta_k}` which will be used in our quantum generator."
msgstr "ここで、量子生成器に用いるパラメータ化量子回路 :math:`G\\left(\\boldsymbol{\\theta}\\right)` を :math:`\\boldsymbol{\\theta} = {\\theta_1, ..., \\theta_k}` で定義します。 これは、量子生成器で使用されます。"

#: ../../tutorials/04_torch_qgan.ipynb:344
msgid "To implement the quantum generator, we choose a depth-:math:`2` ansatz that implements :math:`R_Y` rotations and :math:`CX` gates which takes a uniform distribution as an input state. Notably, for :math:`k>1` the generator’s parameters must be chosen carefully. For example, the circuit depth should be more than :math:`1` because higher circuit depths enable the representation of more complex structures."
msgstr "量子生成器を実装するために、一様分布を入力に取るような、:math:`R_Y` 回転と :math:`CX` ゲートからなる深さ :math:`2` のansatzを用います。特に :math:`k>1` の場合、生成器のパラメーターは慎重に選ばれなければなりません。例えば、回路が深いほどより複雑な構造を表現できるので、回路の深さは :math:`1` より大きくなるべきです。"

#: ../../tutorials/04_torch_qgan.ipynb:374
msgid "Let’s draw our circuit and see what it looks like."
msgstr "回路を描いて見てみましょう。"

#: ../../tutorials/04_torch_qgan.ipynb:405
msgid "Definition of the quantum generator"
msgstr "量子生成器の定義"

#: ../../tutorials/04_torch_qgan.ipynb:407
msgid "Next, we define a function that creates the quantum generator from a given parameterized quantum circuit. As parameters this function takes a quantum instance to be used for data sampling. We wrap a created quantum neural network in ``TorchConnector`` to make use of PyTorch-based training."
msgstr "次に、パラメーター化された量子回路から量子生成器を作成する関数を定義します。パラメーターとして、データのサンプリングに使用する量子インスタンスを受け取ります。作成された量子ニューラルネットワークを ``TorchConnector`` でラップし、PyTorchベースの学習を利用することができるようにします。"

#: ../../tutorials/04_torch_qgan.ipynb:444
msgid "Definition of the classical discriminator"
msgstr "古典識別器の定義"

#: ../../tutorials/04_torch_qgan.ipynb:446
msgid "Next, we define a PyTorch-based classical neural network that represents the classical discriminator. The underlying gradients can be automatically computed with PyTorch."
msgstr "次に、古典的な識別器を表すPyTorchベースの古典的ニューラルネットワークを定義します。基礎となる勾配はPyTorchで自動的に計算することができます。"

#: ../../tutorials/04_torch_qgan.ipynb:485
msgid "Definition of the loss functions"
msgstr "損失関数の定義"

#: ../../tutorials/04_torch_qgan.ipynb:487
msgid "We want to train the generator and the discriminator with binary cross entropy as loss function:"
msgstr "損失関数として2値クロスエントロピーを用い、生成器と識別器を学習させたいと思います。"

#: ../../tutorials/04_torch_qgan.ipynb:489
msgid "L\\left(\\boldsymbol{\\theta}\\right)=\\sum_jp_j\\left(\\boldsymbol{\\theta}\\right)\\left[y_j\\log(x_j) + (1-y_j)\\log(1-x_j)\\right],\n\n"
msgstr "L\\left(\\boldsymbol{\\theta}\\right)=\\sum_jp_j\\left(\\boldsymbol{\\theta}\\right)\\left[y_j\\log(x_j) + (1-y_j)\\log(1-x_j)\\right],\n\n"

#: ../../tutorials/04_torch_qgan.ipynb:491
msgid "where :math:`x_j` refers to a data sample and :math:`y_j` to the corresponding label."
msgstr "ここで、 :math:`x_j` はデータサンプル、 :math:`y_j` はそれに対応するラベルを表します。"

#: ../../tutorials/04_torch_qgan.ipynb:517
msgid "Evaluation of custom gradients for the generator BCE loss function"
msgstr "生成器BCE損失関数のカスタム勾配の評価"

#: ../../tutorials/04_torch_qgan.ipynb:519
msgid "The evaluation of custom gradients for the quantum generator requires us to combine quantum gradients :math:`\\frac{\\partial p_j\\left(\\boldsymbol{\\theta}\\right)}{\\partial \\theta_l}` that we compute with Qiskit’s gradient framework with the binary cross entropy as follows:"
msgstr "量子生成器のカスタム勾配を評価するには、Qiskitのgradient frameworkで計算した量子勾配 :math:`frac{partial p_j̮left(\\boldsymbol{theta}right)}{partial \\theta_l}` と2値クロスエントロピーを次のように結合させる必要があります。"

#: ../../tutorials/04_torch_qgan.ipynb:521
msgid "\\frac{\\partial L\\left(\\boldsymbol{\\theta}\\right)}{\\partial \\theta_l} = \\sum_j \\frac{\\partial p_j\\left(\\boldsymbol{\\theta}\\right)}{\\partial \\theta_l} \\left[y_j\\log(x_j) + (1-y_j)\\log(1-x_j)\\right].\n\n"
msgstr "\\frac{\\partial L\\left(\\boldsymbol{\\theta}\\right)}{\\partial \\theta_l} = \\sum_j \\frac{\\partial p_j\\left(\\boldsymbol{\\theta}\\right)}{\\part \\theta_l} \\left[y_j\\log(x_j) + (1-y_j)\\log(1-x_j)\\right].\n\n"

#: ../../tutorials/04_torch_qgan.ipynb:523
msgid "First, we need to define how gradients will be evaluated. Depending on the backend, the gradients may be returned in a sparse format. Since PyTorch provides only a limited support for sparse gradients, we need to write a custom function for gradient based training."
msgstr "まず、勾配がどのように評価されるかを定義する必要があります。バックエンドによっては、gradientsはsparseなフォーマットで返されるかもしれません。PyTorchはスパース勾配を限定的にしかサポートしていないので、勾配ベースの学習用のカスタム関数を書く必要があります。"

#: ../../tutorials/04_torch_qgan.ipynb:548
msgid "Here, we define the custom function that evaluates gradients of the generator loss function considering the custom gradients of the quantum generator. As the parameters the function takes a list of parameter values and the discriminator as a classical neural network. The function returns a list of gradient values."
msgstr "ここでは、量子生成器のカスタム勾配を考慮した生成器損失関数の勾配を評価するカスタム関数を定義します。パラメーターとして，パラメーター値のリストと識別器（古典ニューラルネットワーク）を受け取ります。そして、この関数は勾配値のリストを返します。"

#: ../../tutorials/04_torch_qgan.ipynb:590
msgid "Relative entropy as benchmarking metric"
msgstr "ベンチマーク指標としての相対エントロピー"

#: ../../tutorials/04_torch_qgan.ipynb:592
msgid "The relative entropy describes a distance metric for distributions. Hence, we can use it to benchmark how close/far away the trained distribution is from the target distribution."
msgstr "相対エントロピーは，分布の距離指標を記述します。したがって、学習された分布が目的の分布にどれだけ近いか／遠いかを評価するために使うことができます。"

#: ../../tutorials/04_torch_qgan.ipynb:594
msgid "In the next function we computes relative entropy between target and trained distribution."
msgstr "次の関数では、目標とする分布と学習させた分布の相対的なエントロピーを計算します。"

#: ../../tutorials/04_torch_qgan.ipynb:624
msgid "Definition of the optimizers"
msgstr "オプティマイザーの定義"

#: ../../tutorials/04_torch_qgan.ipynb:626
msgid "In order to train the generator and discriminator, we need to define optimization schemes. In the following, we employ a momentum based optimizer called Adam, see `Kingma et al., Adam: A method for stochastic optimization <https://arxiv.org/abs/1412.6980>`__ for more details."
msgstr "生成器と識別器を学習させるためには、最適化スキームを定義する必要があります。以下では、Adamと呼ばれる運動量ベースの最適化手法を採用します。詳しくは `Kingma et al., Adam: A method for stochastic optimization <https://arxiv.org/abs/1412.6980>`__ を参照してください。"

#: ../../tutorials/04_torch_qgan.ipynb:662
msgid "Visualization of the training process"
msgstr "学習過程の可視化"

#: ../../tutorials/04_torch_qgan.ipynb:664
msgid "We will visualize what is happening during the training by plotting the evolution of the generator’s and the discriminator’s loss functions during the training, as well as the progress in the relative entropy between the trained and the target distribution. We define a function that plots the loss functions and relative entropy. We call this function once an epoch of training is complete."
msgstr "学習中の生成器と識別器の損失関数の推移と、学習後の分布と目標分布の相対エントロピーの推移をプロットすることで、学習中に何が起こっているかを可視化します。損失関数と相対エントロピーのプロットを行う関数を定義します。この関数は、1エポックの学習が終了した時点で呼び出されます。"

#: ../../tutorials/04_torch_qgan.ipynb:666
msgid "Visualization of the training process begins when training data is collected across two epochs."
msgstr "学習過程の可視化は、2つのエポックに渡って学習データが収集された時点から開始されます。"

#: ../../tutorials/04_torch_qgan.ipynb:715
msgid "Training"
msgstr "学習"

#: ../../tutorials/04_torch_qgan.ipynb:717
msgid "Now, we are ready to train our model. It may take some time to train the model so be patient."
msgstr "これで、モデルを学習させる準備が整いました。モデルのトレーニングには時間がかかるかもしれませんので、気長にお待ちください。"

#: ../../tutorials/04_torch_qgan.ipynb:804
msgid "Results: cumulative distribution functions"
msgstr "結果：累積分布関数"

#: ../../tutorials/04_torch_qgan.ipynb:806
msgid "In the final section we compare the cumulative distribution function (CDF) of the trained distribution to the CDF of the target distribution."
msgstr "最後のセクションでは、学習した分布の累積分布関数（CDF）と目標分布のCDFを比較します。"

#: ../../tutorials/04_torch_qgan.ipynb:808
msgid "We create a new generator for sampling that takes more shots and, hence, gives more information. Recall, the sampling quantum instance was created with a larger number of shots. Then, we set the weights of the sampling generator to the values obtained in the training process."
msgstr "サンプリングのための新しい生成器を作成し、より多くのショットを取り、それゆえ、より多くの情報を与えます。サンプリング量子インスタンスは、より多くのショット数で作成されたことを思い出してください。そして、サンプリング生成器の重みを学習プロセスで得られた値に設定します。"

#: ../../tutorials/04_torch_qgan.ipynb:830
msgid "Next, we wet the generator data samples, corresponding sampling probabilities, and plot the cumulative distribution functions."
msgstr "次に、生成器データのサンプルと対応するサンプリング確率を取得し、累積分布関数をプロットします。"

#: ../../tutorials/04_torch_qgan.ipynb:898
msgid "On the plot above, in the blue color the generated distribution is shown and in the orange color the training one. You may find that both CDFs are similar to each other."
msgstr "上のプロットでは、青色は生成された分布、オレンジ色は学習された分布を示しています。両方のCDFが互いに似ていることがわかると思います。"

#: ../../tutorials/04_torch_qgan.ipynb:910
msgid "Conclusion"
msgstr "結論"

#: ../../tutorials/04_torch_qgan.ipynb:912
msgid "Quantum generative adversarial networks employ the interplay of a generator and discriminator to map an approximate representation of a probability distribution underlying given data samples into a quantum channel. This tutorial presents a self-standing PyTorch-based qGAN implementation where the generator is given by a quantum channel, i.e., a variational quantum circuit, and the discriminator by a classical neural network, and discusses the application of efficient learning and loading of generic probability distributions – implicitly given by data samples – into quantum states. Since, this approximate loading requires only :math:`\\mathscr{O}\\left(poly\\left(n\\right)\\right)` gates and can enable the use of potentially advantageous quantum algorithms by offering an efficient data loading scheme."
msgstr "量子生成敵対ネットワークは、生成器と識別器の相互作用により、与えられたデータサンプルの下にある確率分布の近似表現を量子チャンネルに写像するものです。このチュートリアルでは、生成器を量子チャネル、すなわち変分量子回路、識別器を古典的なニューラルネットワークとして、PyTorchベースのqGANを実装し、データサンプルによって暗黙的に与えられる一般的な確率分布を効率的に学習して量子状態へロードする応用について議論しました。この近似的なローディングに必要なゲートは :math:`mathscr{O}left(poly\\left(n\\right)\\right)` だけであり、効率的なデータローディング方式を提供することにより、潜在的に有利な量子アルゴリズムを使用できるようになります。"

