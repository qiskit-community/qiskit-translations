msgid ""
msgstr ""
"Project-Id-Version: qiskit-docs\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-11-10 17:18+0000\n"
"PO-Revision-Date: 2023-11-10 18:09\n"
"Last-Translator: \n"
"Language: ja\n"
"Language-Team: Japanese\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.13.1\n"
"X-Crowdin-Project: qiskit-docs\n"
"X-Crowdin-Project-ID: 369271\n"
"X-Crowdin-Language: ja\n"
"X-Crowdin-File: /main/machine-learning/docs/locale/en/LC_MESSAGES/tutorials/04_torch_qgan.po\n"
"X-Crowdin-File-ID: 9885\n"

#: ../../tutorials/04_torch_qgan.ipynb:9
msgid "This page was generated from `docs/tutorials/04_torch_qgan.ipynb`__."
msgstr "このページは `docs/tutorials/04_torch_qgan.ipynb`__ から生成されました。"

#: ../../tutorials/04_torch_qgan.ipynb:9
msgid "PyTorch qGAN Implementation"
msgstr "PyTorchによるqGANの実装"

#: ../../tutorials/04_torch_qgan.ipynb:12
msgid "Overview"
msgstr "概要"

#: ../../tutorials/04_torch_qgan.ipynb:14
msgid "This tutorial introduces step-by-step how to build a PyTorch-based Quantum Generative Adversarial Network algorithm."
msgstr "このチュートリアルでは、PyTorchベースの量子敵対的生成ネットワークのアルゴリズムを構築する方法をステップバイステップで紹介します。"

#: ../../tutorials/04_torch_qgan.ipynb:16
msgid "The tutorial is structured as follows:"
msgstr "このチュートリアルの構造は以下のとおりです。"

#: ../../tutorials/04_torch_qgan.ipynb:18
msgid "`Introduction <#1.-Introduction>`__"
msgstr "`はじめに <#1.-Introduction>`__"

#: ../../tutorials/04_torch_qgan.ipynb:19
msgid "`Data and Represtation <#2.-Data-and-Representation>`__"
msgstr "`データと表現 <#2.-Data-and-Representation>`__"

#: ../../tutorials/04_torch_qgan.ipynb:20
msgid "`Definitions of the Neural Networks <#3.-Definitions-of-the-Neural-Networks>`__"
msgstr "`ニューラルネットワークの定義 <#3.-Definitions-of-Neural-Networks>`__"

#: ../../tutorials/04_torch_qgan.ipynb:21
msgid "`Setting up the Training Loop <#4.-Setting-up-the-Training-Loop>`__"
msgstr ""

#: ../../tutorials/04_torch_qgan.ipynb:22
msgid "`Model Training <#5.-Model-Training>`__"
msgstr "`モデルのトレーニング <#5-Model-Training>`__"

#: ../../tutorials/04_torch_qgan.ipynb:23
msgid "`Results: Cumulative Density Functions <#6.-Results:-Cumulative-Density-Functions>`__"
msgstr "`結果: 累積密度関数 <#6.-Results:-Cumulative-Density-Functions>`__"

#: ../../tutorials/04_torch_qgan.ipynb:24
msgid "`Conclusion <#7.-Conclusion>`__"
msgstr "`結論 <#7.-Conclusion>`__"

#: ../../tutorials/04_torch_qgan.ipynb:27
msgid "1. Introduction"
msgstr "1. はじめに"

#: ../../tutorials/04_torch_qgan.ipynb:29
msgid "The qGAN [1] is a hybrid quantum-classical algorithm used for generative modeling tasks. The algorithm uses the interplay of a quantum generator :math:`G_{\\theta}`, i.e., an ansatz (parametrized quantum circuit), and a classical discriminator :math:`D_{\\phi}`, a neural network, to learn the underlying probability distribution given training data."
msgstr "qGAN [1]は、生成モデリングを行うために使用されるハイブリッド量子古典アルゴリズムです。このアルゴリズムでは、量子生成器 :math:`G_{\\theta}` つまり ansatz (パラメーター化された量子回路) と古典識別器 :math:`D_{\\phi}` (ニューラルネットワーク) の相互作用により、トレーニングデータから基礎となる確率分布の学習を行います。"

#: ../../tutorials/04_torch_qgan.ipynb:31
msgid "The generator and discriminator are trained in alternating optimization steps, where the generator aims at generating probabilities that will be classified by the discriminator as training data values (i.e, probabilities from the real training distribution), and the discriminator tries to differentiate between original distribution and probabilities from the generator (in other words, telling apart the real and generated distributions). The final goal is for the quantum generator to learn a representation for the target probability distribution. The trained quantum generator can, thus, be used to load a quantum state which is an approximate model of the target distribution."
msgstr "生成器と識別器は交互に最適化され、生成器は識別器が学習データ値として分類する確率（実際の学習分布から抽出した確率）の生成を目指し、識別器は元の分布と生成器からの確率を区別する（つまり、実際の分布と生成した分布を区別する）ことを目指します。最終的な目標は、量子生成器が目的とする確率分布の表現を学習することです。この学習により、量子生成器は、ターゲット分布の近似モデルである量子状態をロードするために使用することができます。"

#: ../../tutorials/04_torch_qgan.ipynb:34
msgid "**References:**"
msgstr "**参考文献: **"

#: ../../tutorials/04_torch_qgan.ipynb:36
msgid "[1] Zoufal et al., `Quantum Generative Adversarial Networks for learning and loading random distributions <https://www.nature.com/articles/s41534-019-0223-2>`__"
msgstr "[1] Zoufal et al., `Quantum Generative Adversarial Networks for learning and loading random distributions <https://www.nature.com/articles/s41534-019-0223-2>`__"

#: ../../tutorials/04_torch_qgan.ipynb:39
msgid "1.1. qGANs for Loading Random Distributions"
msgstr "1.1 ランダム分布をロードするためのqGAN"

#: ../../tutorials/04_torch_qgan.ipynb:41
msgid "Given :math:`k`-dimensional data samples, we employ a quantum Generative Adversarial Network (qGAN) to learn a random distribution and to load it directly into a quantum state:"
msgstr ":math:`k` 次元のデータサンプルが与えられた場合、量子敵対的生成ネットワーク (qGAN) を使用してランダム分布を学習し、それを量子状態に直接ロードします。"

#: ../../tutorials/04_torch_qgan.ipynb:43
msgid "\\big| g_{\\theta}\\rangle = \\sum_{j=0}^{2^n-1} \\sqrt{p_{\\theta}^{j}}\\big| j \\rangle\n\n"
msgstr "\\big| g_{\\theta}\\rangle = \\sum_{j=0}^{2^n-1} \\sqrt{p_{\\theta}^{j}}\\big| j \\rangle\n\n"

#: ../../tutorials/04_torch_qgan.ipynb:45
msgid "where :math:`p_{\\theta}^{j}` describe the occurrence probabilities of the basis states :math:`\\big| j\\rangle`."
msgstr "ここで :math:`p_{\\theta}^{j}` は、基底状態 :math:`\\big| j\\rangle` の発生確率を表します。"

#: ../../tutorials/04_torch_qgan.ipynb:47
msgid "The aim of the qGAN training is to generate a state :math:`\\big| g_{\\theta}\\rangle` where :math:`p_{\\theta}^{j}`, for :math:`j\\in \\left\\{0, \\ldots, {2^n-1} \\right\\}`, describe a probability distribution that is close to the distribution underlying the training data :math:`X=\\left\\{x^0, \\ldots, x^{k-1} \\right\\}`."
msgstr "qGAN学習の目的は、状態 :math:`\\big| g_{\\theta}\\rangle` を生成することです。ここで、 :math:`p_{\\theta}^{j}` は、 :math:`j\\in \\left\\{0, \\ldots, {2^n-1} \\right\\}` の場合、 学習データ :math:`X=\\left\\{x^0, \\ldots, x^{k-1} \\right\\}` の基礎となる分布に近い確率分布を表します。"

#: ../../tutorials/04_torch_qgan.ipynb:49
msgid "For further details please refer to `Quantum Generative Adversarial Networks for Learning and Loading Random Distributions <https://arxiv.org/abs/1904.00043>`__ *Zoufal, Lucchi, Woerner* [2019]."
msgstr "詳細については、 `Quantum Generative Adversarial Networks for Learning and Loading Random Distributions <https://arxiv.org/abs/1904.00043>`__ *Zoufal, Lucchi, Woerner* [2019] を参照してください。"

#: ../../tutorials/04_torch_qgan.ipynb:51
msgid "For an example of how to use a trained qGAN in an application, the pricing of financial derivatives, please see the `Option Pricing with qGANs <https://qiskit.org/ecosystem/finance/tutorials/10_qgan_option_pricing.html>`__ tutorial."
msgstr "学習済みの qGAN の実用例として、金融デリバティブの価格設定については、チュートリアルの `qGANs によるオプション価格推定 <https://qiskit.org/ecosystem/finance/tutorials/10_qgan_option_pricing.html>`__ を参照してください。"

#: ../../tutorials/04_torch_qgan.ipynb:54
msgid "2. Data and Representation"
msgstr "データと表現"

#: ../../tutorials/04_torch_qgan.ipynb:56
msgid "First, we need to load our training data :math:`X`."
msgstr "まず、学習データ :math:`X` をロードする必要があります。"

#: ../../tutorials/04_torch_qgan.ipynb:58
msgid "In this tutorial, the training data is given by a 2D multivariate normal distribution."
msgstr "このチュートリアルでは、訓練データは2次元多変量正規分布によって与えられます。"

#: ../../tutorials/04_torch_qgan.ipynb:60
msgid "The goal of the generator is to learn how to represent such distribution, and the trained generator should correspond to an :math:`n`-qubit quantum state :nbsphinx-math:`\\begin{equation} |g_{\\text{trained}}\\rangle=\\sum\\limits_{j=0}^{k-1}\\sqrt{p_{j}}|x_{j}\\rangle, \\end{equation}` where the basis states :math:`|x_{j}\\rangle` represent the data items in the training data set :math:`X={x_0, \\ldots, x_{k-1}}` with :math:`k\\leq 2^n` and :math:`p_j` refers to the sampling probability of :math:`|x_{j}\\rangle`."
msgstr "生成器の目標は、そのような分布を表す方法を学習することであり、トレーニングされた生成器は :math:`n`-qubit 量子状態に対応する必要があります :nbsphinx-math:`\\begin{equation} |g_{\\text{trained }}\\rangle=\\sum\\limits_{j=0}^{k-1}\\sqrt{p_{j}}|x_{j}\\rangle, \\end{equation}` ここで、基底状態 :math:`|x_{j}\\rangle` はトレーニング データ セット内のデータ項目を表します :math:`X={x_0, \\ldots, x_{k-1}}` with :math:` k\\leq 2^n` と :math:`p_j` は :math:`|x_{j}\\rangle` のサンプリング確率を指します。"

#: ../../tutorials/04_torch_qgan.ipynb:64
msgid "To facilitate this representation, we need to map the samples from the multivariate normal distribution to discrete values. The number of values that can be represented depends on the number of qubits used for the mapping. Hence, the data resolution is defined by the number of qubits. If we use :math:`3` qubits to represent one feature, we have :math:`2^3 = 8` discrete values."
msgstr "この表現を容易にするために、多変量正規分布からのサンプルを離散的な値にマッピングする必要があります。表現できる値の数はマッピングに使用する量子ビットの数に依存します。したがって、データの分解能は量子ビットの数によって定義されます。math:`3` 個の量子ビットを使って 1 つの特徴を表現すると、 :math:`2^3 = 8` 個の離散値を持つことになります。"

#: ../../tutorials/04_torch_qgan.ipynb:66
msgid "We first begin by fixing seeds in the random number generators for reproducibility of the outcome in this tutorial."
msgstr "まず、このチュートリアルでは、結果の再現性を得るために乱数ジェネレータのシードを固定します。"

#: ../../tutorials/04_torch_qgan.ipynb:91
msgid "We fix the number of dimensions, the discretization number and compute the number of qubits required as :math:`2^3 = 8`."
msgstr "次元の数、離散数を固定し、 :math:`2^3 = 8` として必要な量子ビット数を計算します。"

#: ../../tutorials/04_torch_qgan.ipynb:116
msgid "Then, we prepare a discrete distribution from the continuous 2D normal distribution. We evaluate the continuous probability density function (PDF) on the grid :math:`(-2, 2)^2` with a discretization of :math:`8` values per feature. Thus, we have :math:`64` values of the PDF. Since this will be a discrete distribution we normalize the obtained probabilities."
msgstr "次に、連続2次元正規分布から離散分布を準備します。特徴ごとに :math:`8` 個の値で離散化された、 :math:`(-2, 2)^2` 格子上の連続確率密度関数 (probability density function, PDF) を評価します。したがって、PDFの値は :math:`64` となります。これは離散分布になるので、得られた確率を正規化します。"

#: ../../tutorials/04_torch_qgan.ipynb:143
msgid "Let's visualize our distribution. It is a nice bell-shaped bivariate normal distribution on a discrete grid."
msgstr "我々の分布を可視化しましょう。離散格子上の素敵なベル型の二変量正規分布です。"

#: ../../tutorials/04_torch_qgan.ipynb:183
msgid "3. Definitions of the Neural Networks"
msgstr "3. ニューラルネットワークの定義"

#: ../../tutorials/04_torch_qgan.ipynb:185
msgid "In this section we define two neural networks as described above:"
msgstr "このセクションでは、上記の2つのニューラルネットワークを定義します。"

#: ../../tutorials/04_torch_qgan.ipynb:187
msgid "A quantum generator as a quantum neural network."
msgstr "量子ニューラルネットワークとしての量子生成器。"

#: ../../tutorials/04_torch_qgan.ipynb:188
msgid "A classical discriminator as a PyTorch-based neural network."
msgstr "PyTorchベースのニューラルネットワークとしての古典的な識別器。"

#: ../../tutorials/04_torch_qgan.ipynb:191
msgid "3.1. Definition of the quantum neural network ansatz"
msgstr "3.1. 量子ニューラルネットワークansatzの定義"

#: ../../tutorials/04_torch_qgan.ipynb:193
msgid "Now, we define the parameterized quantum circuit :math:`G\\left(\\boldsymbol{\\theta}\\right)` with :math:`\\boldsymbol{\\theta} = {\\theta_1, ..., \\theta_k}` which will be used in our quantum generator."
msgstr "ここで、量子生成器に用いるパラメータ化量子回路 :math:`G\\left(\\boldsymbol{\\theta}\\right)` を :math:`\\boldsymbol{\\theta} = {\\theta_1, ..., \\theta_k}` で定義します。 これは、量子生成器で使用されます。"

#: ../../tutorials/04_torch_qgan.ipynb:195
msgid "To implement the quantum generator, we choose a hardware efficient ansatz with :math:`6` repetitions. The ansatz implements :math:`R_Y`, :math:`R_Z` rotations and :math:`CX` gates which takes a uniform distribution as an input state. Notably, for :math:`k>1` the generator's parameters must be chosen carefully. For example, the circuit depth should be more than :math:`1` because higher circuit depths enable the representation of more complex structures. Here, we construct quite a deep circuit with a large number of parameters to be able to adequately capture and represent the distribution."
msgstr "量子生成器を実装するために、ハードウェア的に効率的な、:math:`6` 回繰り返しのansatzを選択します。このansatzは :math:`R_Y`, :math:`R_Z` 回転と :math:`CX` ゲートを実装し、入力状態として一様分布を受け取ります。特に :math:`k>1` の場合、生成器のパラメーターは注意深く選択する必要があります。例えば、回路の深さは :math:`1` 以上であるべきです。これは、回路の深さが大きいほど、より複雑な構造を表現できるからです。ここでは、分布を適切にとらえて表現できるように、多くのパラメーターを持つかなり深い回路を構築します。"

#: ../../tutorials/04_torch_qgan.ipynb:224
msgid "Let's draw our circuit and see what it looks like. On the plot we may notice a pattern that appears :math:`6` times."
msgstr "回路を描いて、どのように見えるか確認してみましょう。プロット上に :math:`6` 回現れるパターンに気づくかもしれません。"

#: ../../tutorials/04_torch_qgan.ipynb:283
msgid "Let's print the number of trainable parameters."
msgstr "トレーニング可能なパラメーターの数を表示しましょう。"

#: ../../tutorials/04_torch_qgan.ipynb:330
msgid "3.2. Definition of the quantum generator"
msgstr "3.2. 量子生成器の定義"

#: ../../tutorials/04_torch_qgan.ipynb:332
msgid "We start defining the generator by creating a sampler for the ansatz. The reference implementation is a statevector-based implementation, thus it returns exact probabilities as a result of circuit execution. We add the ``shots`` parameter to add some noise to the results. In this case the implementation samples probabilities from the multinomial distribution constructed from the measured quasi probabilities. And as usual we fix the seed for reproducibility purposes."
msgstr "Ansatz 用のサンプラーを作成することで、生成器の定義を開始します。 リファレンス実装は状態ベクトルベースの実装であり、回路実行の結果として正確な確率を返します。 結果にノイズを追加するために ``shots`` パラメーターを追加します。 この場合、実装は、測定された準確率から構築された多変量正規分布からの確率をサンプルします。 そして、いつものように再現性の目的のためにシードを固定します。"

#: ../../tutorials/04_torch_qgan.ipynb:356
msgid "Next, we define a function that creates the quantum generator from a given parameterized quantum circuit. Inside this function we create a neural network that returns the quasi probability distribution evaluated by the underlying Sampler. We fix ``initial_weights`` for reproducibility purposes. In the end we wrap the created quantum neural network in ``TorchConnector`` to make use of PyTorch-based training."
msgstr "次に、パラメーター化された量子回路から量子生成器を作成する関数を定義します。この関数の中には、基礎となるサンプラーによって評価された準確率分布を返すニューラルネットワークを作成します。再現性のために ``initial_weights`` を固定します。作成された量子ニューラルネットワークを ``TorchConnector`` でラップし、PyTorch ベースのトレーニングを利用できるようにします。"

#: ../../tutorials/04_torch_qgan.ipynb:392
msgid "3.3. Definition of the classical discriminator"
msgstr "3.3. 古典識別子の定義"

#: ../../tutorials/04_torch_qgan.ipynb:394
msgid "Next, we define a PyTorch-based classical neural network that represents the classical discriminator. The underlying gradients can be automatically computed with PyTorch."
msgstr "次に、古典的な識別器を表すPyTorchベースの古典的ニューラルネットワークを定義します。基礎となる勾配はPyTorchで自動的に計算することができます。"

#: ../../tutorials/04_torch_qgan.ipynb:433
msgid "3.4. Create a generator and a discriminator"
msgstr "3.4. 生成器と識別器を作成する"

#: ../../tutorials/04_torch_qgan.ipynb:435
msgid "Now we create a generator and a discriminator."
msgstr "次に、私たちは、生成器と識別器を作成します。"

#: ../../tutorials/04_torch_qgan.ipynb:458
msgid "4. Setting up the Training Loop"
msgstr "4. トレーニング・ループのセットアップ"

#: ../../tutorials/04_torch_qgan.ipynb:460
msgid "In this section we set up:"
msgstr "このセクションでは以下をセットアップします。"

#: ../../tutorials/04_torch_qgan.ipynb:462
msgid "A loss function for the generator and discriminator."
msgstr "生成器と識別器の損失関数。"

#: ../../tutorials/04_torch_qgan.ipynb:463
msgid "Optimizers for both."
msgstr "両方に対するオプティマイザー。"

#: ../../tutorials/04_torch_qgan.ipynb:464
msgid "A utility plotting function to visualize training process."
msgstr "トレーニングプロセスを視覚化するユーティリティプロット関数。"

#: ../../tutorials/04_torch_qgan.ipynb:467
msgid "4.1. Definition of the loss functions"
msgstr "4.1. 損失関数の定義"

#: ../../tutorials/04_torch_qgan.ipynb:469
msgid "We want to train the generator and the discriminator with binary cross entropy as the loss function:"
msgstr "損失関数として2値クロスエントロピーを用い、生成器と識別器を学習させたいと思います。"

#: ../../tutorials/04_torch_qgan.ipynb:471
msgid "L\\left(\\boldsymbol{\\theta}\\right)=\\sum_jp_j\\left(\\boldsymbol{\\theta}\\right)\\left[y_j\\log(x_j) + (1-y_j)\\log(1-x_j)\\right],\n\n"
msgstr "L\\left(\\boldsymbol{\\theta}\\right)=\\sum_jp_j\\left(\\boldsymbol{\\theta}\\right)\\left[y_j\\log(x_j) + (1-y_j)\\log(1-x_j)\\right],\n\n"

#: ../../tutorials/04_torch_qgan.ipynb:473
msgid "where :math:`x_j` refers to a data sample and :math:`y_j` to the corresponding label."
msgstr "ここで、 :math:`x_j` はデータサンプル、 :math:`y_j` はそれに対応するラベルを表します。"

#: ../../tutorials/04_torch_qgan.ipynb:475
msgid "Since PyTorch's ``binary_cross_entropy`` is not differentiable with respect to weights, we implement the loss function manually to be able to evaluate gradients."
msgstr "PyTorchの ``binary_cross_entropy`` は重みに関して微分できないため、勾配を評価できるように手動で損失関数を実装しています。"

#: ../../tutorials/04_torch_qgan.ipynb:501
msgid "4.2. Definition of the optimizers"
msgstr "4.2. オプティマイザーの定義"

#: ../../tutorials/04_torch_qgan.ipynb:503
msgid "In order to train the generator and discriminator, we need to define optimization schemes. In the following, we employ a momentum based optimizer called Adam, see `Kingma et al., Adam: A method for stochastic optimization <https://arxiv.org/abs/1412.6980>`__ for more details."
msgstr "生成器と識別器を学習させるためには、最適化スキームを定義する必要があります。以下では、Adamと呼ばれる運動量ベースの最適化手法を採用します。詳しくは `Kingma et al., Adam: A method for stochastic optimization <https://arxiv.org/abs/1412.6980>`__ を参照してください。"

#: ../../tutorials/04_torch_qgan.ipynb:534
msgid "4.3. Visualization of the training process"
msgstr "4.3. トレーニングプロセスの可視化"

#: ../../tutorials/04_torch_qgan.ipynb:536
msgid "We will visualize what is happening during the training by plotting the evolution of the generator's and the discriminator's loss functions during the training, as well as the progress in the relative entropy between the trained and the target distribution. We define a function that plots the loss functions and relative entropy. We call this function once an epoch of training is complete."
msgstr "トレーニング中の生成器と識別器の損失関数の推移と、トレーニング後の分布とターゲット分布の相対エントロピーの推移をプロットすることで、トレーニング中に何が起こっているかを可視化します。損失関数と相対エントロピーのプロットを行う関数を定義します。この関数は、1エポックのトレーニングが終了した時点で呼び出されます。"

#: ../../tutorials/04_torch_qgan.ipynb:538
msgid "Visualization of the training process begins when training data is collected across two epochs."
msgstr "学習過程の可視化は、2つのエポックに渡って学習データが収集された時点から開始されます。"

#: ../../tutorials/04_torch_qgan.ipynb:587
msgid "5. Model Training"
msgstr "5. モデルのトレーニング"

#: ../../tutorials/04_torch_qgan.ipynb:589
msgid "In the training loop we monitor not only loss functions, but relative entropy as well. The relative entropy describes a distance metric for distributions. Hence, we can use it to benchmark how close/far away the trained distribution is from the target distribution."
msgstr "トレーニング・ループでは、損失関数だけでなく、相対エントロピーも監視します。相対エントロピーは分布の距離指標を表します。 したがって、これを使用して、トレーニングされた分布がターゲット分布からどれだけ近い/遠くにあるかをベンチマークすることができます。"

#: ../../tutorials/04_torch_qgan.ipynb:591
msgid "Now, we are ready to train our model. It may take some time to train the model so be patient."
msgstr "これで、モデルを学習させる準備が整いました。モデルのトレーニングには時間がかかるかもしれませんので、気長にお待ちください。"

#: ../../tutorials/04_torch_qgan.ipynb:702
msgid "6. Results: Cumulative Density Functions"
msgstr "6. 結果: 累積密度関数"

#: ../../tutorials/04_torch_qgan.ipynb:704
msgid "In this section we compare the cumulative distribution function (CDF) of the trained distribution to the CDF of the target distribution."
msgstr "このセクションでは、トレーニングした分布の累積分布関数 (cumulative distribution function, CDF) とターゲット分布の CDF を比較します。"

#: ../../tutorials/04_torch_qgan.ipynb:706
msgid "First, we generate a new probability distribution with PyTorch autograd turned off as we are not going to train the model anymore."
msgstr "まず、モデルをもうトレーニングしないので、PyTorch の autograd をオフにして新しい確率分布を生成します。"

#: ../../tutorials/04_torch_qgan.ipynb:728
msgid "And then, we plot the cumulative distribution functions of the generated distribution, original distribution, and the difference between them. Please, be careful, the scale on the third plot **is not the same** as on the first and second plot, and the actual difference between the two plotted CDFs is pretty small."
msgstr "そして、生成された分布と元の分布の累積分布関数、そしてそれらの差をプロットします。注意してください。3 番目のプロットのスケールは 1 番目と 2 番目のプロットと **同じではありません** 。"

#: ../../tutorials/04_torch_qgan.ipynb:783
msgid "7. Conclusion"
msgstr "7. 結論"

#: ../../tutorials/04_torch_qgan.ipynb:785
msgid "Quantum generative adversarial networks employ the interplay of a generator and discriminator to map an approximate representation of a probability distribution underlying given data samples into a quantum channel. This tutorial presents a self-standing PyTorch-based qGAN implementation where the generator is given by a quantum channel, i.e., a variational quantum circuit, and the discriminator by a classical neural network, and discusses the application of efficient learning and loading of generic probability distributions into quantum states. The loading requires :math:`\\mathscr{O}\\left(poly\\left(n\\right)\\right)` gates and can thus enable the use of potentially advantageous quantum algorithms."
msgstr "量子敵対的生成ネットワークは、生成器と識別器の相互作用により、与えられたデータサンプルの下にある確率分布の近似表現を量子チャンネルにマッピングするものです。このチュートリアルでは、生成器を量子チャネル、すなわち変分量子回路、識別器を古典的なニューラルネットワークとして、PyTorch ベースの qGAN の実装を紹介し、一般的な確率分布の効率的な学習と量子状態へロードする応用について議論しました。この近似的なローディングには :math:`\\mathscr{O}\\left(poly\\left(n\\right)\\right)` 個のゲートが必要であり、潜在的に有利な量子アルゴリズムを利用できます。"

