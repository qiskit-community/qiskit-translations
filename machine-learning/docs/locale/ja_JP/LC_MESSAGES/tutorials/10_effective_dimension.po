msgid ""
msgstr ""
"Project-Id-Version: qiskit-docs\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-02-07 03:44+0000\n"
"PO-Revision-Date: 2023-02-07 05:15\n"
"Last-Translator: \n"
"Language: ja\n"
"Language-Team: Japanese\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.11.0\n"
"X-Crowdin-Project: qiskit-docs\n"
"X-Crowdin-Project-ID: 369271\n"
"X-Crowdin-Language: ja\n"
"X-Crowdin-File: /master/machine-learning/docs/locale/en/LC_MESSAGES/tutorials/10_effective_dimension.po\n"
"X-Crowdin-File-ID: 9730\n"

#: ../../tutorials/10_effective_dimension.ipynb:9
msgid "This page was generated from `docs/tutorials/10_effective_dimension.ipynb`__."
msgstr "このページは `docs/tutorials/10_effective_dimension.ipynb`__ から生成されました。"

#: ../../tutorials/10_effective_dimension.ipynb:9
msgid "Effective Dimension of Qiskit Neural Networks"
msgstr "Qiskitニューラルネットワークの有効な次元"

#: ../../tutorials/10_effective_dimension.ipynb:11
msgid "In this tutorial, we will take advantage of the ``EffectiveDimension`` and ``LocalEffectiveDimension`` classes to evaluate the power of Quantum Neural Network models. These are metrics based on information geometry that connect to notions such as trainability, expressibility or ability to generalize."
msgstr "このチュートリアルでは、 ``EffectiveDimension`` クラスと ``LocalEffectiveDimension`` クラスを利用して、量子ニューラルネットワークモデルの能力を評価します。これらは、トレーニング可能性、表現可能性、一般化能力などの概念に関連する情報幾何学に基づくメトリックです。"

#: ../../tutorials/10_effective_dimension.ipynb:13
msgid "Before diving into the code example, we will briefly explain what is the difference between these two metrics, and why are they relevant to the study of Quantum Neural Networks. More information about global effective dimension can be found in `this paper <https://arxiv.org/pdf/2011.00027.pdf>`__, while the local effective dimension was introduced in a `later work <https://arxiv.org/abs/2112.04807>`__."
msgstr "コード例に飛び込む前に、これら2つのメトリックの違いと、それらが量子ニューラルネットワークの研究に関連する理由を簡単に説明します。グローバル有効次元の詳細については、 `この論文 <https://arxiv.org/pdf/2011.00027.pdf>`__ を参照してください。ローカル有効ディメンションは、 `後の研究 <https://arxiv.org/abs/2112.04807>`__ で紹介されています。"

#: ../../tutorials/10_effective_dimension.ipynb:25
msgid "1. Global vs. Local Effective Dimension"
msgstr "1. グローバルとローカルの有効次元"

#: ../../tutorials/10_effective_dimension.ipynb:27
msgid "Both classical and quantum machine learning models share a common goal: being good at **generalizing**, i.e. learning insights from data and applying them on unseen data."
msgstr "古典的な機械学習モデルと量子機械学習モデルはどちらも共通の目標を共有しています。それは **一般化** に長けていること、つまりデータから洞察を学び、それらを目に見えないデータに適用することです。"

#: ../../tutorials/10_effective_dimension.ipynb:29
msgid "Finding a good metric to assess this ability is a non-trivial matter. In `The Power of Quantum Neural Networks <https://arxiv.org/pdf/2011.00027.pdf>`__, the authors introduce the **global** effective dimension as a useful indicator of how well a particular model will be able to perform on new data. In `Effective Dimension of Machine Learning Models <https://arxiv.org/pdf/2112.04807.pdf>`__, the **local** effective dimension is proposed as a new capacity measure that bounds the generalization error of machine learning models."
msgstr "この能力を評価するための適切な指標を見つけることは重要なことです。 `The Power of Quantum Neural Networks <https://arxiv.org/pdf/2011.00027.pdf>`__ で、著者は、特定のモデルが新しいデータに対してどれだけうまく機能できるかを示す有用な指標として、**グローバル** な有効次元を紹介しています。 `Effective Dimension of Machine Learning Models <https://arxiv.org/pdf/2112.04807.pdf>`__ では、機械学習モデルの汎化誤差を制限する新しい容量測定値として、**ローカル** 有効次元が提案されています。"

#: ../../tutorials/10_effective_dimension.ipynb:32
msgid "The key difference between global (``EffectiveDimension`` class) and **local** effective dimension (``LocalEffectiveDimension`` class) is actually not in the way they are computed, but in the nature of the parameter space that is analyzed. The global effective dimension incorporates the **full parameter space** of the model, and is calculated from a **large number of parameter (weight) sets**. On the other hand, the local effective dimension focuses on how well the **trained** model can generalize to new data, and how **expressive** it can be. Therefore, the local effective dimension is calculated from **a single** set of weight samples (training result). This difference is small in terms of practical implementation, but quite relevant at a conceptual level."
msgstr "グローバル（ ``EffectiveDimension`` クラス）と **ローカル** 有効ディメンション（ ``LocalEffectiveDimension`` クラス）の主な違いは、実際には計算方法ではなく、分析されるパラメーター空間の性質にあります。グローバル有効次元は、モデルの **完全なパラメーター空間** を組み込み、 **多数のパラメーター（重み）セット** から計算されます。一方、ローカルの有効な次元は、**トレーニングされた** モデルが新しいデータにどれだけうまく一般化できるか、そしてそれがどれほど **表現力** があるかに焦点を当てています。したがって、局所的な有効寸法は、**単一の** 重みサンプルのセットから計算されます（トレーニング結果）。この違いは、実際の実装に関してはわずかですが、概念レベルでは非常に重要です。"

#: ../../tutorials/10_effective_dimension.ipynb:45
msgid "2. The Effective Dimension Algorithm"
msgstr "2. 有効次元アルゴリズム"

#: ../../tutorials/10_effective_dimension.ipynb:47
msgid "Both the global and local effective dimension algorithms use the Fisher Information matrix to provide a measure of complexity. The details on how this matrix is calculated are provided in the `reference paper <https://arxiv.org/pdf/2011.00027.pdf>`__, but in general terms, this matrix captures how sensitive a neural network’s output is to changes in the network’s parameter space."
msgstr "グローバルおよびローカルの有効次元アルゴリズムはどちらも、フィッシャー情報量マトリックスを使用して複雑さの尺度を提供します。この行列の計算方法の詳細は `参考論文 <https://arxiv.org/pdf/2011.00027.pdf>`__ に記載されていますが、一般的にこの行列は、ニューラルネットワークの出力がネットワークのパラメータ空間の変化に対してどれほど敏感であるかを示しています。"

#: ../../tutorials/10_effective_dimension.ipynb:49
msgid "In particular, this algorithm follows 4 main steps:"
msgstr "特に、このアルゴリズムは4つの主要なステップに従います。"

#: ../../tutorials/10_effective_dimension.ipynb:51
msgid "**Monte Carlo simulation:** the forward and backward passes (gradients) of the neural network are computed for each pair of input and weight samples."
msgstr "**モンテカルロシミュレーション** ：ニューラルネットワークの順方向パスと逆方向パス（勾配）は、入力サンプルと重みサンプルの各ペアに対して計算されます。"

#: ../../tutorials/10_effective_dimension.ipynb:52
msgid "**Fisher Matrix Computation:** these outputs and gradients are used to compute the Fisher Information Matrix."
msgstr "**フィッシャー行列の計算** ：これらの出力と勾配は、フィッシャー情報量の計算に使用されます。"

#: ../../tutorials/10_effective_dimension.ipynb:53
msgid "**Fisher Matrix Normalization:** averaging over all input samples and dividing by the matrix trace"
msgstr "**フィッシャー行列の正規化** ：すべての入力サンプルの平均と行列トレースによる除算"

#: ../../tutorials/10_effective_dimension.ipynb:54
msgid "**Effective Dimension Calculation:** according to the formula from `Abbas et al. <https://arxiv.org/pdf/2011.00027.pdf>`__"
msgstr "**有効寸法の計算** ： `Abbas et al. <https://arxiv.org/pdf/2011.00027.pdf>`__ の式による。"

#: ../../tutorials/10_effective_dimension.ipynb:66
msgid "3. Basic Example (SamplerQNN)"
msgstr "3. 基本的な例（SamplerQNN）"

#: ../../tutorials/10_effective_dimension.ipynb:68
msgid "This example shows how to set up a QNN model problem and run the global effective dimension algorithm. Both Qiskit ``SamplerQNN`` (shown in this example) and ``EstimatorQNN`` (shown in a later example) can be used with the ``EffectiveDimension`` class."
msgstr "この例は、QNNモデルの問題を設定し、グローバル有効次元アルゴリズムを実行する方法を示しています。Qiskit ``SamplerQNN``（この例に示されている）と ``EstimatorQNN`` （後の例に示されている）の両方を ``EffectiveDimension`` クラスで使用できます。"

#: ../../tutorials/10_effective_dimension.ipynb:70
msgid "We start off from the required imports and a fixed seed for the random number generator for reproducibility purposes."
msgstr "再現性を目的として、必要なインポートと乱数ジェネレーターの固定シードから開始します。"

#: ../../tutorials/10_effective_dimension.ipynb:108
msgid "3.1 Define QNN"
msgstr "3.1 QNNを定義する"

#: ../../tutorials/10_effective_dimension.ipynb:110
msgid "The first step to create a ``SamplerQNN`` is to define a parametrized feature map and ansatz. In this toy example, we will use 3 qubits, and we will define the circuit used in the ``SamplerQNN`` class."
msgstr "``SamplerQNN`` を作成するための最初のステップは、パラメーター化された特徴量マップとAnsatz を定義することです。このトイ・サンプルでは、3量子ビットを使用し、 ``SamplerQNN`` クラスで使用される回路を定義します。"

#: ../../tutorials/10_effective_dimension.ipynb:150
msgid "The parametrized circuit can then be sent together with an optional interpret map (parity in this case) to the ``SamplerQNN`` constructor."
msgstr "次に、パラメーター化された回路をオプションの解釈マップ（この場合はパリティ）と一緒に ``SamplerQNN`` コンストラクタに送信できます。"

#: ../../tutorials/10_effective_dimension.ipynb:195
msgid "3.2 Set up Effective Dimension calculation"
msgstr "3.2 実効次元計算の設定"

#: ../../tutorials/10_effective_dimension.ipynb:197
msgid "In order to compute the effective dimension of our QNN using the ``EffectiveDimension`` class, we need a series of sets of input samples and weights, as well as the total number of data samples available in a dataset. The ``input_samples`` and ``weight_samples`` are set in the class constructor, while the number of data samples is given during the call to the effective dimension computation, to be able to test and compare how this measure changes with different dataset sizes."
msgstr "``EffectiveDimension`` クラスを使用してQNNの有効次元を計算するには、一連の入力サンプルと重みのセット、およびデータセットで使用可能なデータサンプルの総数が必要です。 ``input_samples`` と ``weight_samples`` はクラスコンストラクターで設定され、データサンプルの数は有効なディメンション計算の呼び出し中に指定されるため、このメジャーがさまざまなデータセットサイズでどのように変化するかをテストおよび比較できます。"

#: ../../tutorials/10_effective_dimension.ipynb:208
msgid "We can define the number of input samples and weight samples and the class will randomly sample a corresponding array from a normal (for ``input_samples``) or a uniform (for ``weight_samples``) distribution. Instead of passing a number of samples we can pass an array, sampled manually."
msgstr "入力サンプルと重みサンプルの数を定義できます。クラスは、正規（ ``input_samples`` の場合）または均一（ ``weight_samples`` の場合）の分布から対応する配列をランダムにサンプリングします。 多数のサンプルを渡す代わりに、手動でサンプリングされた配列を渡すことができます。"

#: ../../tutorials/10_effective_dimension.ipynb:235
msgid "If we want to test a specific set of input samples and weight samples, we can provide it directly to the ``EffectiveDimension`` class as shown in the following snippet:"
msgstr "入力サンプルと重みサンプルの特定のセットをテストする場合は、次のスニペットに示すように、それを ``EffectiveDimension`` クラスに直接提供できます。"

#: ../../tutorials/10_effective_dimension.ipynb:260
msgid "The effective dimension algorithm also requires a dataset size. In this example, we will define an array of sizes to later see how this input affects the result."
msgstr "有効な次元アルゴリズムには、データセットサイズも必要です。この例では、サイズの配列を定義して、後でこの入力が結果にどのように影響するかを確認します。"

#: ../../tutorials/10_effective_dimension.ipynb:283
msgid "3.3 Compute Global Effective Dimension"
msgstr "3.3 グローバル実効次元の計算"

#: ../../tutorials/10_effective_dimension.ipynb:285
msgid "Let’s now calculate the effective dimension of our network for the previously defined set of input samples, weights, and a dataset size of 5000."
msgstr "次に、以前に定義した一連の入力サンプル、重み、および5000のデータセットサイズについて、ネットワークの有効な次元を計算しましょう。"

#: ../../tutorials/10_effective_dimension.ipynb:306
msgid "The effective dimension values will range between 0 and ``d``, where ``d`` represents the dimension of the model, and it’s practically obtained from the number of weights of the QNN. By dividing the result by ``d``, we can obtain the normalized effective dimension, which correlates directly with the capacity of the model."
msgstr "有効な次元値の範囲は0〜 ``d`` です。ここで、 ``d`` はモデルの次元を表し、実際にはQNNの重みの数から取得されます。結果を ``d`` で割ることにより、モデルの容量と直接相関する正規化された実効次元を取得できます。"

#: ../../tutorials/10_effective_dimension.ipynb:359
msgid "By calling the ``EffectiveDimension`` class with an array if input sizes ``n``, we can monitor how the effective dimension changes with the dataset size."
msgstr "入力サイズが ``n`` の場合に配列を使用して ``EffectiveDimension`` クラスを呼び出すことにより、データセットサイズによって有効な次元がどのように変化するかを監視できます。"

#: ../../tutorials/10_effective_dimension.ipynb:444
msgid "4. Local Effective Dimension Example"
msgstr "4. ローカル有効次元の例"

#: ../../tutorials/10_effective_dimension.ipynb:446
msgid "As explained in the introduction, the local effective dimension algorithm only uses **one** set of weights, and it can be used to monitor how training affects the expressiveness of a neural network. The ``LocalEffectiveDimension`` class enforces this constraint to ensure that these calculations are conceptually separate, but the rest of the implementation is shared with ``EffectiveDimension``."
msgstr "はじめに説明したように、ローカル有効次元アルゴリズムは **1** セットの重みのみを使用し、トレーニングがニューラルネットワークの表現力にどのように影響するかを監視するために使用できます。 ``LocalEffectiveDimension`` クラスは、これらの計算が概念的に分離されていることを確認するためにこの制約を適用しますが、実装の残りの部分は ``EffectiveDimension`` と共有されます。"

#: ../../tutorials/10_effective_dimension.ipynb:448
msgid "This example shows how to leverage the ``LocalEffectiveDimension`` class to analyze the effect of training on QNN expressiveness."
msgstr "この例は、 ``LocalEffectiveDimension`` クラスを活用して、QNNの表現力に対するトレーニングの効果を分析する方法を示しています。"

#: ../../tutorials/10_effective_dimension.ipynb:460
msgid "4.1 Define Dataset and QNN"
msgstr "4.1 データセットとQNNの定義"

#: ../../tutorials/10_effective_dimension.ipynb:462
msgid "We start by creating a 3D binary classification dataset using ``make_classification`` function from scikit-learn."
msgstr "まず、scikit-learnの ``make_classification`` 関数を用いて、3Dバイナリー分類データセットを作成することから始めます。"

#: ../../tutorials/10_effective_dimension.ipynb:495
msgid "The next step is to create a QNN, an instance of ``EstimatorQNN`` in our case in the same fashion we created an instance of ``SamplerQNN``."
msgstr "次のステップはQNNを作成することで、この場合は ``EstimatorQNN`` のインスタンスを ``SamplerQNN`` のインスタンスと同じ方法で作成します。"

#: ../../tutorials/10_effective_dimension.ipynb:519
msgid "4.2 Train QNN"
msgstr "4.2 QNNのトレーニング"

#: ../../tutorials/10_effective_dimension.ipynb:521
msgid "We can now proceed to train the QNN. The training step may take some time, be patient. You can pass a callback to the classifier to observe how the training process is going on. We fix ``initial_point`` for reproducibility purposes as usual."
msgstr "これで、QNNのトレーニングに進むことができます。トレーニングステップには時間がかかる場合があります。しばらくお待ちください。 分類器にコールバックを渡して、トレーニングプロセスがどのように進行しているかを観察できます。 通常どおり、再現性のために ``initial_point`` を修正します。"

#: ../../tutorials/10_effective_dimension.ipynb:594
msgid "The classifier can now differentiate between classes with an accuracy of:"
msgstr "分類器は、次の精度でクラスを区別できるようになりました。"

#: ../../tutorials/10_effective_dimension.ipynb:642
msgid "4.3 Compute Local Effective Dimension of trained QNN"
msgstr "4.3 トレーニングされたQNNのローカル有効次元を計算する"

#: ../../tutorials/10_effective_dimension.ipynb:644
msgid "Now that we have trained our network, let’s evaluate the local effective dimension based on the trained weights. To do that we access the trained weights directly from the classifier."
msgstr "ネットワークをトレーニングしたので、トレーニングした重みに基づいてローカルの有効次元を評価しましょう。そのために、分類器から直接トレーニング済みの重みにアクセスします。"

#: ../../tutorials/10_effective_dimension.ipynb:705
msgid "4.4 Compute Local Effective Dimension of untrained QNN"
msgstr "4.4 トレーニングされていないQNNのローカル有効次元を計算する"

#: ../../tutorials/10_effective_dimension.ipynb:707
msgid "We can compare this result with the effective dimension of the untrained network, using the ``initial_point`` as our weight sample:"
msgstr "重みのサンプルとして ``initial_point`` を使用して、この結果をトレーニングされていないネットワークの有効な次元と比較できます。"

#: ../../tutorials/10_effective_dimension.ipynb:766
msgid "4.5 Plot and analyze results"
msgstr "4.5 結果のプロットと分析"

#: ../../tutorials/10_effective_dimension.ipynb:768
msgid "If we plot the effective dimension values before and after training, we can see the following result:"
msgstr "トレーニングの前後の実効次元値をプロットすると、次の結果が得られます。"

#: ../../tutorials/10_effective_dimension.ipynb:806
msgid "In general, we should expect the value of the local effective dimension to decrease after training. This can be understood by looking back into the main goal of machine learning, which is to pick a model that is expressive enough to fit your data, but not too expressive that it overfits and performs badly on new data samples."
msgstr "一般に、トレーニング後にローカル有効次元の値が減少することを期待する必要があります。これは、機械学習の主な目標を振り返ることで理解できます。これは、データに適合するのに十分な表現力があるが、新しいデータサンプルに適合しすぎてパフォーマンスが悪いほど表現力のないモデルを選択することです。"

#: ../../tutorials/10_effective_dimension.ipynb:808
msgid "Certain optimizers help regularize the overfitting of a model by learning parameters, and this action of learning inherently reduces a model’s expressiveness, as measured by the local effective dimension. Following this logic, a randomly initialized parameter set will most likely produce a higher effective dimension that the final set of trained weights, because that model with that particular parameterization is “using more parameters” unnecessarily to fit the data. After training (with the implicit regularization), a trained model will not need to use so many parameters and thus have more “inactive parameters” and a lower effective dimension."
msgstr "特定のオプティマイザは、パラメータを学習することでモデルの過剰適合を正規化するのに役立ちます。この学習アクションは、ローカルの有効次元で測定されるように、モデルの表現力を本質的に低下させます。このロジックに従うと、ランダムに初期化されたパラメーターセットは、トレーニングされた重みの最終セットよりも有効な次元を生成する可能性が高くなります。これは、その特定のパラメーター化を使用したモデルが、データを適合させるために不必要に「より多くのパラメーターを使用する」ためです。 トレーニング後（暗黙の正則化を使用）、トレーニングされたモデルはそれほど多くのパラメーターを使用する必要がないため、「非アクティブなパラメーター」が多くなり、有効な次元が低くなります。"

#: ../../tutorials/10_effective_dimension.ipynb:811
msgid "We must keep in mind though that this is the general intuition, and there might be cases where a randomly selected set of weights happens to provide a lower effective dimension than the trained weights for a specific model."
msgstr "これは一般的な洞察であり、あるモデルの訓練された重みよりもランダムに選ばれた重みが偶然低い有効次元を提供するなど、さまざまな場合がありうることに留意しなければなりません。"

