msgid ""
msgstr ""
"Project-Id-Version: qiskit-docs\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-07-12 22:21+0000\n"
"PO-Revision-Date: 2021-07-13 15:44\n"
"Last-Translator: \n"
"Language-Team: Japanese\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"X-Crowdin-Project: qiskit-docs\n"
"X-Crowdin-Project-ID: 369271\n"
"X-Crowdin-Language: ja\n"
"X-Crowdin-File: /master/machine-learning/docs/locale/en/LC_MESSAGES/tutorials.po\n"
"X-Crowdin-File-ID: 9528\n"
"Language: ja_JP\n"

#: ../../tutorials/01_neural_networks.ipynb:13
#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:13
#: ../../tutorials/03_quantum_kernel.ipynb:13
#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:13
#: ../../tutorials/05_torch_connector.ipynb:13
msgid "Run interactively in jupyter notebook."
msgstr "Jupyter ノートブックでインタラクティブに実行します。"

#: ../../tutorials/01_neural_networks.ipynb:9
msgid "Quantum Neural Networks"
msgstr "量子ニューラル・ネットワーク"

#: ../../tutorials/01_neural_networks.ipynb:11
msgid "This notebook demonstrates the different generic quantum neural network (QNN) implementations provided in Qiskit Machine Learning. The networks are meant as application-agnostic computational units that can be used for many different use cases. Depending on the application, a particular type of network might more or less suitable and might require to be set up in a particular way. The following different available neural networks will now be discussed in more detail:"
msgstr "このノートブックでは、 Qiskit 機械学習で提供される、さまざまな汎用的な量子ニューラル・ネットワーク (QNN) の実装を紹介します。 このネットワークは、アプリケーションに依存しない計算ユニットとして、様々なユースケースに使用することができます。アプリケーションによっては、特定のタイプのネットワークが適していたり、適していなかったり、特定の方法でセットアップする必要があったりします。ここでは、次のような種類のニューラルネットワークについて、詳しく説明します。"

#: ../../tutorials/01_neural_networks.ipynb:13
msgid "``NeuralNetwork``: The interface for neural networks."
msgstr "``NeuralNetwork``: ニューラル・ネットワークのインターフェース。"

#: ../../tutorials/01_neural_networks.ipynb:14
msgid "``OpflowQNN``: A network based on the evaluation of quantum mechanical observables."
msgstr "``OflowQNN``: 量子力学的観測量の評価に基づくネットワーク。"

#: ../../tutorials/01_neural_networks.ipynb:15
msgid "``TwoLayerQNN``: A special ``OpflowQNN`` implementation for convenience."
msgstr "``TwoLayerQNN``: 利便性のための特別な ``OpflowQNN`` の実装。"

#: ../../tutorials/01_neural_networks.ipynb:16
msgid "``CircuitQNN``: A network based on the samples resulting from measuring a quantum circuit."
msgstr "``CircuitQNN``: 量子回路を測定することによって得られたサンプルに基づいたネットワーク。"

#: ../../tutorials/01_neural_networks.ipynb:64
msgid "1. ``NeuralNetwork``"
msgstr "1. ``NeuralNetwork``"

#: ../../tutorials/01_neural_networks.ipynb:66
msgid "The ``NeuralNetwork`` represents the interface for all neural networks available in Qiskit Machine Learning. It exposes a forward and a backward pass taking the data samples and trainable weights as input. A ``NeuralNetwork`` does not contain any training capabilities, these are pushed to the actual algorithms / applications. Thus, a ``NeuralNetwork`` also does not store the values for trainable weights. In the following, different implementations of this interfaces are introduced."
msgstr " ``NeuralNetwork`` は、Qiskit MachineLearningで利用可能なすべてのニューラルネットワークのインターフェースを表します。データサンプルとトレーニング可能な重みを入力として取得する順伝播パスと逆伝搬パスを公開します。  ``NeuralNetwork`` にはトレーニング機能は含まれていません。これらは、実際のアルゴリズム/アプリケーションにプッシュされます。したがって、  ``NeuralNetwork`` は、トレーニング可能な重みの値も格納しません。以下では、このインターフェースのさまざまな実装を紹介します。"

#: ../../tutorials/01_neural_networks.ipynb:68
msgid "Suppose a ``NeuralNetwork`` called ``nn``. Then, the ``nn.forward(input, weights)`` pass takes either flat inputs for the data and weights of size ``nn.num_inputs`` and ``nn.num_weights``, respectively. ``NeuralNetwork`` supports batching of inputs and returns batches of output of the corresponding shape."
msgstr "``nn`` という ``NeuralNetwork``  を想定します。次に、 ``nn.forward(input, weights)``  パスは、データのフラット入力と、サイズ　``nn.num_inputs`` および ``nn.num_weights`` の重みをそれぞれ受け取ります。 ``NeuralNetwork`` は入力のバッチ処理をサポートし、対応する形状の出力のバッチを返します。"

#: ../../tutorials/01_neural_networks.ipynb:80
msgid "2. ``OpflowQNN``"
msgstr "2. ``OpflowQNN`` "

#: ../../tutorials/01_neural_networks.ipynb:82
msgid "The ``OpflowQNN`` takes a (parametrized) operator from Qiskit and leverages Qiskit’s gradient framework to provide the backward pass. Such an operator can for instance be an expected value of a quantum mechanical observable with respect to a parametrized quantum state. The Parameters can be used to load classical data as well as represent trainable weights. The ``OpflowQNN`` also allows lists of operators and more complex structures to construct more complex QNNs."
msgstr "``OpflowQNN`` は、Qiskitから（パラメーター化された）演算子を取得し、Qiskitのgradientフレームワークを利用して逆伝搬パスを提供します。このような演算子は、たとえば、パラメーター化された量子状態に関する量子力学における観測量の期待値です。パラメーターを使用して、古典的なデータをロードしたり、トレーニング可能な重みを表すことができます。``OpflowQNN`` を使用すると、演算子のリストとより複雑な構造を使用して、より複雑なQNNを構築することもできます。"

#: ../../tutorials/01_neural_networks.ipynb:321
msgid "Combining multiple observables in a ``ListOp`` also allows to create more complex QNNs"
msgstr "``ListOp`` で複数の観測量（オブザーバブル）を組み合わせることで、より複雑なQNNを作ることもできます。"

#: ../../tutorials/01_neural_networks.ipynb:412
msgid "3. ``TwoLayerQNN``"
msgstr "3. ``TwoLayerQNN``"

#: ../../tutorials/01_neural_networks.ipynb:414
msgid "The ``TwoLayerQNN`` is a special ``OpflowQNN`` on :math:`n` qubits that consists of first a feature map to insert data and second an ansatz that is trained. The default observable is :math:`Z^{\\otimes n}`, i.e., parity."
msgstr "``TwoLayerQNN`` は、:math:`n` 量子ビット上の特別な ``OpflowQNN`` で、第一にデータを挿入する特徴マップ、第二に学習されるアンサツから構成されています。デフォルトの観測値は :math:`Z^{\\otimes n}` 、すなわちパリティです。"

#: ../../tutorials/01_neural_networks.ipynb:612
msgid "4. ``CircuitQNN``"
msgstr "4. ``CircuitQNN``"

#: ../../tutorials/01_neural_networks.ipynb:614
msgid "The ``CircuitQNN`` is based on a (parametrized) ``QuantumCircuit``. This can take input as well as weight parameters and produces samples from the measurement. The samples can either be interpreted as probabilities of measuring the integer index corresponding to a bitstring or directly as a batch of binary output. In the case of probabilities, gradients can be estimated efficiently and the ``CircuitQNN`` provides a backward pass as well. In case of samples, differentiation is not possible and the backward pass returns ``(None, None)``."
msgstr "``CircuitQNN`` は、（パラメータ化された） ``QuantumCircuit`` をベースにしています。これは、入力だけでなくウェイトパラメーターも取ることができ、測定からサンプルを生成します。サンプルは、ビット列に対応する整数インデックスを測定する確率として解釈することも、バイナリー出力のバッチとして直接解釈することもできます。確率の場合、勾配を効率的に推定することができ、 ``CircuitQNN`` は逆伝搬パスも提供します。サンプルの場合、微分は不可能であり、逆伝搬パスは ``(None, None)`` を返します。"

#: ../../tutorials/01_neural_networks.ipynb:617
msgid "Further, the ``CircuitQNN`` allows to specify an ``interpret`` function to post-process the samples. This is expected to take a measured integer (from a bitstring) and map it to a new index, i.e. non-negative integer. In this case, the output shape needs to be provided and the probabilities are aggregated accordingly."
msgstr "さらに、 ``CircuitQNN`` では、サンプルを後処理するための ``interpret`` 関数を指定することができます。これは、（ビット列から）測定された整数を受け取り、それを新しいインデックス、すなわち非負の整数にマッピングすることが期待されます。この場合、出力の形を提供する必要があり、それに応じて確率が調整されます。"

#: ../../tutorials/01_neural_networks.ipynb:619
msgid "A ``CircuitQNN`` can be configured to return sparse as well as dense probability vectors. If no ``interpret`` function is used, the dimension of the probability vector scales exponentially with the number of qubits and a sparse recommendation is usually recommended. In case of an ``interpret`` function it depends on the expected outcome. If, for instance, an index is mapped to the parity of the corresponding bitstring, i.e., to 0 or 1, a dense output makes sense and the result will be a probability vector of length 2."
msgstr " ``CircuitQNN`` は、密な確率ベクトルだけでなく、疎な確率ベクトルを返すように設定できます。 ``interpret`` 関数を使用しない場合、確率ベクトルの次元は量子ビット数に応じて指数関数的に増加するため、通常は疎なものを推奨します。 ``interpret`` 関数を使用する場合は、期待される結果に依存します。例えば、インデックスが対応するビット列のパリティにマッピングされる場合、つまり0または1にマッピングされる場合、密な出力が意味を持ち、結果は長さ2の確率ベクトルになります。"

#: ../../tutorials/01_neural_networks.ipynb:662
msgid "4.1 Output: sparse integer probabilities"
msgstr "4.1 出力: 疎な整数の確率"

#: ../../tutorials/01_neural_networks.ipynb:761
msgid "4.2 Output: dense parity probabilities"
msgstr "4.2 出力: 密なパリティ確率"

#: ../../tutorials/01_neural_networks.ipynb:869
msgid "4.3 Output: Samples"
msgstr "4.3 出力: サンプル"

#: ../../tutorials/01_neural_networks.ipynb:985
msgid "4.4 Output: Parity Samples"
msgstr "4.4 出力: パリティー・サンプル"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:9
msgid "Neural Network Classifier & Regressor"
msgstr "ニューラル・ネットワーク分類器と回帰器"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:11
msgid "In this tutorial we show how the ``NeuralNetworkClassifier`` and ``NeuralNetworkRegressor`` are used. Both take as an input a (Quantum) ``NeuralNetwork`` and leverage it in a specific context. In both cases we also provide a pre-configured variant for convenience, the Variational Quantum Classifier (``VQC``) and Variational Quantum Regressor (``VQR``). The tutorial is structured as follows:"
msgstr "このチュートリアルでは、``NeuralNetworkClassifier`` と ``NeuralNetworkRegressor`` がどのように使用されるかを示します。どちらも入力として (量子) ``NeuralNetwork`` を受け取り、特定のコンテキストでそれを活用します。どちらの場合も、利便性のためにあらかじめ設定されたバリエーション、変分量子分類器 (Variational Quantum Classifier, ``VQC``) と変分量子回帰器 (Variational Quantum Regressor, ``VQR``) を提供しています。チュートリアルの構成は以下の通りです。"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:13
msgid "`Classification <#Classification>`__"
msgstr "`分類 <#Classification>`__"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:15
msgid "Classification with an ``OpflowQNN``"
msgstr "``OpflowQNN`` による分類"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:16
#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:249
msgid "Classification with a ``CircuitQNN``"
msgstr "``CircuitQNN`` による分類"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:17
#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:398
msgid "Variational Quantum Classifier (``VQC``)"
msgstr "変分量子分類器 (``VQC``)"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:19
msgid "`Regression <#Regression>`__"
msgstr "`回帰 <#Regression>`__"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:21
#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:539
msgid "Regression with an ``OpflowQNN``"
msgstr "``OpflowQNN`` による回帰"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:22
msgid "Variational Quantum Regressor (``VQR``)"
msgstr "変分量子回帰器 (``VQR``)"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:70
#: ../../tutorials/03_quantum_kernel.ipynb:53
msgid "Classification"
msgstr "分類"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:72
msgid "We prepare a simple classification dataset to illustrate the following algorithms."
msgstr "以下のアルゴリズムを説明するために、簡単な分類データセットを用意します。"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:117
msgid "Classification with the an ``OpflowQNN``"
msgstr "``OpflowQNN`` による分類"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:119
msgid "First we show how an ``OpflowQNN`` can be used for classification within a ``NeuralNetworkClassifier``. In this context, the ``OpflowQNN`` is expected to return one-dimensional output in :math:`[-1, +1]`. This only works for binary classification and we assign the two classes to :math:`\\{-1, +1\\}`. For convenience, we use the ``TwoLayerQNN``, which is a special type of ``OpflowQNN`` defined via a feature map and an ansatz."
msgstr "まず、 ``OpflowQNN`` が ``NeuralNetworkClassifier`` の中でどのように分類に使われるかを示します。ここでは、 ``OpflowQNN`` は、 :math:`[-1, +1]` の1次元の出力を返すことが期待されています。 これは、二値分類にしか使えないので、2つのクラスを :math:`\\{-1, +1\\}` に割り当てます。ここでは、便宜上、特徴マップとansatzを用いて定義された ``OpflowQNN`` の特別なタイプである ``TwoLayerQNN`` を使用します。"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:251
msgid "Next we show how a ``CircuitQNN`` can be used for classification within a ``NeuralNetworkClassifier``. In this context, the ``CircuitQNN`` is expected to return :math:`d`-dimensional probability vector as output, where :math:`d` denotes the number of classes. Sampling from a ``QuantumCircuit`` automatically results in a probability distribution and we just need to define a mapping from the measured bitstrings to the different classes. For binary classification we use the parity mapping."
msgstr "次に、 ``CircuitQNN`` が ``NeuralNetworkClassifier`` の中でどのように分類に使われるかを示します。ここでは、 ``CircuitQNN`` は、 :math:`d` -次元の確率ベクトルを出力として返すことが期待されます。ここで :math:`d` はクラス数です。``QuantumCircuit`` からのサンプリングは、自動的に確率分布になりますので、測定されたビット列から異なるクラスへのマッピングを定義するだけです。バイナリー分類には，パリティー・マッピングを使います。"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:400
msgid "The ``VQC`` is a special variant of the ``NeuralNetworkClassifier`` with a ``CircuitQNN``. It applies a parity mapping (or extensions to multiple classes) to map from the bitstring to the classification, which results in a probability vector, which is interpreted as a one-hot encoded result. By default, it applies this the ``CrossEntropyLoss`` function that expects labels given in one-hot encoded format and will return predictions in that format too."
msgstr "``VQC`` は、 ``CircuitQNN`` を用いた ``NeuralNetworkClassifier`` の特別なバリエーションです。これは、パリティー・マッピング（または複数のクラスへの拡張）を適用して、ビット列から分類にマッピングし、その結果、確率ベクトルが得られ、ワンショットでエンコードされた結果として解釈されます。デフォルトでは、 ``CrossEntropyLoss`` 関数を適用します。この関数は、ワンショットでエンコードされたフォーマットで与えられたラベルを想定しており、そのフォーマットでも予測値を返します。"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:496
msgid "Regression"
msgstr "回帰"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:498
msgid "We prepare a simple regression dataset to illustrate the following algorithms."
msgstr "以下のアルゴリズムを説明するために、簡単な回帰データセットを用意します。"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:541
msgid "Here we restrict to regression with an ``OpflowQNN`` that returns values in :math:`[-1, +1]`. More complex and also multi-dimensional models could be constructed, also based on ``CircuitQNN`` but that exceeds the scope of this tutorial."
msgstr "ここでは、 :math:`[-1, +1]` の値を返す ``OpflowQNN`` を使った回帰に限定します。もっと複雑で多次元のモデルを ``CircuitQNN`` をベースにして構築することもできますが、このチュートリアルの範囲を超えています。"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:648
msgid "Regression with the Variational Quantum Regressor (``VQR``)"
msgstr "変分量子回帰器 (``VQR``) による回帰"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:650
msgid "Similar to the ``VQC`` for classification, the ``VQR`` is a special variant of the ``NeuralNetworkRegressor`` with a ``OpflowQNN``. By default it considers the ``L2Loss`` function to minimize the mean squared error between predictions and targets."
msgstr "``VQR`` は、分類用の ``VQC`` と同様に、 ``OpflowQNN`` を用いた ``NeuralNetworkRegressor`` の特別な改良版です。デフォルトでは、予測値と目標値の間の平均二乗誤差を最小化するために、 ``L2Loss`` 関数を考慮します。"

#: ../../tutorials/03_quantum_kernel.ipynb:9
msgid "Quantum Kernel Machine Learning"
msgstr "量子カーネル法機械学習"

#: ../../tutorials/03_quantum_kernel.ipynb:11
msgid "The general task of machine learning is to find and study patterns in data. For many datasets, the datapoints are better understood in a higher dimensional feature space, through the use of a kernel function: :math:`k(\\vec{x}_i, \\vec{x}_j) = \\langle f(\\vec{x}_i), f(\\vec{x}_j) \\rangle` where :math:`k` is the kernel function, :math:`\\vec{x}_i, \\vec{x}_j` are :math:`n` dimensional inputs, :math:`f` is a map from :math:`n`-dimension to :math:`m`-dimension space and :math:`\\langle a,b \\rangle` denotes the dot product. When considering finite data, a kernel function can be represented as a matrix: :math:`K_{ij} = k(\\vec{x}_i,\\vec{x}_j)`."
msgstr "機械学習の一般的タスクはデータからパターンを発見したり学習することです。多くのデータセットに対し、データポイントはカーネル関数: :math:`k(\\vec{x}_i, \\vec{x}_j) = \\langle f(\\vec{x}_i), f(\\vec{x}_j) \\rangle` を用いて高次元特徴空間で理解することができます。ここで、:math:`k` はカーネル関数、:math:`\\vec{x}_i, \\vec{x}_j`  は n 次元のインプット、:math:`f` は :math:`n`-次元空間から :math:`m`-次元空間への写像、:math:`\\langle a,b \\rangle` は内積を表します。有限のデータに対して、カーネル関数は行列 :math:`K_{ij} = k(\\vec{x}_i,\\vec{x}_j)` で表現することができます。"

#: ../../tutorials/03_quantum_kernel.ipynb:14
msgid "In quantum kernel machine learning, a quantum feature map :math:`\\phi(\\vec{x})` is used to map a classical feature vector :math:`\\vec{x}` to a quantum Hilbert space, :math:`| \\phi(\\vec{x})\\rangle \\langle \\phi(\\vec{x})|`, such that :math:`K_{ij} = \\left| \\langle \\phi^\\dagger(\\vec{x}_j)| \\phi(\\vec{x}_i) \\rangle \\right|^{2}`. See `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__ for more details."
msgstr "量子カーネル法機械学習において、量子特徴マップ :math:`\\phi(\\vec{x})` は古典特徴ベクトル :math:`\\vec{x}` を :math:`K_{ij} = \\left| \\langle \\phi^\\dagger(\\vec{x}_j)| \\phi(\\vec{x}_i) \\rangle \\right|^{2}` なる量子ヒルベルト空間 :math:`| \\phi(\\vec{x})\\rangle \\langle \\phi(\\vec{x})|` にマップする際に使用されます。詳細は `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__ を参照してください。"

#: ../../tutorials/03_quantum_kernel.ipynb:16
msgid "In this notebook, we use ``qiskit`` to calculate a kernel matrix using a quantum feature map, then use this kernel matrix in ``scikit-learn`` classification and clustering algorithms."
msgstr "このノートブックでは、量子特徴マップを使ってカーネル行列を計算するために ``qiskit`` を使用し、 ``scikit-learn`` の分類とクラスタリングアルゴリズムでこのカーネル行列を使用します。"

#: ../../tutorials/03_quantum_kernel.ipynb:55
msgid "For our classification example, we will use the *ad hoc dataset* as described in `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__, and the ``scikit-learn`` `support vector machine <https://scikit-learn.org/stable/modules/svm.html>`__ classification (``svc``) algorithm."
msgstr "分類の例では、`Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__ で説明されている *アドホック・データセット* と ``scikit-learn`` の `サポートベクターマシン <https://scikit-learn.org/stable/modules/svm.html>`__ (svc) 分類アルゴリズムを使用します。"

#: ../../tutorials/03_quantum_kernel.ipynb:111
msgid "With our training and testing datasets ready, we set up the ``QuantumKernel`` class to calculate a kernel matrix using the `ZZFeatureMap <https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZZFeatureMap.html>`__, and the ``BasicAer`` ``qasm_simulator`` using 1024 shots."
msgstr "学習用とテスト用のデータセットの準備ができたら、`ZZFeatureMap <https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZZFeatureMap.html>`__ を使用してカーネル行列を計算するための ``QuantumKernel`` クラスを設定します。そして ``BasicAer`` ``qasm_simulator`` で 1024 回のショットを実行します。"

#: ../../tutorials/03_quantum_kernel.ipynb:138
msgid "The ``scikit-learn`` ``svc`` algorithm allows us to define a `custom kernel <https://scikit-learn.org/stable/modules/svm.html#custom-kernels>`__ in two ways: by providing the kernel as a callable function or by precomputing the kernel matrix. We can do either of these using the ``QuantumKernel`` class in ``qiskit``."
msgstr "``scikit-learn`` の ``svc`` アルゴリズムでは、`custom kernel <https://scikit-learn.org/stable/modules/svm.html#custom-kernels>`__ を次の2つの方法で定義することができます: 呼び出し可能な関数としてカーネルを提供するか、カーネル行列を予測することです。 ``qiskit`` の ``QuantumKernel`` クラスを使用して、どちらかを行うことができます。"

#: ../../tutorials/03_quantum_kernel.ipynb:140
msgid "The following code gives the kernel as a callable function:"
msgstr "次のコードは、カーネルを呼び出し可能な関数として提供します。"

#: ../../tutorials/03_quantum_kernel.ipynb:184
msgid "The following code precomputes and plots the training and testing kernel matrices before providing them to the ``scikit-learn`` ``svc`` algorithm:"
msgstr "以下のコードは、``scikit-learn`` ``svc`` アルゴリズムにそれらを提供する前に、カーネル行列の学習とテストを事前計算してプロットします。"

#: ../../tutorials/03_quantum_kernel.ipynb:250
msgid "``qiskit`` also contains the ``qsvc`` class that extends the ``sklearn svc`` class, that can be used as follows:"
msgstr "``qiskit`` には、``sklearn svc`` クラスを拡張した ``qsvc`` クラスも含まれており、次のように使用できます。"

#: ../../tutorials/03_quantum_kernel.ipynb:295
msgid "Clustering"
msgstr "クラスタリング"

#: ../../tutorials/03_quantum_kernel.ipynb:297
msgid "For our clustering example, we will again use the *ad hoc dataset* as described in `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__, and the ``scikit-learn`` ``spectral`` clustering algorithm."
msgstr "クラスタリングの例では、`Supervised learning, with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__ で説明されている *アドホック・データセット* と ``scikit-learn`` の ``spectral`` クラスタリング・アルゴリズムを再度使用します。"

#: ../../tutorials/03_quantum_kernel.ipynb:299
msgid "We will regenerate the dataset with a larger gap between the two classes, and as clustering is an unsupervised machine learning task, we don’t need a test sample."
msgstr "2 つのクラス間に大きなギャップがあるデータ・セットを再生成します。クラスタリングは教師なし機械学習タスクであるため、テスト・サンプルは必要ありません。"

#: ../../tutorials/03_quantum_kernel.ipynb:350
msgid "We again set up the ``QuantumKernel`` class to calculate a kernel matrix using the `ZZFeatureMap <https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZZFeatureMap.html>`__, and the BasicAer ``qasm_simulator`` using 1024 shots."
msgstr "再度、 `ZZFeatureMap <https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZZFeatureMap.html>`__ を使用してカーネル行列を計算するために ``QuantumKernel`` クラスを設定し、BasicAerの ``qasm_simulator`` を1024ショットに設定します。"

#: ../../tutorials/03_quantum_kernel.ipynb:377
msgid "The scikit-learn spectral clustering algorithm allows us to define a [custom kernel] in two ways: by providing the kernel as a callable function or by precomputing the kernel matrix. Using the QuantumKernel class in qiskit, we can only use the latter."
msgstr "scikit-learnのスペクトラル・クラスタリング・アルゴリズムでは、[カスタム・カーネル] を2つの方法で定義することができます。つまり、カーネルを呼び出し可能な関数として提供するか、カーネル行列を予測することです。 QiskitのQuantumKernelクラスを使用する場合、後者のみを使用できます。"

#: ../../tutorials/03_quantum_kernel.ipynb:379
msgid "The following code precomputes and plots the kernel matrices before providing it to the scikit-learn spectral clustering algorithm, and scoring the labels using normalized mutual information, since we a priori know the class labels."
msgstr "クラス・ラベルを前もって知っているので、以下のコードは、scikit-learnのスペクトラル・クラスタリング・アルゴリズムに投入し正規化相互情報量でラベルをスコアリングする前に、カーネル行列を事前計算してプロットします。"

#: ../../tutorials/03_quantum_kernel.ipynb:439
msgid "``scikit-learn`` has other algorithms that can use a precomputed kernel matrix, here are a few:"
msgstr "``scikit-learn`` には、事前計算されたカーネル行列を使用できる次のような他のアルゴリズムがあります:"

#: ../../tutorials/03_quantum_kernel.ipynb:441
msgid "`Agglomerative clustering <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html>`__"
msgstr "`Agglomerative clustering <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html>`__"

#: ../../tutorials/03_quantum_kernel.ipynb:442
msgid "`Support vector regression <https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html>`__"
msgstr "`Support vector regression <https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html>`__"

#: ../../tutorials/03_quantum_kernel.ipynb:443
msgid "`Ridge regression <https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html>`__"
msgstr "`Ridge regression <https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html>`__"

#: ../../tutorials/03_quantum_kernel.ipynb:444
msgid "`Gaussian process regression <https://scikit-learn.org/stable/modules/gaussian_process.html>`__"
msgstr "`Gaussian process regression <https://scikit-learn.org/stable/modules/gaussian_process.html>`__"

#: ../../tutorials/03_quantum_kernel.ipynb:445
msgid "`Principal component analysis <https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html>`__"
msgstr "`Principal component analysis <https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html>`__"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:9
msgid "qGANs for Loading Random Distributions"
msgstr "qGANによるランダム分布の書き込み"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:11
msgid "Given :math:`k`-dimensional data samples, we employ a quantum Generative Adversarial Network (qGAN) to learn the data’s underlying random distribution and to load it directly into a quantum state:"
msgstr ":math:`k` 次元のデータサンプルが与えられた場合、量子敵対的生成ネットワーク(qGAN) を使用して、データの基礎となるランダム分布を学習し、それを量子状態に直接ロードします。"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:13
msgid "\\big| g_{\\theta}\\rangle = \\sum_{j=0}^{2^n-1} \\sqrt{p_{\\theta}^{j}}\\big| j \\rangle\n\n"
msgstr "\\big| g_{\\theta}\\rangle = \\sum_{j=0}^{2^n-1} \\sqrt{p_{\\theta}^{j}}\\big| j \\rangle\n\n"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:15
msgid "where :math:`p_{\\theta}^{j}` describe the occurrence probabilities of the basis states :math:`\\big| j\\rangle`."
msgstr "ここで、 :math:`p_{\\theta}^{j}` は、基底状態 :math:`\\big| j\\rangle` の発生確率を表します。"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:17
msgid "The aim of the qGAN training is to generate a state :math:`\\big| g_{\\theta}\\rangle` where :math:`p_{\\theta}^{j}`, for :math:`j\\in \\left\\{0, \\ldots, {2^n-1} \\right\\}`, describe a probability distribution that is close to the distribution underlying the training data :math:`X=\\left\\{x^0, \\ldots, x^{k-1} \\right\\}`."
msgstr "qGAN学習の目的は、状態 :math:`\\big| g_{\\theta}\\rangle` を生成することです。ここで、 :math:`p_{\\theta}^{j}` は、 :math:`j\\in \\left\\{0, \\ldots, {2^n-1} \\right\\}` の場合、 学習データ :math:`X=\\left\\{x^0, \\ldots, x^{k-1} \\right\\}` の基礎となる分布に近い確率分布を表します。"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:19
msgid "For further details please refer to `Quantum Generative Adversarial Networks for Learning and Loading Random Distributions <https://arxiv.org/abs/1904.00043>`__ *Zoufal, Lucchi, Woerner* [2019]."
msgstr "詳細については、「 `Quantum Generative Adversarial Networks for Learning and Loading Random Distributions <https://arxiv.org/abs/1904.00043>`__ *Zoufal, Lucchi, Woerner* [2019] 」を参照してください。"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:21
msgid "For an example of how to use a trained qGAN in an application, the pricing of financial derivatives, please see the `Option Pricing with qGANs <https://github.com/Qiskit/qiskit-finance/tree/main/docs/tutorials/10_qgan_option_pricing.ipynb>`__ tutorial."
msgstr "学習済みの qGAN の実用例として、金融デリバティブの価格設定については、チュートリアルの「 `Option Pricing with qGANs <https://github.com/Qiskit/qiskit-finance/tree/main/docs/tutorials/10_qgan_option_pricing.ipynb>`__ 」を参照してください。"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:56
msgid "Load the Training Data"
msgstr "学習データのロード"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:58
msgid "First, we need to load the :math:`k`-dimensional training data samples (here k=1)."
msgstr "まず :math:`k` 次元の学習データ・サンプルを読み込む必要があります (ここでは k=1 です) 。"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:60
msgid "Next, the data resolution is set, i.e. the min/max data values and the number of qubits used to represent each data dimension."
msgstr "次にデータの分解能、すなわち最小・最大データ値と、各データ次元を表現するために用いられる量子ビット数を設定します。"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:95
msgid "Initialize the qGAN"
msgstr "qGANの初期化"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:97
msgid "The qGAN consists of a quantum generator :math:`G_{\\theta}`, i.e., an ansatz, and a classical discriminator :math:`D_{\\phi}`, a neural network."
msgstr "qGAN は、量子生成器 :math:`G_{\\theta}` 、いわゆるansatzと、ニューラル・ネットワークの 古典識別器 :math:`D_{\\phi}` で構成されています。"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:99
msgid "To implement the quantum generator, we choose a depth-\\ :math:`1` ansatz that implements :math:`R_Y` rotations and :math:`CZ` gates which takes a uniform distribution as an input state. Notably, for :math:`k>1` the generator’s parameters must be chosen carefully. For example, the circuit depth should be :math:`>1` because higher circuit depths enable the representation of more complex structures."
msgstr "量子生成器を実装するために、一様分布を入力に取るような、:math:`R_Y` 回転と :math:`CZ` ゲートからなる深さ :math:`1` のansatzを用います。特に :math:`k>1` の場合、生成器のパラメーターは慎重に選ばれなければなりません。例えば、回路が深いほどより複雑な構造を表現できるので、回路の深さは :math:`>1` であるべきです。"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:101
msgid "The classical discriminator used here is based on a neural network implementation using NumPy. There is also a discriminator based on PyTorch which is not installed by default when installing Qiskit - see `Optional Install <https://github.com/Qiskit/qiskit-machine-learning#optional-installs>`__ for more information."
msgstr "ここで使用する古典識別器は、 NumPyによるニューラル・ネットワークの実装に基づいています。また、 Qiskit のインストール時にはデフォルトでインストールされていない PyTorch ベースの識別器もあります。詳細は「 `Optional Install <https://github.com/Qiskit/qiskit-machine-learning#optional-installs>`__ 」を参照してください。"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:103
msgid "Here, both networks are updated with the ADAM optimization algorithm (ADAM is qGAN optimizer default)."
msgstr "ここでは、両方のネットワークが ADAM 最適化アルゴリズムによって更新されます (ADAM は qGAN のデフォルトのオプティマイザーです) 。"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:164
msgid "Run the qGAN Training"
msgstr "qGAN の学習"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:166
msgid "During the training the discriminator’s and the generator’s parameters are updated alternately w.r.t the following loss functions:"
msgstr "学習では、識別器と生成器のパラメーターが、以下の損失関数に基づいて交互に更新されます。"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:168
msgid "L_G\\left(\\phi, \\theta\\right) = -\\frac{1}{m}\\sum\\limits_{l=1}^{m}\\left[\\log\\left(D_{\\phi}\\left(g^{l}\\right)\\right)\\right]\n\n"
msgstr "L_G\\left(\\phi, \\theta\\right) = -\\frac{1}{m}\\sum\\limits_{l=1}^{m}\\left[\\log\\left(D_{\\phi}\\left(g^{l}\\right)\\right)\\right]\n\n"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:170
msgid "and"
msgstr "および"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:172
msgid "L_D\\left(\\phi, \\theta\\right) =\n"
"  \\frac{1}{m}\\sum\\limits_{l=1}^{m}\\left[\\log D_{\\phi}\\left(x^{l}\\right) + \\log\\left(1-D_{\\phi}\\left(g^{l}\\right)\\right)\\right],"
msgstr "L_D\\left(\\phi, \\theta\\right) =\n"
"  \\frac{1}{m}\\sum\\limits_{l=1}^{m}\\left[\\log D_{\\phi}\\left(x^{l}\\right) + \\log\\left(1-D_{\\phi}\\left(g^{l}\\right)\\right)\\right],"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:177
msgid "with :math:`m` denoting the batch size and :math:`g^l` describing the data samples generated by the quantum generator."
msgstr "ここで :math:`m` はバッチ・サイズを、:math:`g^l` は量子生成器によって生成されたデータ・サンプルを表しています。"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:179
msgid "Please note that the training, for the purpose of this notebook, has been kept briefer by the selection of a known initial point (``init_params``). Without such prior knowledge be aware training may take some while."
msgstr "このノートブックの目的のために、既知の初期点 (``init_params``) を指定することで、学習をより簡単にしていることに注意してください。このような予備知識がないと、学習が時間を要するかもしれません。"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:245
msgid "Training Progress & Outcome"
msgstr "学習過程と結果"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:247
msgid "Now, we plot the evolution of the generator’s and the discriminator’s loss functions during the training, as well as the progress in the relative entropy between the trained and the target distribution."
msgstr "次に、学習における生成器と識別器の損失関数の推移、および学習分布と目標分布の間の相対エントロピーの推移をプロットします。"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:249
msgid "Finally, we also compare the cumulative distribution function (CDF) of the trained distribution to the CDF of the target distribution."
msgstr "最後に、訓練分布の累積分布関数 (cumulative distribution function、CDF) と目標分布の CDF も比較します。"

#: ../../tutorials/05_torch_connector.ipynb:9
msgid "Torch Connector and Hybrid QNNs"
msgstr "Torch コネクターおよびハイブリッド QNN"

#: ../../tutorials/05_torch_connector.ipynb:11
msgid "This tutorial introduces Qiskit’s ``TorchConnector`` class, and demonstrates how the ``TorchConnector`` allows for a natural integration of any ``NeuralNetwork`` from Qiskit Machine Learning into a PyTorch workflow. ``TorchConnector`` takes a Qiskit ``NeuralNetwork`` and makes it available as a PyTorch ``Module``. The resulting module can be seamlessly incorporated into PyTorch classical architectures and trained jointly without additional considerations, enabling the development and testing of novel **hybrid quantum-classical** machine learning architectures."
msgstr "このチュートリアルでは、Qiskitの ``TorchConnector`` クラスを紹介します。そして、 ``TorchConnector`` が Qiskit 機械学習から PyTorch ワークフローに ``NeuralNetwork`` を自然に統合する方法を示します。 ``TorchConnector`` は Qiskitの ``NeuralNetwork`` を受け取り、PyTorchの ``Module`` として利用できるようにします。 得られたモジュールは、PyTorchの古典アーキテクチャーにシームレスに組み込むことができ、追加の考慮事項なしに一緒に学習することができます。 また、新しい **ハイブリッド量子古典** 機械学習アーキテクチャーの開発とテストを可能にします。"

#: ../../tutorials/05_torch_connector.ipynb:15
msgid "Content:"
msgstr "目次:"

#: ../../tutorials/05_torch_connector.ipynb:17
msgid "`Part 1: Simple Classification & Regression <#Part-1:-Simple-Classification-&-Regression>`__"
msgstr "`パート 1: 簡単な分類と回帰 <#Part-1:-Simple-Classification-&-Regression>`__"

#: ../../tutorials/05_torch_connector.ipynb:19
msgid "The first part of this tutorial shows how quantum neural networks can be trained using PyTorch’s automatic differentiation engine (``torch.autograd``, `link <https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html>`__) for simple classification and regression tasks."
msgstr "このチュートリアルの最初の部分は、PyTorchの自動微分エンジン( ``torch.autograd``、 `リンク <https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html>`__) を使用して簡単な分類と回帰タスクのための量子ニューラルネットワークを学習させる方法を示しています。"

#: ../../tutorials/05_torch_connector.ipynb:21
msgid "`Classification <#1.-Classification>`__"
msgstr "`分類 <#1.-Classification>`__"

#: ../../tutorials/05_torch_connector.ipynb:23
msgid "Classification with PyTorch and ``OpflowQNN``"
msgstr "PyTorch と ``OpflowQNN`` を用いた分類"

#: ../../tutorials/05_torch_connector.ipynb:24
msgid "Classification with PyTorch and ``CircuitQNN``"
msgstr "PyTorch と ``CircuitQNN`` を用いた分類"

#: ../../tutorials/05_torch_connector.ipynb:26
msgid "`Regression <#2.-Regression>`__"
msgstr "`回帰 <#2.-Regression>`__"

#: ../../tutorials/05_torch_connector.ipynb:28
msgid "Regression with PyTorch and ``OpflowQNN``"
msgstr "PyTorchと ``OpflowQNN`` による回帰"

#: ../../tutorials/05_torch_connector.ipynb:30
msgid "`Part 2: MNIST Classification, Hybrid QNNs <#Part-2:-MNIST-Classification,-Hybrid-QNNs>`__"
msgstr "`パート 2: MNIST 分類、ハイブリッドQNN <#Part-2:-MNIST-Classification,-Hybrid-QNNs>`__"

#: ../../tutorials/05_torch_connector.ipynb:32
msgid "The second part of this tutorial illustrates how to embed a (Quantum) ``NeuralNetwork`` into a target PyTorch workflow (in this case, a typical CNN architecture) to classify MNIST data in a hybrid quantum-classical manner."
msgstr "このチュートリアルの 2 番目の部分では、ハイブリッドの量子古典的な方法で MNIST データを分類するため、 (量子) ``NeuralNetwork`` をターゲットの PyTorch ワークフロー ( この場合は、典型的な CNN アーキテクチャー) に組み込む方法を説明しています。"

#: ../../tutorials/05_torch_connector.ipynb:74
msgid "Part 1: Simple Classification & Regression"
msgstr "パート 1: 簡単な分類と回帰"

#: ../../tutorials/05_torch_connector.ipynb:86
msgid "1. Classification"
msgstr "1. 分類"

#: ../../tutorials/05_torch_connector.ipynb:88
msgid "First, we show how ``TorchConnector`` allows to train a Quantum ``NeuralNetwork`` to solve a classification tasks using PyTorch’s automatic differentiation engine. In order to illustrate this, we will perform **binary classification** on a randomly generated dataset."
msgstr "最初に、``TorchConnector`` が PyTorch の自動微分エンジンを使用して、分類タスクを解決するために 量子 ``NeuralNetwork`` を学習させる方法を示します。 これを示すために、ランダムに生成されたデータセットに対し **バイナリ分類** を実行します。"

#: ../../tutorials/05_torch_connector.ipynb:144
msgid "A. Classification with PyTorch and ``OpflowQNN``"
msgstr "A. PyTorch と ``OpflowQNN`` を用いた分類"

#: ../../tutorials/05_torch_connector.ipynb:146
msgid "Linking an ``OpflowQNN`` to PyTorch is relatively straightforward. Here we illustrate this using the ``TwoLayerQNN``, a sub-case of ``OpflowQNN`` introduced in previous tutorials."
msgstr "``OpflowQNN`` を PyTorch にリンクするのは比較的簡単です。ここでは、前のチュートリアルで紹介した ``OpflowQNN`` のサブケースである ``TwoLayerQNN`` を使用して説明します。"

#: ../../tutorials/05_torch_connector.ipynb:254
msgid "Optimizer"
msgstr "オプティマイザー"

#: ../../tutorials/05_torch_connector.ipynb:256
msgid "The choice of optimizer for training any machine learning model can be crucial in determining the success of our training’s outcome. When using ``TorchConnector``, we get access to all of the optimizer algorithms defined in the [``torch.optim``] package (`link <https://pytorch.org/docs/stable/optim.html>`__). Some of the most famous algorithms used in popular machine learning architectures include *Adam*, *SGD*, or *Adagrad*. However, for this tutorial we will be using the L-BFGS algorithm (``torch.optim.LBFGS``), one of the most well know second-order optimization algorithms for numerical optimization."
msgstr "あらゆる機械学習モデルを学習させる上でオプティマイザーの選択は、学習の成果を決定する上で非常に重要です。 ``TorchConnector`` を使用する場合、[``torch.optim``] パッケージ (`リンク <https://pytorch.org/docs/stable/optim.html>`__) で定義されているすべてのオプティマイザー・アルゴリズムを使用できます。 一般的な機械学習アーキテクチャーで使用される最も有名なアルゴリズムには、*Adam*、*SGD*、または *Adagrad* があります。 しかし、このチュートリアルではL-BFGSアルゴリズム(``torch.optim.LBFGS``)を使用します。 数値最適化のための最もよく知られている2次最適化アルゴリズムの1つです。"

#: ../../tutorials/05_torch_connector.ipynb:260
msgid "Loss Function"
msgstr "損失関数"

#: ../../tutorials/05_torch_connector.ipynb:262
msgid "As for the loss function, we can also take advantage of PyTorch’s pre-defined modules from ``torch.nn``, such as the `Cross-Entropy <https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html>`__ or `Mean Squared Error <https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html>`__ losses."
msgstr "損失関数については、`交差エントロピー <https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html>`__ や `平均二乗誤差 <https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html>`__ 損失といった、 ``torch.nn`` パッケージからPyTorchの事前定義モジュールを利用することができます。"

#: ../../tutorials/05_torch_connector.ipynb:264
msgid "**💡 Clarification :** In classical machine learning, the general rule of thumb is to apply a Cross-Entropy loss to classification tasks, and MSE loss to regression tasks. However, this recommendation is given under the assumption that the output of the classification network is a class probability value in the [0,1] range (usually this is achieved through a Softmax layer). Because the following example for ``TwoLayerQNN`` does not include such layer, and we don’t apply any mapping to the output (the following section shows an example of application of parity mapping with ``CircuitQNNs``), the QNN’s output can take any value in the range [-1,1]. In case you were wondering, this is the reason why this particular example uses MSELoss for classification despite it not being the norm (but we encourage you to experiment with different loss functions and see how they can impact training results)."
msgstr "**💡 解説 :** 古典機械学習において一般的な経験則は、分類タスクに交差エントロピー損失を適用し、回帰タスクにMSE損失を適用することです。 しかし、この推奨は、分類ネットワークの出力が [0 , 1] 範囲 (通常は Softmax レイヤーを介して達成) の分類確率値であることを前提としています。 ``TwoLayerQNN`` の以下の例にはそのようなレイヤーが含まれないため、また、出力にマッピングを適用することもない (次のセクションでは ``CircuitQNNs`` を使用したパリティ・マッピングの例を示します) ため、QNNの出力は、[-1,1] の範囲で任意の値を取ることができます。 因みに、これが、この特定の例でMSELossを一般的でないのにも関わらず分類に使用している理由です(ただし、さまざまな損失関数を試して、学習結果にどのような影響を与えるかを確認することをお勧めします)。"

#: ../../tutorials/05_torch_connector.ipynb:442
#: ../../tutorials/05_torch_connector.ipynb:674
msgid "The red circles indicate wrongly classified data points."
msgstr "赤い丸は、誤って分類されたデータポイントを示します。"

#: ../../tutorials/05_torch_connector.ipynb:454
msgid "B. Classification with PyTorch and ``CircuitQNN``"
msgstr "B. PyTorch と ``CircuitQNN`` を用いた分類"

#: ../../tutorials/05_torch_connector.ipynb:456
msgid "Linking an ``CircuitQNN`` to PyTorch requires a bit more attention than ``OpflowQNN``. Without the correct setup, backpropagation is not possible."
msgstr "``CircuitQNN`` を PyTorch にリンクするには、``OpflowQNN`` よりも少し注意が必要です。正しい設定がなければ、バックプロパゲーションはできません。"

#: ../../tutorials/05_torch_connector.ipynb:458
msgid "In particular, we must make sure that we are returning a dense array of probabilities in the network’s forward pass (``sparse=False``). This parameter is set up to ``False`` by default, so we just have to make sure that it has not been changed."
msgstr "特に、ネットワークのフォワードパス(``sparse=False``) に確率の密な配列を返していることを確認する必要があります。 このパラメーターはデフォルトでは ``False`` に設定されているため、変更されていないことを確認する必要があります。"

#: ../../tutorials/05_torch_connector.ipynb:460
msgid "**⚠️ Attention:** If we define a custom interpret function ( in the example: ``parity``), we must remember to explicitly provide the desired output shape ( in the example: ``2``). For more info on the initial parameter setup for ``CircuitQNN``, please check out the `official qiskit documentation <https://qiskit.org/documentation/machine-learning/stubs/qiskit_machine_learning.neural_networks.CircuitQNN.html>`__."
msgstr "**⚠️ 注意:** カスタムのinterpret関数を定義した場合 (例: ``parity``) 、期待する出力の形状 (例: ``2``) を明示的に指定する必要があります。``CircuitQNN`` の初期設定に関する詳細は、`公式のQiskitドキュメンテーション <https://qiskit.org/documentation/machine-learning/stubs/qiskit_machine_learning.neural_networks.CircuitQNN.html>`__ を参照してください。"

#: ../../tutorials/05_torch_connector.ipynb:523
#: ../../tutorials/05_torch_connector.ipynb:815
msgid "For a reminder on optimizer and loss function choices, you can go back to `this section <#Optimizer>`__."
msgstr "オプティマイザーと損失関数の選択について思い出すには、`このセクション <#Optimizer>`__ に戻ってください。"

#: ../../tutorials/05_torch_connector.ipynb:686
msgid "2. Regression"
msgstr "2. 回帰"

#: ../../tutorials/05_torch_connector.ipynb:688
msgid "We use a model based on the ``TwoLayerQNN`` to also illustrate how to perform a regression task. The chosen dataset in this case is randomly generated following a sine wave."
msgstr "``TwoLayerQNN`` に基づいたモデルを使用して、回帰タスクの実行方法を説明します。 今回選択されたデータセットは、正弦波に沿ってランダムに生成されたものです。"

#: ../../tutorials/05_torch_connector.ipynb:730
msgid "A. Regression with PyTorch and ``OpflowQNN``"
msgstr "A. PyTorchと ``OpflowQNN`` による回帰"

#: ../../tutorials/05_torch_connector.ipynb:741
msgid "The network definition and training loop will be analogous to those of the classification task using ``TwoLayerQNN``. In this case, we define our own feature map and ansatz, instead of using the default values."
msgstr "ネットワーク定義と学習ループは、``TwoLayerQNN`` を使用した分類タスクのものと類似しています。 この場合、デフォルト値を使用するのではなく、独自の特徴マップとansatzを定義します。"

#: ../../tutorials/05_torch_connector.ipynb:963
msgid "Part 2: MNIST Classification, Hybrid QNNs"
msgstr "パート 2: MNIST 分類、ハイブリッドQNN"

#: ../../tutorials/05_torch_connector.ipynb:965
msgid "In this second part, we show how to leverage a hybrid quantum-classical neural network using ``TorchConnector``, to perform a more complex image classification task on the MNIST handwritten digits dataset."
msgstr "2番目の部分では、``TorchConnector`` を使用したハイブリッドの量子古典的なニューラルネットワークの活用方法を示します。 より複雑な画像分類タスクをMNISTの手書きの数字データセットで実行します。"

#: ../../tutorials/05_torch_connector.ipynb:967
msgid "For a more detailed (pre-``TorchConnector``) explanation on hybrid quantum-classical neural networks, you can check out the corresponding section in the `Qiskit Textbook <https://qiskit.org/textbook/ch-machine-learning/machine-learning-qiskit-pytorch.html>`__."
msgstr "ハイブリッドの量子古典ニューラルネットワークの詳細(``TorchConnector`` の前)については、`Qiskit Textbook <https://qiskit.org/textbook/ch-machine-learning/machine-learning-qiskit-pytorch.html>`__ の対応するセクションを参照してください。"

#: ../../tutorials/05_torch_connector.ipynb:996
msgid "Step 1: Defining Data-loaders for train and test"
msgstr "ステップ 1: 学習とテスト用のデータ・ローダーの定義"

#: ../../tutorials/05_torch_connector.ipynb:1007
msgid "We take advantage of the ``torchvision`` `API <https://pytorch.org/vision/stable/datasets.html>`__ to directly load a subset of the `MNIST dataset <https://en.wikipedia.org/wiki/MNIST_database>`__ and define torch ``DataLoader``\\ s (`link <https://pytorch.org/docs/stable/data.html>`__) for train and test."
msgstr "``torchvision`` `API <https://pytorch.org/vision/stable/datasets.html>`__ を利用して、 `MNIST データセット <https://en.wikipedia.org/wiki/MNIST_database>`__ のサブセットを直接ロードし、学習とテストのための ``DataLoader`` (`リンク <https://pytorch.org/docs/stable/data.html>`__) を定義します。"

#: ../../tutorials/05_torch_connector.ipynb:1048
msgid "If we perform a quick visualization we can see that the train dataset consists of images of handwritten 0s and 1s."
msgstr "簡単な可視化を実行すると、学習のデータセットは手書きの0と1の画像で構成されていることがわかります。"

#: ../../tutorials/05_torch_connector.ipynb:1120
msgid "Step 2: Defining the QNN and Hybrid Model"
msgstr "ステップ 2: QNNとハイブリッド・モデルの定義"

#: ../../tutorials/05_torch_connector.ipynb:1131
msgid "This second step shows the power of the ``TorchConnector``. After defining our quantum neural network layer (in this case, a ``TwoLayerQNN``), we can embed it into a layer in our torch ``Module`` by initializing a torch connector as ``TorchConnector(qnn)``."
msgstr "この2番目のステップは、 ``TorchConnector`` の実力を示します。 量子ニューラルネットワーク層を定義した後 (この場合は ``TwoLayerQNN``) 、torchコネクターを ``TorchConnector(qnn)`` として初期化することで、torch ``Module`` のレイヤーに埋め込むことができます。"

#: ../../tutorials/05_torch_connector.ipynb:1133
msgid "**⚠️ Attention:** In order to have an adequate gradient backpropagation in hybrid models, we MUST set the initial parameter ``input_gradients`` to TRUE during the qnn initialization."
msgstr "**⚠️ 注意:** ハイブリッド・モデルで、適切な勾配バックプロパゲーションを行うためには、QNNの初期化中に、初期パラメーター ``input_gradients`` を TRUE に設定する必要があります。"

#: ../../tutorials/05_torch_connector.ipynb:1235
msgid "Step 3: Training"
msgstr "ステップ 3: 学習"

#: ../../tutorials/05_torch_connector.ipynb:1337
msgid "Step 4: Evaluation"
msgstr "ステップ 4: 評価"

#: ../../tutorials/05_torch_connector.ipynb:1440
msgid "🎉🎉🎉🎉 **You are now able to experiment with your own hybrid datasets and architectures using Qiskit Machine Learning.** **Good Luck!**"
msgstr "🎉🎉🎉🎉🎉 **これで、Qiskit 機械学習を使用して、独自のハイブリッド・データセットとアーキテクチャを試すことができます。** **頑張ってください!**"

#: ../../tutorials/index.rst:3
msgid "Machine Learning Tutorials"
msgstr "機械学習チュートリアル"

