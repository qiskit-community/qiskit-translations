msgid ""
msgstr ""
"Project-Id-Version: qiskit-docs\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-07-12 22:21+0000\n"
"PO-Revision-Date: 2021-07-13 15:44\n"
"Last-Translator: \n"
"Language-Team: Japanese\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"
"Plural-Forms: nplurals=1; plural=0;\n"
"X-Crowdin-Project: qiskit-docs\n"
"X-Crowdin-Project-ID: 369271\n"
"X-Crowdin-Language: ja\n"
"X-Crowdin-File: /master/machine-learning/docs/locale/en/LC_MESSAGES/tutorials.po\n"
"X-Crowdin-File-ID: 9528\n"
"Language: ja_JP\n"

#: ../../tutorials/01_neural_networks.ipynb:13
#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:13
#: ../../tutorials/03_quantum_kernel.ipynb:13
#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:13
#: ../../tutorials/05_torch_connector.ipynb:13
msgid "Run interactively in jupyter notebook."
msgstr "Jupyter ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã«å®Ÿè¡Œã—ã¾ã™ã€‚"

#: ../../tutorials/01_neural_networks.ipynb:9
msgid "Quantum Neural Networks"
msgstr "é‡å­ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒ»ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯"

#: ../../tutorials/01_neural_networks.ipynb:11
msgid "This notebook demonstrates the different generic quantum neural network (QNN) implementations provided in Qiskit Machine Learning. The networks are meant as application-agnostic computational units that can be used for many different use cases. Depending on the application, a particular type of network might more or less suitable and might require to be set up in a particular way. The following different available neural networks will now be discussed in more detail:"
msgstr "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€ Qiskit æ©Ÿæ¢°å­¦ç¿’ã§æä¾›ã•ã‚Œã‚‹ã€ã•ã¾ã–ã¾ãªæ±ç”¨çš„ãªé‡å­ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒ»ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ (QNN) ã®å®Ÿè£…ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚ ã“ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯ã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ä¾å­˜ã—ãªã„è¨ˆç®—ãƒ¦ãƒ‹ãƒƒãƒˆã¨ã—ã¦ã€æ§˜ã€…ãªãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã«ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚ˆã£ã¦ã¯ã€ç‰¹å®šã®ã‚¿ã‚¤ãƒ—ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒé©ã—ã¦ã„ãŸã‚Šã€é©ã—ã¦ã„ãªã‹ã£ãŸã‚Šã€ç‰¹å®šã®æ–¹æ³•ã§ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹å¿…è¦ãŒã‚ã£ãŸã‚Šã—ã¾ã™ã€‚ã“ã“ã§ã¯ã€æ¬¡ã®ã‚ˆã†ãªç¨®é¡ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ã¤ã„ã¦ã€è©³ã—ãèª¬æ˜ã—ã¾ã™ã€‚"

#: ../../tutorials/01_neural_networks.ipynb:13
msgid "``NeuralNetwork``: The interface for neural networks."
msgstr "``NeuralNetwork``: ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒ»ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã€‚"

#: ../../tutorials/01_neural_networks.ipynb:14
msgid "``OpflowQNN``: A network based on the evaluation of quantum mechanical observables."
msgstr "``OflowQNN``: é‡å­åŠ›å­¦çš„è¦³æ¸¬é‡ã®è©•ä¾¡ã«åŸºã¥ããƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€‚"

#: ../../tutorials/01_neural_networks.ipynb:15
msgid "``TwoLayerQNN``: A special ``OpflowQNN`` implementation for convenience."
msgstr "``TwoLayerQNN``: åˆ©ä¾¿æ€§ã®ãŸã‚ã®ç‰¹åˆ¥ãª ``OpflowQNN`` ã®å®Ÿè£…ã€‚"

#: ../../tutorials/01_neural_networks.ipynb:16
msgid "``CircuitQNN``: A network based on the samples resulting from measuring a quantum circuit."
msgstr "``CircuitQNN``: é‡å­å›è·¯ã‚’æ¸¬å®šã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦å¾—ã‚‰ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«ã«åŸºã¥ã„ãŸãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€‚"

#: ../../tutorials/01_neural_networks.ipynb:64
msgid "1. ``NeuralNetwork``"
msgstr "1. ``NeuralNetwork``"

#: ../../tutorials/01_neural_networks.ipynb:66
msgid "The ``NeuralNetwork`` represents the interface for all neural networks available in Qiskit Machine Learning. It exposes a forward and a backward pass taking the data samples and trainable weights as input. A ``NeuralNetwork`` does not contain any training capabilities, these are pushed to the actual algorithms / applications. Thus, a ``NeuralNetwork`` also does not store the values for trainable weights. In the following, different implementations of this interfaces are introduced."
msgstr " ``NeuralNetwork`` ã¯ã€Qiskit MachineLearningã§åˆ©ç”¨å¯èƒ½ãªã™ã¹ã¦ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’è¡¨ã—ã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªé‡ã¿ã‚’å…¥åŠ›ã¨ã—ã¦å–å¾—ã™ã‚‹é †ä¼æ’­ãƒ‘ã‚¹ã¨é€†ä¼æ¬ãƒ‘ã‚¹ã‚’å…¬é–‹ã—ã¾ã™ã€‚  ``NeuralNetwork`` ã«ã¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ©Ÿèƒ½ã¯å«ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã“ã‚Œã‚‰ã¯ã€å®Ÿéš›ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ /ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ãƒ—ãƒƒã‚·ãƒ¥ã•ã‚Œã¾ã™ã€‚ã—ãŸãŒã£ã¦ã€  ``NeuralNetwork`` ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªé‡ã¿ã®å€¤ã‚‚æ ¼ç´ã—ã¾ã›ã‚“ã€‚ä»¥ä¸‹ã§ã¯ã€ã“ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®ã•ã¾ã–ã¾ãªå®Ÿè£…ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚"

#: ../../tutorials/01_neural_networks.ipynb:68
msgid "Suppose a ``NeuralNetwork`` called ``nn``. Then, the ``nn.forward(input, weights)`` pass takes either flat inputs for the data and weights of size ``nn.num_inputs`` and ``nn.num_weights``, respectively. ``NeuralNetwork`` supports batching of inputs and returns batches of output of the corresponding shape."
msgstr "``nn`` ã¨ã„ã† ``NeuralNetwork``  ã‚’æƒ³å®šã—ã¾ã™ã€‚æ¬¡ã«ã€ ``nn.forward(input, weights)``  ãƒ‘ã‚¹ã¯ã€ãƒ‡ãƒ¼ã‚¿ã®ãƒ•ãƒ©ãƒƒãƒˆå…¥åŠ›ã¨ã€ã‚µã‚¤ã‚ºã€€``nn.num_inputs`` ãŠã‚ˆã³ ``nn.num_weights`` ã®é‡ã¿ã‚’ãã‚Œãã‚Œå—ã‘å–ã‚Šã¾ã™ã€‚ ``NeuralNetwork`` ã¯å…¥åŠ›ã®ãƒãƒƒãƒå‡¦ç†ã‚’ã‚µãƒãƒ¼ãƒˆã—ã€å¯¾å¿œã™ã‚‹å½¢çŠ¶ã®å‡ºåŠ›ã®ãƒãƒƒãƒã‚’è¿”ã—ã¾ã™ã€‚"

#: ../../tutorials/01_neural_networks.ipynb:80
msgid "2. ``OpflowQNN``"
msgstr "2. ``OpflowQNN`` "

#: ../../tutorials/01_neural_networks.ipynb:82
msgid "The ``OpflowQNN`` takes a (parametrized) operator from Qiskit and leverages Qiskitâ€™s gradient framework to provide the backward pass. Such an operator can for instance be an expected value of a quantum mechanical observable with respect to a parametrized quantum state. The Parameters can be used to load classical data as well as represent trainable weights. The ``OpflowQNN`` also allows lists of operators and more complex structures to construct more complex QNNs."
msgstr "``OpflowQNN`` ã¯ã€Qiskitã‹ã‚‰ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼åŒ–ã•ã‚ŒãŸï¼‰æ¼”ç®—å­ã‚’å–å¾—ã—ã€Qiskitã®gradientãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’åˆ©ç”¨ã—ã¦é€†ä¼æ¬ãƒ‘ã‚¹ã‚’æä¾›ã—ã¾ã™ã€‚ã“ã®ã‚ˆã†ãªæ¼”ç®—å­ã¯ã€ãŸã¨ãˆã°ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼åŒ–ã•ã‚ŒãŸé‡å­çŠ¶æ…‹ã«é–¢ã™ã‚‹é‡å­åŠ›å­¦ã«ãŠã‘ã‚‹è¦³æ¸¬é‡ã®æœŸå¾…å€¤ã§ã™ã€‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’ä½¿ç”¨ã—ã¦ã€å¤å…¸çš„ãªãƒ‡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ã—ãŸã‚Šã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªé‡ã¿ã‚’è¡¨ã™ã“ã¨ãŒã§ãã¾ã™ã€‚``OpflowQNN`` ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€æ¼”ç®—å­ã®ãƒªã‚¹ãƒˆã¨ã‚ˆã‚Šè¤‡é›‘ãªæ§‹é€ ã‚’ä½¿ç”¨ã—ã¦ã€ã‚ˆã‚Šè¤‡é›‘ãªQNNã‚’æ§‹ç¯‰ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚"

#: ../../tutorials/01_neural_networks.ipynb:321
msgid "Combining multiple observables in a ``ListOp`` also allows to create more complex QNNs"
msgstr "``ListOp`` ã§è¤‡æ•°ã®è¦³æ¸¬é‡ï¼ˆã‚ªãƒ–ã‚¶ãƒ¼ãƒãƒ–ãƒ«ï¼‰ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šè¤‡é›‘ãªQNNã‚’ä½œã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚"

#: ../../tutorials/01_neural_networks.ipynb:412
msgid "3. ``TwoLayerQNN``"
msgstr "3. ``TwoLayerQNN``"

#: ../../tutorials/01_neural_networks.ipynb:414
msgid "The ``TwoLayerQNN`` is a special ``OpflowQNN`` on :math:`n` qubits that consists of first a feature map to insert data and second an ansatz that is trained. The default observable is :math:`Z^{\\otimes n}`, i.e., parity."
msgstr "``TwoLayerQNN`` ã¯ã€:math:`n` é‡å­ãƒ“ãƒƒãƒˆä¸Šã®ç‰¹åˆ¥ãª ``OpflowQNN`` ã§ã€ç¬¬ä¸€ã«ãƒ‡ãƒ¼ã‚¿ã‚’æŒ¿å…¥ã™ã‚‹ç‰¹å¾´ãƒãƒƒãƒ—ã€ç¬¬äºŒã«å­¦ç¿’ã•ã‚Œã‚‹ã‚¢ãƒ³ã‚µãƒ„ã‹ã‚‰æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®è¦³æ¸¬å€¤ã¯ :math:`Z^{\\otimes n}` ã€ã™ãªã‚ã¡ãƒ‘ãƒªãƒ†ã‚£ã§ã™ã€‚"

#: ../../tutorials/01_neural_networks.ipynb:612
msgid "4. ``CircuitQNN``"
msgstr "4. ``CircuitQNN``"

#: ../../tutorials/01_neural_networks.ipynb:614
msgid "The ``CircuitQNN`` is based on a (parametrized) ``QuantumCircuit``. This can take input as well as weight parameters and produces samples from the measurement. The samples can either be interpreted as probabilities of measuring the integer index corresponding to a bitstring or directly as a batch of binary output. In the case of probabilities, gradients can be estimated efficiently and the ``CircuitQNN`` provides a backward pass as well. In case of samples, differentiation is not possible and the backward pass returns ``(None, None)``."
msgstr "``CircuitQNN`` ã¯ã€ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã•ã‚ŒãŸï¼‰ ``QuantumCircuit`` ã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ã¦ã„ã¾ã™ã€‚ã“ã‚Œã¯ã€å…¥åŠ›ã ã‘ã§ãªãã‚¦ã‚§ã‚¤ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚‚å–ã‚‹ã“ã¨ãŒã§ãã€æ¸¬å®šã‹ã‚‰ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆã—ã¾ã™ã€‚ã‚µãƒ³ãƒ—ãƒ«ã¯ã€ãƒ“ãƒƒãƒˆåˆ—ã«å¯¾å¿œã™ã‚‹æ•´æ•°ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ¸¬å®šã™ã‚‹ç¢ºç‡ã¨ã—ã¦è§£é‡ˆã™ã‚‹ã“ã¨ã‚‚ã€ãƒã‚¤ãƒŠãƒªãƒ¼å‡ºåŠ›ã®ãƒãƒƒãƒã¨ã—ã¦ç›´æ¥è§£é‡ˆã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ç¢ºç‡ã®å ´åˆã€å‹¾é…ã‚’åŠ¹ç‡çš„ã«æ¨å®šã™ã‚‹ã“ã¨ãŒã§ãã€ ``CircuitQNN`` ã¯é€†ä¼æ¬ãƒ‘ã‚¹ã‚‚æä¾›ã—ã¾ã™ã€‚ã‚µãƒ³ãƒ—ãƒ«ã®å ´åˆã€å¾®åˆ†ã¯ä¸å¯èƒ½ã§ã‚ã‚Šã€é€†ä¼æ¬ãƒ‘ã‚¹ã¯ ``(None, None)`` ã‚’è¿”ã—ã¾ã™ã€‚"

#: ../../tutorials/01_neural_networks.ipynb:617
msgid "Further, the ``CircuitQNN`` allows to specify an ``interpret`` function to post-process the samples. This is expected to take a measured integer (from a bitstring) and map it to a new index, i.e.Â non-negative integer. In this case, the output shape needs to be provided and the probabilities are aggregated accordingly."
msgstr "ã•ã‚‰ã«ã€ ``CircuitQNN`` ã§ã¯ã€ã‚µãƒ³ãƒ—ãƒ«ã‚’å¾Œå‡¦ç†ã™ã‚‹ãŸã‚ã® ``interpret`` é–¢æ•°ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã“ã‚Œã¯ã€ï¼ˆãƒ“ãƒƒãƒˆåˆ—ã‹ã‚‰ï¼‰æ¸¬å®šã•ã‚ŒãŸæ•´æ•°ã‚’å—ã‘å–ã‚Šã€ãã‚Œã‚’æ–°ã—ã„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã€ã™ãªã‚ã¡éè² ã®æ•´æ•°ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã™ã‚‹ã“ã¨ãŒæœŸå¾…ã•ã‚Œã¾ã™ã€‚ã“ã®å ´åˆã€å‡ºåŠ›ã®å½¢ã‚’æä¾›ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã€ãã‚Œã«å¿œã˜ã¦ç¢ºç‡ãŒèª¿æ•´ã•ã‚Œã¾ã™ã€‚"

#: ../../tutorials/01_neural_networks.ipynb:619
msgid "A ``CircuitQNN`` can be configured to return sparse as well as dense probability vectors. If no ``interpret`` function is used, the dimension of the probability vector scales exponentially with the number of qubits and a sparse recommendation is usually recommended. In case of an ``interpret`` function it depends on the expected outcome. If, for instance, an index is mapped to the parity of the corresponding bitstring, i.e., to 0 or 1, a dense output makes sense and the result will be a probability vector of length 2."
msgstr " ``CircuitQNN`` ã¯ã€å¯†ãªç¢ºç‡ãƒ™ã‚¯ãƒˆãƒ«ã ã‘ã§ãªãã€ç–ãªç¢ºç‡ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿”ã™ã‚ˆã†ã«è¨­å®šã§ãã¾ã™ã€‚ ``interpret`` é–¢æ•°ã‚’ä½¿ç”¨ã—ãªã„å ´åˆã€ç¢ºç‡ãƒ™ã‚¯ãƒˆãƒ«ã®æ¬¡å…ƒã¯é‡å­ãƒ“ãƒƒãƒˆæ•°ã«å¿œã˜ã¦æŒ‡æ•°é–¢æ•°çš„ã«å¢—åŠ ã™ã‚‹ãŸã‚ã€é€šå¸¸ã¯ç–ãªã‚‚ã®ã‚’æ¨å¥¨ã—ã¾ã™ã€‚ ``interpret`` é–¢æ•°ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€æœŸå¾…ã•ã‚Œã‚‹çµæœã«ä¾å­˜ã—ã¾ã™ã€‚ä¾‹ãˆã°ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒå¯¾å¿œã™ã‚‹ãƒ“ãƒƒãƒˆåˆ—ã®ãƒ‘ãƒªãƒ†ã‚£ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã•ã‚Œã‚‹å ´åˆã€ã¤ã¾ã‚Š0ã¾ãŸã¯1ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã•ã‚Œã‚‹å ´åˆã€å¯†ãªå‡ºåŠ›ãŒæ„å‘³ã‚’æŒã¡ã€çµæœã¯é•·ã•2ã®ç¢ºç‡ãƒ™ã‚¯ãƒˆãƒ«ã«ãªã‚Šã¾ã™ã€‚"

#: ../../tutorials/01_neural_networks.ipynb:662
msgid "4.1 Output: sparse integer probabilities"
msgstr "4.1 å‡ºåŠ›: ç–ãªæ•´æ•°ã®ç¢ºç‡"

#: ../../tutorials/01_neural_networks.ipynb:761
msgid "4.2 Output: dense parity probabilities"
msgstr "4.2 å‡ºåŠ›: å¯†ãªãƒ‘ãƒªãƒ†ã‚£ç¢ºç‡"

#: ../../tutorials/01_neural_networks.ipynb:869
msgid "4.3 Output: Samples"
msgstr "4.3 å‡ºåŠ›: ã‚µãƒ³ãƒ—ãƒ«"

#: ../../tutorials/01_neural_networks.ipynb:985
msgid "4.4 Output: Parity Samples"
msgstr "4.4 å‡ºåŠ›: ãƒ‘ãƒªãƒ†ã‚£ãƒ¼ãƒ»ã‚µãƒ³ãƒ—ãƒ«"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:9
msgid "Neural Network Classifier & Regressor"
msgstr "ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒ»ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†é¡å™¨ã¨å›å¸°å™¨"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:11
msgid "In this tutorial we show how the ``NeuralNetworkClassifier`` and ``NeuralNetworkRegressor`` are used. Both take as an input a (Quantum) ``NeuralNetwork`` and leverage it in a specific context. In both cases we also provide a pre-configured variant for convenience, the Variational Quantum Classifier (``VQC``) and Variational Quantum Regressor (``VQR``). The tutorial is structured as follows:"
msgstr "ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ã€``NeuralNetworkClassifier`` ã¨ ``NeuralNetworkRegressor`` ãŒã©ã®ã‚ˆã†ã«ä½¿ç”¨ã•ã‚Œã‚‹ã‹ã‚’ç¤ºã—ã¾ã™ã€‚ã©ã¡ã‚‰ã‚‚å…¥åŠ›ã¨ã—ã¦ (é‡å­) ``NeuralNetwork`` ã‚’å—ã‘å–ã‚Šã€ç‰¹å®šã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ãã‚Œã‚’æ´»ç”¨ã—ã¾ã™ã€‚ã©ã¡ã‚‰ã®å ´åˆã‚‚ã€åˆ©ä¾¿æ€§ã®ãŸã‚ã«ã‚ã‚‰ã‹ã˜ã‚è¨­å®šã•ã‚ŒãŸãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã€å¤‰åˆ†é‡å­åˆ†é¡å™¨ (Variational Quantum Classifier, ``VQC``) ã¨å¤‰åˆ†é‡å­å›å¸°å™¨ (Variational Quantum Regressor, ``VQR``) ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã®æ§‹æˆã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ã€‚"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:13
msgid "`Classification <#Classification>`__"
msgstr "`åˆ†é¡ <#Classification>`__"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:15
msgid "Classification with an ``OpflowQNN``"
msgstr "``OpflowQNN`` ã«ã‚ˆã‚‹åˆ†é¡"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:16
#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:249
msgid "Classification with a ``CircuitQNN``"
msgstr "``CircuitQNN`` ã«ã‚ˆã‚‹åˆ†é¡"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:17
#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:398
msgid "Variational Quantum Classifier (``VQC``)"
msgstr "å¤‰åˆ†é‡å­åˆ†é¡å™¨ (``VQC``)"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:19
msgid "`Regression <#Regression>`__"
msgstr "`å›å¸° <#Regression>`__"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:21
#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:539
msgid "Regression with an ``OpflowQNN``"
msgstr "``OpflowQNN`` ã«ã‚ˆã‚‹å›å¸°"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:22
msgid "Variational Quantum Regressor (``VQR``)"
msgstr "å¤‰åˆ†é‡å­å›å¸°å™¨ (``VQR``)"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:70
#: ../../tutorials/03_quantum_kernel.ipynb:53
msgid "Classification"
msgstr "åˆ†é¡"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:72
msgid "We prepare a simple classification dataset to illustrate the following algorithms."
msgstr "ä»¥ä¸‹ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’èª¬æ˜ã™ã‚‹ãŸã‚ã«ã€ç°¡å˜ãªåˆ†é¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨æ„ã—ã¾ã™ã€‚"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:117
msgid "Classification with the an ``OpflowQNN``"
msgstr "``OpflowQNN`` ã«ã‚ˆã‚‹åˆ†é¡"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:119
msgid "First we show how an ``OpflowQNN`` can be used for classification within a ``NeuralNetworkClassifier``. In this context, the ``OpflowQNN`` is expected to return one-dimensional output in :math:`[-1, +1]`. This only works for binary classification and we assign the two classes to :math:`\\{-1, +1\\}`. For convenience, we use the ``TwoLayerQNN``, which is a special type of ``OpflowQNN`` defined via a feature map and an ansatz."
msgstr "ã¾ãšã€ ``OpflowQNN`` ãŒ ``NeuralNetworkClassifier`` ã®ä¸­ã§ã©ã®ã‚ˆã†ã«åˆ†é¡ã«ä½¿ã‚ã‚Œã‚‹ã‹ã‚’ç¤ºã—ã¾ã™ã€‚ã“ã“ã§ã¯ã€ ``OpflowQNN`` ã¯ã€ :math:`[-1, +1]` ã®1æ¬¡å…ƒã®å‡ºåŠ›ã‚’è¿”ã™ã“ã¨ãŒæœŸå¾…ã•ã‚Œã¦ã„ã¾ã™ã€‚ ã“ã‚Œã¯ã€äºŒå€¤åˆ†é¡ã«ã—ã‹ä½¿ãˆãªã„ã®ã§ã€2ã¤ã®ã‚¯ãƒ©ã‚¹ã‚’ :math:`\\{-1, +1\\}` ã«å‰²ã‚Šå½“ã¦ã¾ã™ã€‚ã“ã“ã§ã¯ã€ä¾¿å®œä¸Šã€ç‰¹å¾´ãƒãƒƒãƒ—ã¨ansatzã‚’ç”¨ã„ã¦å®šç¾©ã•ã‚ŒãŸ ``OpflowQNN`` ã®ç‰¹åˆ¥ãªã‚¿ã‚¤ãƒ—ã§ã‚ã‚‹ ``TwoLayerQNN`` ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:251
msgid "Next we show how a ``CircuitQNN`` can be used for classification within a ``NeuralNetworkClassifier``. In this context, the ``CircuitQNN`` is expected to return :math:`d`-dimensional probability vector as output, where :math:`d` denotes the number of classes. Sampling from a ``QuantumCircuit`` automatically results in a probability distribution and we just need to define a mapping from the measured bitstrings to the different classes. For binary classification we use the parity mapping."
msgstr "æ¬¡ã«ã€ ``CircuitQNN`` ãŒ ``NeuralNetworkClassifier`` ã®ä¸­ã§ã©ã®ã‚ˆã†ã«åˆ†é¡ã«ä½¿ã‚ã‚Œã‚‹ã‹ã‚’ç¤ºã—ã¾ã™ã€‚ã“ã“ã§ã¯ã€ ``CircuitQNN`` ã¯ã€ :math:`d` -æ¬¡å…ƒã®ç¢ºç‡ãƒ™ã‚¯ãƒˆãƒ«ã‚’å‡ºåŠ›ã¨ã—ã¦è¿”ã™ã“ã¨ãŒæœŸå¾…ã•ã‚Œã¾ã™ã€‚ã“ã“ã§ :math:`d` ã¯ã‚¯ãƒ©ã‚¹æ•°ã§ã™ã€‚``QuantumCircuit`` ã‹ã‚‰ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã¯ã€è‡ªå‹•çš„ã«ç¢ºç‡åˆ†å¸ƒã«ãªã‚Šã¾ã™ã®ã§ã€æ¸¬å®šã•ã‚ŒãŸãƒ“ãƒƒãƒˆåˆ—ã‹ã‚‰ç•°ãªã‚‹ã‚¯ãƒ©ã‚¹ã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’å®šç¾©ã™ã‚‹ã ã‘ã§ã™ã€‚ãƒã‚¤ãƒŠãƒªãƒ¼åˆ†é¡ã«ã¯ï¼Œãƒ‘ãƒªãƒ†ã‚£ãƒ¼ãƒ»ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½¿ã„ã¾ã™ã€‚"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:400
msgid "The ``VQC`` is a special variant of the ``NeuralNetworkClassifier`` with a ``CircuitQNN``. It applies a parity mapping (or extensions to multiple classes) to map from the bitstring to the classification, which results in a probability vector, which is interpreted as a one-hot encoded result. By default, it applies this the ``CrossEntropyLoss`` function that expects labels given in one-hot encoded format and will return predictions in that format too."
msgstr "``VQC`` ã¯ã€ ``CircuitQNN`` ã‚’ç”¨ã„ãŸ ``NeuralNetworkClassifier`` ã®ç‰¹åˆ¥ãªãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã§ã™ã€‚ã“ã‚Œã¯ã€ãƒ‘ãƒªãƒ†ã‚£ãƒ¼ãƒ»ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆã¾ãŸã¯è¤‡æ•°ã®ã‚¯ãƒ©ã‚¹ã¸ã®æ‹¡å¼µï¼‰ã‚’é©ç”¨ã—ã¦ã€ãƒ“ãƒƒãƒˆåˆ—ã‹ã‚‰åˆ†é¡ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã—ã€ãã®çµæœã€ç¢ºç‡ãƒ™ã‚¯ãƒˆãƒ«ãŒå¾—ã‚‰ã‚Œã€ãƒ¯ãƒ³ã‚·ãƒ§ãƒƒãƒˆã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸçµæœã¨ã—ã¦è§£é‡ˆã•ã‚Œã¾ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€ ``CrossEntropyLoss`` é–¢æ•°ã‚’é©ç”¨ã—ã¾ã™ã€‚ã“ã®é–¢æ•°ã¯ã€ãƒ¯ãƒ³ã‚·ãƒ§ãƒƒãƒˆã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ä¸ãˆã‚‰ã‚ŒãŸãƒ©ãƒ™ãƒ«ã‚’æƒ³å®šã—ã¦ãŠã‚Šã€ãã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ã‚‚äºˆæ¸¬å€¤ã‚’è¿”ã—ã¾ã™ã€‚"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:496
msgid "Regression"
msgstr "å›å¸°"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:498
msgid "We prepare a simple regression dataset to illustrate the following algorithms."
msgstr "ä»¥ä¸‹ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’èª¬æ˜ã™ã‚‹ãŸã‚ã«ã€ç°¡å˜ãªå›å¸°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨æ„ã—ã¾ã™ã€‚"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:541
msgid "Here we restrict to regression with an ``OpflowQNN`` that returns values in :math:`[-1, +1]`. More complex and also multi-dimensional models could be constructed, also based on ``CircuitQNN`` but that exceeds the scope of this tutorial."
msgstr "ã“ã“ã§ã¯ã€ :math:`[-1, +1]` ã®å€¤ã‚’è¿”ã™ ``OpflowQNN`` ã‚’ä½¿ã£ãŸå›å¸°ã«é™å®šã—ã¾ã™ã€‚ã‚‚ã£ã¨è¤‡é›‘ã§å¤šæ¬¡å…ƒã®ãƒ¢ãƒ‡ãƒ«ã‚’ ``CircuitQNN`` ã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ã¦æ§‹ç¯‰ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ãŒã€ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã®ç¯„å›²ã‚’è¶…ãˆã¦ã„ã¾ã™ã€‚"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:648
msgid "Regression with the Variational Quantum Regressor (``VQR``)"
msgstr "å¤‰åˆ†é‡å­å›å¸°å™¨ (``VQR``) ã«ã‚ˆã‚‹å›å¸°"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:650
msgid "Similar to the ``VQC`` for classification, the ``VQR`` is a special variant of the ``NeuralNetworkRegressor`` with a ``OpflowQNN``. By default it considers the ``L2Loss`` function to minimize the mean squared error between predictions and targets."
msgstr "``VQR`` ã¯ã€åˆ†é¡ç”¨ã® ``VQC`` ã¨åŒæ§˜ã«ã€ ``OpflowQNN`` ã‚’ç”¨ã„ãŸ ``NeuralNetworkRegressor`` ã®ç‰¹åˆ¥ãªæ”¹è‰¯ç‰ˆã§ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€äºˆæ¸¬å€¤ã¨ç›®æ¨™å€¤ã®é–“ã®å¹³å‡äºŒä¹—èª¤å·®ã‚’æœ€å°åŒ–ã™ã‚‹ãŸã‚ã«ã€ ``L2Loss`` é–¢æ•°ã‚’è€ƒæ…®ã—ã¾ã™ã€‚"

#: ../../tutorials/03_quantum_kernel.ipynb:9
msgid "Quantum Kernel Machine Learning"
msgstr "é‡å­ã‚«ãƒ¼ãƒãƒ«æ³•æ©Ÿæ¢°å­¦ç¿’"

#: ../../tutorials/03_quantum_kernel.ipynb:11
msgid "The general task of machine learning is to find and study patterns in data. For many datasets, the datapoints are better understood in a higher dimensional feature space, through the use of a kernel function: :math:`k(\\vec{x}_i, \\vec{x}_j) = \\langle f(\\vec{x}_i), f(\\vec{x}_j) \\rangle` where :math:`k` is the kernel function, :math:`\\vec{x}_i, \\vec{x}_j` are :math:`n` dimensional inputs, :math:`f` is a map from :math:`n`-dimension to :math:`m`-dimension space and :math:`\\langle a,b \\rangle` denotes the dot product. When considering finite data, a kernel function can be represented as a matrix: :math:`K_{ij} = k(\\vec{x}_i,\\vec{x}_j)`."
msgstr "æ©Ÿæ¢°å­¦ç¿’ã®ä¸€èˆ¬çš„ã‚¿ã‚¹ã‚¯ã¯ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç™ºè¦‹ã—ãŸã‚Šå­¦ç¿’ã™ã‚‹ã“ã¨ã§ã™ã€‚å¤šãã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã—ã€ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã¯ã‚«ãƒ¼ãƒãƒ«é–¢æ•°: :math:`k(\\vec{x}_i, \\vec{x}_j) = \\langle f(\\vec{x}_i), f(\\vec{x}_j) \\rangle` ã‚’ç”¨ã„ã¦é«˜æ¬¡å…ƒç‰¹å¾´ç©ºé–“ã§ç†è§£ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã“ã“ã§ã€:math:`k` ã¯ã‚«ãƒ¼ãƒãƒ«é–¢æ•°ã€:math:`\\vec{x}_i, \\vec{x}_j`  ã¯ n æ¬¡å…ƒã®ã‚¤ãƒ³ãƒ—ãƒƒãƒˆã€:math:`f` ã¯ :math:`n`-æ¬¡å…ƒç©ºé–“ã‹ã‚‰ :math:`m`-æ¬¡å…ƒç©ºé–“ã¸ã®å†™åƒã€:math:`\\langle a,b \\rangle` ã¯å†…ç©ã‚’è¡¨ã—ã¾ã™ã€‚æœ‰é™ã®ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã€ã‚«ãƒ¼ãƒãƒ«é–¢æ•°ã¯è¡Œåˆ— :math:`K_{ij} = k(\\vec{x}_i,\\vec{x}_j)` ã§è¡¨ç¾ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚"

#: ../../tutorials/03_quantum_kernel.ipynb:14
msgid "In quantum kernel machine learning, a quantum feature map :math:`\\phi(\\vec{x})` is used to map a classical feature vector :math:`\\vec{x}` to a quantum Hilbert space, :math:`| \\phi(\\vec{x})\\rangle \\langle \\phi(\\vec{x})|`, such that :math:`K_{ij} = \\left| \\langle \\phi^\\dagger(\\vec{x}_j)| \\phi(\\vec{x}_i) \\rangle \\right|^{2}`. See `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__ for more details."
msgstr "é‡å­ã‚«ãƒ¼ãƒãƒ«æ³•æ©Ÿæ¢°å­¦ç¿’ã«ãŠã„ã¦ã€é‡å­ç‰¹å¾´ãƒãƒƒãƒ— :math:`\\phi(\\vec{x})` ã¯å¤å…¸ç‰¹å¾´ãƒ™ã‚¯ãƒˆãƒ« :math:`\\vec{x}` ã‚’ :math:`K_{ij} = \\left| \\langle \\phi^\\dagger(\\vec{x}_j)| \\phi(\\vec{x}_i) \\rangle \\right|^{2}` ãªã‚‹é‡å­ãƒ’ãƒ«ãƒ™ãƒ«ãƒˆç©ºé–“ :math:`| \\phi(\\vec{x})\\rangle \\langle \\phi(\\vec{x})|` ã«ãƒãƒƒãƒ—ã™ã‚‹éš›ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚è©³ç´°ã¯ `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__ ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚"

#: ../../tutorials/03_quantum_kernel.ipynb:16
msgid "In this notebook, we use ``qiskit`` to calculate a kernel matrix using a quantum feature map, then use this kernel matrix in ``scikit-learn`` classification and clustering algorithms."
msgstr "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€é‡å­ç‰¹å¾´ãƒãƒƒãƒ—ã‚’ä½¿ã£ã¦ã‚«ãƒ¼ãƒãƒ«è¡Œåˆ—ã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã« ``qiskit`` ã‚’ä½¿ç”¨ã—ã€ ``scikit-learn`` ã®åˆ†é¡ã¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã“ã®ã‚«ãƒ¼ãƒãƒ«è¡Œåˆ—ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚"

#: ../../tutorials/03_quantum_kernel.ipynb:55
msgid "For our classification example, we will use the *ad hoc dataset* as described in `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__, and the ``scikit-learn`` `support vector machine <https://scikit-learn.org/stable/modules/svm.html>`__ classification (``svc``) algorithm."
msgstr "åˆ†é¡ã®ä¾‹ã§ã¯ã€`Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__ ã§èª¬æ˜ã•ã‚Œã¦ã„ã‚‹ *ã‚¢ãƒ‰ãƒ›ãƒƒã‚¯ãƒ»ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ* ã¨ ``scikit-learn`` ã® `ã‚µãƒãƒ¼ãƒˆãƒ™ã‚¯ã‚¿ãƒ¼ãƒã‚·ãƒ³ <https://scikit-learn.org/stable/modules/svm.html>`__ (svc) åˆ†é¡ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚"

#: ../../tutorials/03_quantum_kernel.ipynb:111
msgid "With our training and testing datasets ready, we set up the ``QuantumKernel`` class to calculate a kernel matrix using the `ZZFeatureMap <https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZZFeatureMap.html>`__, and the ``BasicAer`` ``qasm_simulator`` using 1024 shots."
msgstr "å­¦ç¿’ç”¨ã¨ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™ãŒã§ããŸã‚‰ã€`ZZFeatureMap <https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZZFeatureMap.html>`__ ã‚’ä½¿ç”¨ã—ã¦ã‚«ãƒ¼ãƒãƒ«è¡Œåˆ—ã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã® ``QuantumKernel`` ã‚¯ãƒ©ã‚¹ã‚’è¨­å®šã—ã¾ã™ã€‚ãã—ã¦ ``BasicAer`` ``qasm_simulator`` ã§ 1024 å›ã®ã‚·ãƒ§ãƒƒãƒˆã‚’å®Ÿè¡Œã—ã¾ã™ã€‚"

#: ../../tutorials/03_quantum_kernel.ipynb:138
msgid "The ``scikit-learn`` ``svc`` algorithm allows us to define a `custom kernel <https://scikit-learn.org/stable/modules/svm.html#custom-kernels>`__ in two ways: by providing the kernel as a callable function or by precomputing the kernel matrix. We can do either of these using the ``QuantumKernel`` class in ``qiskit``."
msgstr "``scikit-learn`` ã® ``svc`` ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã¯ã€`custom kernel <https://scikit-learn.org/stable/modules/svm.html#custom-kernels>`__ ã‚’æ¬¡ã®2ã¤ã®æ–¹æ³•ã§å®šç¾©ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™: å‘¼ã³å‡ºã—å¯èƒ½ãªé–¢æ•°ã¨ã—ã¦ã‚«ãƒ¼ãƒãƒ«ã‚’æä¾›ã™ã‚‹ã‹ã€ã‚«ãƒ¼ãƒãƒ«è¡Œåˆ—ã‚’äºˆæ¸¬ã™ã‚‹ã“ã¨ã§ã™ã€‚ ``qiskit`` ã® ``QuantumKernel`` ã‚¯ãƒ©ã‚¹ã‚’ä½¿ç”¨ã—ã¦ã€ã©ã¡ã‚‰ã‹ã‚’è¡Œã†ã“ã¨ãŒã§ãã¾ã™ã€‚"

#: ../../tutorials/03_quantum_kernel.ipynb:140
msgid "The following code gives the kernel as a callable function:"
msgstr "æ¬¡ã®ã‚³ãƒ¼ãƒ‰ã¯ã€ã‚«ãƒ¼ãƒãƒ«ã‚’å‘¼ã³å‡ºã—å¯èƒ½ãªé–¢æ•°ã¨ã—ã¦æä¾›ã—ã¾ã™ã€‚"

#: ../../tutorials/03_quantum_kernel.ipynb:184
msgid "The following code precomputes and plots the training and testing kernel matrices before providing them to the ``scikit-learn`` ``svc`` algorithm:"
msgstr "ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã¯ã€``scikit-learn`` ``svc`` ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ãã‚Œã‚‰ã‚’æä¾›ã™ã‚‹å‰ã«ã€ã‚«ãƒ¼ãƒãƒ«è¡Œåˆ—ã®å­¦ç¿’ã¨ãƒ†ã‚¹ãƒˆã‚’äº‹å‰è¨ˆç®—ã—ã¦ãƒ—ãƒ­ãƒƒãƒˆã—ã¾ã™ã€‚"

#: ../../tutorials/03_quantum_kernel.ipynb:250
msgid "``qiskit`` also contains the ``qsvc`` class that extends the ``sklearn svc`` class, that can be used as follows:"
msgstr "``qiskit`` ã«ã¯ã€``sklearn svc`` ã‚¯ãƒ©ã‚¹ã‚’æ‹¡å¼µã—ãŸ ``qsvc`` ã‚¯ãƒ©ã‚¹ã‚‚å«ã¾ã‚Œã¦ãŠã‚Šã€æ¬¡ã®ã‚ˆã†ã«ä½¿ç”¨ã§ãã¾ã™ã€‚"

#: ../../tutorials/03_quantum_kernel.ipynb:295
msgid "Clustering"
msgstr "ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°"

#: ../../tutorials/03_quantum_kernel.ipynb:297
msgid "For our clustering example, we will again use the *ad hoc dataset* as described in `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__, and the ``scikit-learn`` ``spectral`` clustering algorithm."
msgstr "ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã®ä¾‹ã§ã¯ã€`Supervised learning, with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__ ã§èª¬æ˜ã•ã‚Œã¦ã„ã‚‹ *ã‚¢ãƒ‰ãƒ›ãƒƒã‚¯ãƒ»ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ* ã¨ ``scikit-learn`` ã® ``spectral`` ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ»ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å†åº¦ä½¿ç”¨ã—ã¾ã™ã€‚"

#: ../../tutorials/03_quantum_kernel.ipynb:299
msgid "We will regenerate the dataset with a larger gap between the two classes, and as clustering is an unsupervised machine learning task, we donâ€™t need a test sample."
msgstr "2 ã¤ã®ã‚¯ãƒ©ã‚¹é–“ã«å¤§ããªã‚®ãƒ£ãƒƒãƒ—ãŒã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒ»ã‚»ãƒƒãƒˆã‚’å†ç”Ÿæˆã—ã¾ã™ã€‚ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã¯æ•™å¸«ãªã—æ©Ÿæ¢°å­¦ç¿’ã‚¿ã‚¹ã‚¯ã§ã‚ã‚‹ãŸã‚ã€ãƒ†ã‚¹ãƒˆãƒ»ã‚µãƒ³ãƒ—ãƒ«ã¯å¿…è¦ã‚ã‚Šã¾ã›ã‚“ã€‚"

#: ../../tutorials/03_quantum_kernel.ipynb:350
msgid "We again set up the ``QuantumKernel`` class to calculate a kernel matrix using the `ZZFeatureMap <https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZZFeatureMap.html>`__, and the BasicAer ``qasm_simulator`` using 1024 shots."
msgstr "å†åº¦ã€ `ZZFeatureMap <https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZZFeatureMap.html>`__ ã‚’ä½¿ç”¨ã—ã¦ã‚«ãƒ¼ãƒãƒ«è¡Œåˆ—ã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã« ``QuantumKernel`` ã‚¯ãƒ©ã‚¹ã‚’è¨­å®šã—ã€BasicAerã® ``qasm_simulator`` ã‚’1024ã‚·ãƒ§ãƒƒãƒˆã«è¨­å®šã—ã¾ã™ã€‚"

#: ../../tutorials/03_quantum_kernel.ipynb:377
msgid "The scikit-learn spectral clustering algorithm allows us to define a [custom kernel] in two ways: by providing the kernel as a callable function or by precomputing the kernel matrix. Using the QuantumKernel class in qiskit, we can only use the latter."
msgstr "scikit-learnã®ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«ãƒ»ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ»ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã¯ã€[ã‚«ã‚¹ã‚¿ãƒ ãƒ»ã‚«ãƒ¼ãƒãƒ«] ã‚’2ã¤ã®æ–¹æ³•ã§å®šç¾©ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã¤ã¾ã‚Šã€ã‚«ãƒ¼ãƒãƒ«ã‚’å‘¼ã³å‡ºã—å¯èƒ½ãªé–¢æ•°ã¨ã—ã¦æä¾›ã™ã‚‹ã‹ã€ã‚«ãƒ¼ãƒãƒ«è¡Œåˆ—ã‚’äºˆæ¸¬ã™ã‚‹ã“ã¨ã§ã™ã€‚ Qiskitã®QuantumKernelã‚¯ãƒ©ã‚¹ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€å¾Œè€…ã®ã¿ã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚"

#: ../../tutorials/03_quantum_kernel.ipynb:379
msgid "The following code precomputes and plots the kernel matrices before providing it to the scikit-learn spectral clustering algorithm, and scoring the labels using normalized mutual information, since we a priori know the class labels."
msgstr "ã‚¯ãƒ©ã‚¹ãƒ»ãƒ©ãƒ™ãƒ«ã‚’å‰ã‚‚ã£ã¦çŸ¥ã£ã¦ã„ã‚‹ã®ã§ã€ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã¯ã€scikit-learnã®ã‚¹ãƒšã‚¯ãƒˆãƒ©ãƒ«ãƒ»ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ãƒ»ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«æŠ•å…¥ã—æ­£è¦åŒ–ç›¸äº’æƒ…å ±é‡ã§ãƒ©ãƒ™ãƒ«ã‚’ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã™ã‚‹å‰ã«ã€ã‚«ãƒ¼ãƒãƒ«è¡Œåˆ—ã‚’äº‹å‰è¨ˆç®—ã—ã¦ãƒ—ãƒ­ãƒƒãƒˆã—ã¾ã™ã€‚"

#: ../../tutorials/03_quantum_kernel.ipynb:439
msgid "``scikit-learn`` has other algorithms that can use a precomputed kernel matrix, here are a few:"
msgstr "``scikit-learn`` ã«ã¯ã€äº‹å‰è¨ˆç®—ã•ã‚ŒãŸã‚«ãƒ¼ãƒãƒ«è¡Œåˆ—ã‚’ä½¿ç”¨ã§ãã‚‹æ¬¡ã®ã‚ˆã†ãªä»–ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒã‚ã‚Šã¾ã™:"

#: ../../tutorials/03_quantum_kernel.ipynb:441
msgid "`Agglomerative clustering <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html>`__"
msgstr "`Agglomerative clustering <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html>`__"

#: ../../tutorials/03_quantum_kernel.ipynb:442
msgid "`Support vector regression <https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html>`__"
msgstr "`Support vector regression <https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html>`__"

#: ../../tutorials/03_quantum_kernel.ipynb:443
msgid "`Ridge regression <https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html>`__"
msgstr "`Ridge regression <https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html>`__"

#: ../../tutorials/03_quantum_kernel.ipynb:444
msgid "`Gaussian process regression <https://scikit-learn.org/stable/modules/gaussian_process.html>`__"
msgstr "`Gaussian process regression <https://scikit-learn.org/stable/modules/gaussian_process.html>`__"

#: ../../tutorials/03_quantum_kernel.ipynb:445
msgid "`Principal component analysis <https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html>`__"
msgstr "`Principal component analysis <https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html>`__"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:9
msgid "qGANs for Loading Random Distributions"
msgstr "qGANã«ã‚ˆã‚‹ãƒ©ãƒ³ãƒ€ãƒ åˆ†å¸ƒã®æ›¸ãè¾¼ã¿"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:11
msgid "Given :math:`k`-dimensional data samples, we employ a quantum Generative Adversarial Network (qGAN) to learn the dataâ€™s underlying random distribution and to load it directly into a quantum state:"
msgstr ":math:`k` æ¬¡å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«ãŒä¸ãˆã‚‰ã‚ŒãŸå ´åˆã€é‡å­æ•µå¯¾çš„ç”Ÿæˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯(qGAN) ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã®åŸºç¤ã¨ãªã‚‹ãƒ©ãƒ³ãƒ€ãƒ åˆ†å¸ƒã‚’å­¦ç¿’ã—ã€ãã‚Œã‚’é‡å­çŠ¶æ…‹ã«ç›´æ¥ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:13
msgid "\\big| g_{\\theta}\\rangle = \\sum_{j=0}^{2^n-1} \\sqrt{p_{\\theta}^{j}}\\big| j \\rangle\n\n"
msgstr "\\big| g_{\\theta}\\rangle = \\sum_{j=0}^{2^n-1} \\sqrt{p_{\\theta}^{j}}\\big| j \\rangle\n\n"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:15
msgid "where :math:`p_{\\theta}^{j}` describe the occurrence probabilities of the basis states :math:`\\big| j\\rangle`."
msgstr "ã“ã“ã§ã€ :math:`p_{\\theta}^{j}` ã¯ã€åŸºåº•çŠ¶æ…‹ :math:`\\big| j\\rangle` ã®ç™ºç”Ÿç¢ºç‡ã‚’è¡¨ã—ã¾ã™ã€‚"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:17
msgid "The aim of the qGAN training is to generate a state :math:`\\big| g_{\\theta}\\rangle` where :math:`p_{\\theta}^{j}`, for :math:`j\\in \\left\\{0, \\ldots, {2^n-1} \\right\\}`, describe a probability distribution that is close to the distribution underlying the training data :math:`X=\\left\\{x^0, \\ldots, x^{k-1} \\right\\}`."
msgstr "qGANå­¦ç¿’ã®ç›®çš„ã¯ã€çŠ¶æ…‹ :math:`\\big| g_{\\theta}\\rangle` ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã§ã™ã€‚ã“ã“ã§ã€ :math:`p_{\\theta}^{j}` ã¯ã€ :math:`j\\in \\left\\{0, \\ldots, {2^n-1} \\right\\}` ã®å ´åˆã€ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ :math:`X=\\left\\{x^0, \\ldots, x^{k-1} \\right\\}` ã®åŸºç¤ã¨ãªã‚‹åˆ†å¸ƒã«è¿‘ã„ç¢ºç‡åˆ†å¸ƒã‚’è¡¨ã—ã¾ã™ã€‚"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:19
msgid "For further details please refer to `Quantum Generative Adversarial Networks for Learning and Loading Random Distributions <https://arxiv.org/abs/1904.00043>`__ *Zoufal, Lucchi, Woerner* [2019]."
msgstr "è©³ç´°ã«ã¤ã„ã¦ã¯ã€ã€Œ `Quantum Generative Adversarial Networks for Learning and Loading Random Distributions <https://arxiv.org/abs/1904.00043>`__ *Zoufal, Lucchi, Woerner* [2019] ã€ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:21
msgid "For an example of how to use a trained qGAN in an application, the pricing of financial derivatives, please see the `Option Pricing with qGANs <https://github.com/Qiskit/qiskit-finance/tree/main/docs/tutorials/10_qgan_option_pricing.ipynb>`__ tutorial."
msgstr "å­¦ç¿’æ¸ˆã¿ã® qGAN ã®å®Ÿç”¨ä¾‹ã¨ã—ã¦ã€é‡‘èãƒ‡ãƒªãƒãƒ†ã‚£ãƒ–ã®ä¾¡æ ¼è¨­å®šã«ã¤ã„ã¦ã¯ã€ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã®ã€Œ `Option Pricing with qGANs <https://github.com/Qiskit/qiskit-finance/tree/main/docs/tutorials/10_qgan_option_pricing.ipynb>`__ ã€ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:56
msgid "Load the Training Data"
msgstr "å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ãƒ¼ãƒ‰"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:58
msgid "First, we need to load the :math:`k`-dimensional training data samples (here k=1)."
msgstr "ã¾ãš :math:`k` æ¬¡å…ƒã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãƒ»ã‚µãƒ³ãƒ—ãƒ«ã‚’èª­ã¿è¾¼ã‚€å¿…è¦ãŒã‚ã‚Šã¾ã™ (ã“ã“ã§ã¯ k=1 ã§ã™) ã€‚"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:60
msgid "Next, the data resolution is set, i.e.Â the min/max data values and the number of qubits used to represent each data dimension."
msgstr "æ¬¡ã«ãƒ‡ãƒ¼ã‚¿ã®åˆ†è§£èƒ½ã€ã™ãªã‚ã¡æœ€å°ãƒ»æœ€å¤§ãƒ‡ãƒ¼ã‚¿å€¤ã¨ã€å„ãƒ‡ãƒ¼ã‚¿æ¬¡å…ƒã‚’è¡¨ç¾ã™ã‚‹ãŸã‚ã«ç”¨ã„ã‚‰ã‚Œã‚‹é‡å­ãƒ“ãƒƒãƒˆæ•°ã‚’è¨­å®šã—ã¾ã™ã€‚"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:95
msgid "Initialize the qGAN"
msgstr "qGANã®åˆæœŸåŒ–"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:97
msgid "The qGAN consists of a quantum generator :math:`G_{\\theta}`, i.e., an ansatz, and a classical discriminator :math:`D_{\\phi}`, a neural network."
msgstr "qGAN ã¯ã€é‡å­ç”Ÿæˆå™¨ :math:`G_{\\theta}` ã€ã„ã‚ã‚†ã‚‹ansatzã¨ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒ»ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã® å¤å…¸è­˜åˆ¥å™¨ :math:`D_{\\phi}` ã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:99
msgid "To implement the quantum generator, we choose a depth-\\ :math:`1` ansatz that implements :math:`R_Y` rotations and :math:`CZ` gates which takes a uniform distribution as an input state. Notably, for :math:`k>1` the generatorâ€™s parameters must be chosen carefully. For example, the circuit depth should be :math:`>1` because higher circuit depths enable the representation of more complex structures."
msgstr "é‡å­ç”Ÿæˆå™¨ã‚’å®Ÿè£…ã™ã‚‹ãŸã‚ã«ã€ä¸€æ§˜åˆ†å¸ƒã‚’å…¥åŠ›ã«å–ã‚‹ã‚ˆã†ãªã€:math:`R_Y` å›è»¢ã¨ :math:`CZ` ã‚²ãƒ¼ãƒˆã‹ã‚‰ãªã‚‹æ·±ã• :math:`1` ã®ansatzã‚’ç”¨ã„ã¾ã™ã€‚ç‰¹ã« :math:`k>1` ã®å ´åˆã€ç”Ÿæˆå™¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã¯æ…é‡ã«é¸ã°ã‚Œãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“ã€‚ä¾‹ãˆã°ã€å›è·¯ãŒæ·±ã„ã»ã©ã‚ˆã‚Šè¤‡é›‘ãªæ§‹é€ ã‚’è¡¨ç¾ã§ãã‚‹ã®ã§ã€å›è·¯ã®æ·±ã•ã¯ :math:`>1` ã§ã‚ã‚‹ã¹ãã§ã™ã€‚"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:101
msgid "The classical discriminator used here is based on a neural network implementation using NumPy. There is also a discriminator based on PyTorch which is not installed by default when installing Qiskit - see `Optional Install <https://github.com/Qiskit/qiskit-machine-learning#optional-installs>`__ for more information."
msgstr "ã“ã“ã§ä½¿ç”¨ã™ã‚‹å¤å…¸è­˜åˆ¥å™¨ã¯ã€ NumPyã«ã‚ˆã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒ»ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å®Ÿè£…ã«åŸºã¥ã„ã¦ã„ã¾ã™ã€‚ã¾ãŸã€ Qiskit ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ™‚ã«ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„ PyTorch ãƒ™ãƒ¼ã‚¹ã®è­˜åˆ¥å™¨ã‚‚ã‚ã‚Šã¾ã™ã€‚è©³ç´°ã¯ã€Œ `Optional Install <https://github.com/Qiskit/qiskit-machine-learning#optional-installs>`__ ã€ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:103
msgid "Here, both networks are updated with the ADAM optimization algorithm (ADAM is qGAN optimizer default)."
msgstr "ã“ã“ã§ã¯ã€ä¸¡æ–¹ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒ ADAM æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã‚ˆã£ã¦æ›´æ–°ã•ã‚Œã¾ã™ (ADAM ã¯ qGAN ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã§ã™) ã€‚"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:164
msgid "Run the qGAN Training"
msgstr "qGAN ã®å­¦ç¿’"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:166
msgid "During the training the discriminatorâ€™s and the generatorâ€™s parameters are updated alternately w.r.t the following loss functions:"
msgstr "å­¦ç¿’ã§ã¯ã€è­˜åˆ¥å™¨ã¨ç”Ÿæˆå™¨ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ãŒã€ä»¥ä¸‹ã®æå¤±é–¢æ•°ã«åŸºã¥ã„ã¦äº¤äº’ã«æ›´æ–°ã•ã‚Œã¾ã™ã€‚"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:168
msgid "L_G\\left(\\phi, \\theta\\right) = -\\frac{1}{m}\\sum\\limits_{l=1}^{m}\\left[\\log\\left(D_{\\phi}\\left(g^{l}\\right)\\right)\\right]\n\n"
msgstr "L_G\\left(\\phi, \\theta\\right) = -\\frac{1}{m}\\sum\\limits_{l=1}^{m}\\left[\\log\\left(D_{\\phi}\\left(g^{l}\\right)\\right)\\right]\n\n"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:170
msgid "and"
msgstr "ãŠã‚ˆã³"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:172
msgid "L_D\\left(\\phi, \\theta\\right) =\n"
"  \\frac{1}{m}\\sum\\limits_{l=1}^{m}\\left[\\log D_{\\phi}\\left(x^{l}\\right) + \\log\\left(1-D_{\\phi}\\left(g^{l}\\right)\\right)\\right],"
msgstr "L_D\\left(\\phi, \\theta\\right) =\n"
"  \\frac{1}{m}\\sum\\limits_{l=1}^{m}\\left[\\log D_{\\phi}\\left(x^{l}\\right) + \\log\\left(1-D_{\\phi}\\left(g^{l}\\right)\\right)\\right],"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:177
msgid "with :math:`m` denoting the batch size and :math:`g^l` describing the data samples generated by the quantum generator."
msgstr "ã“ã“ã§ :math:`m` ã¯ãƒãƒƒãƒãƒ»ã‚µã‚¤ã‚ºã‚’ã€:math:`g^l` ã¯é‡å­ç”Ÿæˆå™¨ã«ã‚ˆã£ã¦ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒ»ã‚µãƒ³ãƒ—ãƒ«ã‚’è¡¨ã—ã¦ã„ã¾ã™ã€‚"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:179
msgid "Please note that the training, for the purpose of this notebook, has been kept briefer by the selection of a known initial point (``init_params``). Without such prior knowledge be aware training may take some while."
msgstr "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®ç›®çš„ã®ãŸã‚ã«ã€æ—¢çŸ¥ã®åˆæœŸç‚¹ (``init_params``) ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§ã€å­¦ç¿’ã‚’ã‚ˆã‚Šç°¡å˜ã«ã—ã¦ã„ã‚‹ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚ã“ã®ã‚ˆã†ãªäºˆå‚™çŸ¥è­˜ãŒãªã„ã¨ã€å­¦ç¿’ãŒæ™‚é–“ã‚’è¦ã™ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:245
msgid "Training Progress & Outcome"
msgstr "å­¦ç¿’éç¨‹ã¨çµæœ"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:247
msgid "Now, we plot the evolution of the generatorâ€™s and the discriminatorâ€™s loss functions during the training, as well as the progress in the relative entropy between the trained and the target distribution."
msgstr "æ¬¡ã«ã€å­¦ç¿’ã«ãŠã‘ã‚‹ç”Ÿæˆå™¨ã¨è­˜åˆ¥å™¨ã®æå¤±é–¢æ•°ã®æ¨ç§»ã€ãŠã‚ˆã³å­¦ç¿’åˆ†å¸ƒã¨ç›®æ¨™åˆ†å¸ƒã®é–“ã®ç›¸å¯¾ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®æ¨ç§»ã‚’ãƒ—ãƒ­ãƒƒãƒˆã—ã¾ã™ã€‚"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:249
msgid "Finally, we also compare the cumulative distribution function (CDF) of the trained distribution to the CDF of the target distribution."
msgstr "æœ€å¾Œã«ã€è¨“ç·´åˆ†å¸ƒã®ç´¯ç©åˆ†å¸ƒé–¢æ•° (cumulative distribution functionã€CDF) ã¨ç›®æ¨™åˆ†å¸ƒã® CDF ã‚‚æ¯”è¼ƒã—ã¾ã™ã€‚"

#: ../../tutorials/05_torch_connector.ipynb:9
msgid "Torch Connector and Hybrid QNNs"
msgstr "Torch ã‚³ãƒã‚¯ã‚¿ãƒ¼ãŠã‚ˆã³ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ QNN"

#: ../../tutorials/05_torch_connector.ipynb:11
msgid "This tutorial introduces Qiskitâ€™s ``TorchConnector`` class, and demonstrates how the ``TorchConnector`` allows for a natural integration of any ``NeuralNetwork`` from Qiskit Machine Learning into a PyTorch workflow. ``TorchConnector`` takes a Qiskit ``NeuralNetwork`` and makes it available as a PyTorch ``Module``. The resulting module can be seamlessly incorporated into PyTorch classical architectures and trained jointly without additional considerations, enabling the development and testing of novel **hybrid quantum-classical** machine learning architectures."
msgstr "ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ã€Qiskitã® ``TorchConnector`` ã‚¯ãƒ©ã‚¹ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚ãã—ã¦ã€ ``TorchConnector`` ãŒ Qiskit æ©Ÿæ¢°å­¦ç¿’ã‹ã‚‰ PyTorch ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã« ``NeuralNetwork`` ã‚’è‡ªç„¶ã«çµ±åˆã™ã‚‹æ–¹æ³•ã‚’ç¤ºã—ã¾ã™ã€‚ ``TorchConnector`` ã¯ Qiskitã® ``NeuralNetwork`` ã‚’å—ã‘å–ã‚Šã€PyTorchã® ``Module`` ã¨ã—ã¦åˆ©ç”¨ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚ å¾—ã‚‰ã‚ŒãŸãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã€PyTorchã®å¤å…¸ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãƒ¼ã«ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ã«çµ„ã¿è¾¼ã‚€ã“ã¨ãŒã§ãã€è¿½åŠ ã®è€ƒæ…®äº‹é …ãªã—ã«ä¸€ç·’ã«å­¦ç¿’ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ ã¾ãŸã€æ–°ã—ã„ **ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰é‡å­å¤å…¸** æ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãƒ¼ã®é–‹ç™ºã¨ãƒ†ã‚¹ãƒˆã‚’å¯èƒ½ã«ã—ã¾ã™ã€‚"

#: ../../tutorials/05_torch_connector.ipynb:15
msgid "Content:"
msgstr "ç›®æ¬¡:"

#: ../../tutorials/05_torch_connector.ipynb:17
msgid "`Part 1: Simple Classification & Regression <#Part-1:-Simple-Classification-&-Regression>`__"
msgstr "`ãƒ‘ãƒ¼ãƒˆ 1: ç°¡å˜ãªåˆ†é¡ã¨å›å¸° <#Part-1:-Simple-Classification-&-Regression>`__"

#: ../../tutorials/05_torch_connector.ipynb:19
msgid "The first part of this tutorial shows how quantum neural networks can be trained using PyTorchâ€™s automatic differentiation engine (``torch.autograd``, `link <https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html>`__) for simple classification and regression tasks."
msgstr "ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã®æœ€åˆã®éƒ¨åˆ†ã¯ã€PyTorchã®è‡ªå‹•å¾®åˆ†ã‚¨ãƒ³ã‚¸ãƒ³( ``torch.autograd``ã€ `ãƒªãƒ³ã‚¯ <https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html>`__) ã‚’ä½¿ç”¨ã—ã¦ç°¡å˜ãªåˆ†é¡ã¨å›å¸°ã‚¿ã‚¹ã‚¯ã®ãŸã‚ã®é‡å­ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’å­¦ç¿’ã•ã›ã‚‹æ–¹æ³•ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚"

#: ../../tutorials/05_torch_connector.ipynb:21
msgid "`Classification <#1.-Classification>`__"
msgstr "`åˆ†é¡ <#1.-Classification>`__"

#: ../../tutorials/05_torch_connector.ipynb:23
msgid "Classification with PyTorch and ``OpflowQNN``"
msgstr "PyTorch ã¨ ``OpflowQNN`` ã‚’ç”¨ã„ãŸåˆ†é¡"

#: ../../tutorials/05_torch_connector.ipynb:24
msgid "Classification with PyTorch and ``CircuitQNN``"
msgstr "PyTorch ã¨ ``CircuitQNN`` ã‚’ç”¨ã„ãŸåˆ†é¡"

#: ../../tutorials/05_torch_connector.ipynb:26
msgid "`Regression <#2.-Regression>`__"
msgstr "`å›å¸° <#2.-Regression>`__"

#: ../../tutorials/05_torch_connector.ipynb:28
msgid "Regression with PyTorch and ``OpflowQNN``"
msgstr "PyTorchã¨ ``OpflowQNN`` ã«ã‚ˆã‚‹å›å¸°"

#: ../../tutorials/05_torch_connector.ipynb:30
msgid "`Part 2: MNIST Classification, Hybrid QNNs <#Part-2:-MNIST-Classification,-Hybrid-QNNs>`__"
msgstr "`ãƒ‘ãƒ¼ãƒˆ 2: MNIST åˆ†é¡ã€ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰QNN <#Part-2:-MNIST-Classification,-Hybrid-QNNs>`__"

#: ../../tutorials/05_torch_connector.ipynb:32
msgid "The second part of this tutorial illustrates how to embed a (Quantum) ``NeuralNetwork`` into a target PyTorch workflow (in this case, a typical CNN architecture) to classify MNIST data in a hybrid quantum-classical manner."
msgstr "ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã® 2 ç•ªç›®ã®éƒ¨åˆ†ã§ã¯ã€ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã®é‡å­å¤å…¸çš„ãªæ–¹æ³•ã§ MNIST ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†é¡ã™ã‚‹ãŸã‚ã€ (é‡å­) ``NeuralNetwork`` ã‚’ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã® PyTorch ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ ( ã“ã®å ´åˆã¯ã€å…¸å‹çš„ãª CNN ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãƒ¼) ã«çµ„ã¿è¾¼ã‚€æ–¹æ³•ã‚’èª¬æ˜ã—ã¦ã„ã¾ã™ã€‚"

#: ../../tutorials/05_torch_connector.ipynb:74
msgid "Part 1: Simple Classification & Regression"
msgstr "ãƒ‘ãƒ¼ãƒˆ 1: ç°¡å˜ãªåˆ†é¡ã¨å›å¸°"

#: ../../tutorials/05_torch_connector.ipynb:86
msgid "1. Classification"
msgstr "1. åˆ†é¡"

#: ../../tutorials/05_torch_connector.ipynb:88
msgid "First, we show how ``TorchConnector`` allows to train a Quantum ``NeuralNetwork`` to solve a classification tasks using PyTorchâ€™s automatic differentiation engine. In order to illustrate this, we will perform **binary classification** on a randomly generated dataset."
msgstr "æœ€åˆã«ã€``TorchConnector`` ãŒ PyTorch ã®è‡ªå‹•å¾®åˆ†ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½¿ç”¨ã—ã¦ã€åˆ†é¡ã‚¿ã‚¹ã‚¯ã‚’è§£æ±ºã™ã‚‹ãŸã‚ã« é‡å­ ``NeuralNetwork`` ã‚’å­¦ç¿’ã•ã›ã‚‹æ–¹æ³•ã‚’ç¤ºã—ã¾ã™ã€‚ ã“ã‚Œã‚’ç¤ºã™ãŸã‚ã«ã€ãƒ©ãƒ³ãƒ€ãƒ ã«ç”Ÿæˆã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã— **ãƒã‚¤ãƒŠãƒªåˆ†é¡** ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚"

#: ../../tutorials/05_torch_connector.ipynb:144
msgid "A. Classification with PyTorch and ``OpflowQNN``"
msgstr "A. PyTorch ã¨ ``OpflowQNN`` ã‚’ç”¨ã„ãŸåˆ†é¡"

#: ../../tutorials/05_torch_connector.ipynb:146
msgid "Linking an ``OpflowQNN`` to PyTorch is relatively straightforward. Here we illustrate this using the ``TwoLayerQNN``, a sub-case of ``OpflowQNN`` introduced in previous tutorials."
msgstr "``OpflowQNN`` ã‚’ PyTorch ã«ãƒªãƒ³ã‚¯ã™ã‚‹ã®ã¯æ¯”è¼ƒçš„ç°¡å˜ã§ã™ã€‚ã“ã“ã§ã¯ã€å‰ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ç´¹ä»‹ã—ãŸ ``OpflowQNN`` ã®ã‚µãƒ–ã‚±ãƒ¼ã‚¹ã§ã‚ã‚‹ ``TwoLayerQNN`` ã‚’ä½¿ç”¨ã—ã¦èª¬æ˜ã—ã¾ã™ã€‚"

#: ../../tutorials/05_torch_connector.ipynb:254
msgid "Optimizer"
msgstr "ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼"

#: ../../tutorials/05_torch_connector.ipynb:256
msgid "The choice of optimizer for training any machine learning model can be crucial in determining the success of our trainingâ€™s outcome. When using ``TorchConnector``, we get access to all of the optimizer algorithms defined in the [``torch.optim``] package (`link <https://pytorch.org/docs/stable/optim.html>`__). Some of the most famous algorithms used in popular machine learning architectures include *Adam*, *SGD*, or *Adagrad*. However, for this tutorial we will be using the L-BFGS algorithm (``torch.optim.LBFGS``), one of the most well know second-order optimization algorithms for numerical optimization."
msgstr "ã‚ã‚‰ã‚†ã‚‹æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã•ã›ã‚‹ä¸Šã§ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã®é¸æŠã¯ã€å­¦ç¿’ã®æˆæœã‚’æ±ºå®šã™ã‚‹ä¸Šã§éå¸¸ã«é‡è¦ã§ã™ã€‚ ``TorchConnector`` ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€[``torch.optim``] ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ (`ãƒªãƒ³ã‚¯ <https://pytorch.org/docs/stable/optim.html>`__) ã§å®šç¾©ã•ã‚Œã¦ã„ã‚‹ã™ã¹ã¦ã®ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ãƒ»ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚ ä¸€èˆ¬çš„ãªæ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãƒ¼ã§ä½¿ç”¨ã•ã‚Œã‚‹æœ€ã‚‚æœ‰åãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã¯ã€*Adam*ã€*SGD*ã€ã¾ãŸã¯ *Adagrad* ãŒã‚ã‚Šã¾ã™ã€‚ ã—ã‹ã—ã€ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯L-BFGSã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ (``torch.optim.LBFGS``)ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ æ•°å€¤æœ€é©åŒ–ã®ãŸã‚ã®æœ€ã‚‚ã‚ˆãçŸ¥ã‚‰ã‚Œã¦ã„ã‚‹2æ¬¡æœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®1ã¤ã§ã™ã€‚"

#: ../../tutorials/05_torch_connector.ipynb:260
msgid "Loss Function"
msgstr "æå¤±é–¢æ•°"

#: ../../tutorials/05_torch_connector.ipynb:262
msgid "As for the loss function, we can also take advantage of PyTorchâ€™s pre-defined modules from ``torch.nn``, such as the `Cross-Entropy <https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html>`__ or `Mean Squared Error <https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html>`__ losses."
msgstr "æå¤±é–¢æ•°ã«ã¤ã„ã¦ã¯ã€`äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ <https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html>`__ ã‚„ `å¹³å‡äºŒä¹—èª¤å·® <https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html>`__ æå¤±ã¨ã„ã£ãŸã€ ``torch.nn`` ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‹ã‚‰PyTorchã®äº‹å‰å®šç¾©ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚"

#: ../../tutorials/05_torch_connector.ipynb:264
msgid "**ğŸ’¡ Clarification :** In classical machine learning, the general rule of thumb is to apply a Cross-Entropy loss to classification tasks, and MSE loss to regression tasks. However, this recommendation is given under the assumption that the output of the classification network is a class probability value in the [0,1] range (usually this is achieved through a Softmax layer). Because the following example for ``TwoLayerQNN`` does not include such layer, and we donâ€™t apply any mapping to the output (the following section shows an example of application of parity mapping with ``CircuitQNNs``), the QNNâ€™s output can take any value in the range [-1,1]. In case you were wondering, this is the reason why this particular example uses MSELoss for classification despite it not being the norm (but we encourage you to experiment with different loss functions and see how they can impact training results)."
msgstr "**ğŸ’¡ è§£èª¬ :** å¤å…¸æ©Ÿæ¢°å­¦ç¿’ã«ãŠã„ã¦ä¸€èˆ¬çš„ãªçµŒé¨“å‰‡ã¯ã€åˆ†é¡ã‚¿ã‚¹ã‚¯ã«äº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±ã‚’é©ç”¨ã—ã€å›å¸°ã‚¿ã‚¹ã‚¯ã«MSEæå¤±ã‚’é©ç”¨ã™ã‚‹ã“ã¨ã§ã™ã€‚ ã—ã‹ã—ã€ã“ã®æ¨å¥¨ã¯ã€åˆ†é¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å‡ºåŠ›ãŒ [0 , 1] ç¯„å›² (é€šå¸¸ã¯ Softmax ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’ä»‹ã—ã¦é”æˆ) ã®åˆ†é¡ç¢ºç‡å€¤ã§ã‚ã‚‹ã“ã¨ã‚’å‰æã¨ã—ã¦ã„ã¾ã™ã€‚ ``TwoLayerQNN`` ã®ä»¥ä¸‹ã®ä¾‹ã«ã¯ãã®ã‚ˆã†ãªãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒå«ã¾ã‚Œãªã„ãŸã‚ã€ã¾ãŸã€å‡ºåŠ›ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’é©ç”¨ã™ã‚‹ã“ã¨ã‚‚ãªã„ (æ¬¡ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ ``CircuitQNNs`` ã‚’ä½¿ç”¨ã—ãŸãƒ‘ãƒªãƒ†ã‚£ãƒ»ãƒãƒƒãƒ”ãƒ³ã‚°ã®ä¾‹ã‚’ç¤ºã—ã¾ã™) ãŸã‚ã€QNNã®å‡ºåŠ›ã¯ã€[-1,1] ã®ç¯„å›²ã§ä»»æ„ã®å€¤ã‚’å–ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ å› ã¿ã«ã€ã“ã‚ŒãŒã€ã“ã®ç‰¹å®šã®ä¾‹ã§MSELossã‚’ä¸€èˆ¬çš„ã§ãªã„ã®ã«ã‚‚é–¢ã‚ã‚‰ãšåˆ†é¡ã«ä½¿ç”¨ã—ã¦ã„ã‚‹ç†ç”±ã§ã™(ãŸã ã—ã€ã•ã¾ã–ã¾ãªæå¤±é–¢æ•°ã‚’è©¦ã—ã¦ã€å­¦ç¿’çµæœã«ã©ã®ã‚ˆã†ãªå½±éŸ¿ã‚’ä¸ãˆã‚‹ã‹ã‚’ç¢ºèªã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™)ã€‚"

#: ../../tutorials/05_torch_connector.ipynb:442
#: ../../tutorials/05_torch_connector.ipynb:674
msgid "The red circles indicate wrongly classified data points."
msgstr "èµ¤ã„ä¸¸ã¯ã€èª¤ã£ã¦åˆ†é¡ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‚’ç¤ºã—ã¾ã™ã€‚"

#: ../../tutorials/05_torch_connector.ipynb:454
msgid "B. Classification with PyTorch and ``CircuitQNN``"
msgstr "B. PyTorch ã¨ ``CircuitQNN`` ã‚’ç”¨ã„ãŸåˆ†é¡"

#: ../../tutorials/05_torch_connector.ipynb:456
msgid "Linking an ``CircuitQNN`` to PyTorch requires a bit more attention than ``OpflowQNN``. Without the correct setup, backpropagation is not possible."
msgstr "``CircuitQNN`` ã‚’ PyTorch ã«ãƒªãƒ³ã‚¯ã™ã‚‹ã«ã¯ã€``OpflowQNN`` ã‚ˆã‚Šã‚‚å°‘ã—æ³¨æ„ãŒå¿…è¦ã§ã™ã€‚æ­£ã—ã„è¨­å®šãŒãªã‘ã‚Œã°ã€ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã¯ã§ãã¾ã›ã‚“ã€‚"

#: ../../tutorials/05_torch_connector.ipynb:458
msgid "In particular, we must make sure that we are returning a dense array of probabilities in the networkâ€™s forward pass (``sparse=False``). This parameter is set up to ``False`` by default, so we just have to make sure that it has not been changed."
msgstr "ç‰¹ã«ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹(``sparse=False``) ã«ç¢ºç‡ã®å¯†ãªé…åˆ—ã‚’è¿”ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ ã“ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ ``False`` ã«è¨­å®šã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€å¤‰æ›´ã•ã‚Œã¦ã„ãªã„ã“ã¨ã‚’ç¢ºèªã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚"

#: ../../tutorials/05_torch_connector.ipynb:460
msgid "**âš ï¸ Attention:** If we define a custom interpret function ( in the example: ``parity``), we must remember to explicitly provide the desired output shape ( in the example: ``2``). For more info on the initial parameter setup for ``CircuitQNN``, please check out the `official qiskit documentation <https://qiskit.org/documentation/machine-learning/stubs/qiskit_machine_learning.neural_networks.CircuitQNN.html>`__."
msgstr "**âš ï¸ æ³¨æ„:** ã‚«ã‚¹ã‚¿ãƒ ã®interpreté–¢æ•°ã‚’å®šç¾©ã—ãŸå ´åˆ (ä¾‹: ``parity``) ã€æœŸå¾…ã™ã‚‹å‡ºåŠ›ã®å½¢çŠ¶ (ä¾‹: ``2``) ã‚’æ˜ç¤ºçš„ã«æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚``CircuitQNN`` ã®åˆæœŸè¨­å®šã«é–¢ã™ã‚‹è©³ç´°ã¯ã€`å…¬å¼ã®Qiskitãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ <https://qiskit.org/documentation/machine-learning/stubs/qiskit_machine_learning.neural_networks.CircuitQNN.html>`__ ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚"

#: ../../tutorials/05_torch_connector.ipynb:523
#: ../../tutorials/05_torch_connector.ipynb:815
msgid "For a reminder on optimizer and loss function choices, you can go back to `this section <#Optimizer>`__."
msgstr "ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã¨æå¤±é–¢æ•°ã®é¸æŠã«ã¤ã„ã¦æ€ã„å‡ºã™ã«ã¯ã€`ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ <#Optimizer>`__ ã«æˆ»ã£ã¦ãã ã•ã„ã€‚"

#: ../../tutorials/05_torch_connector.ipynb:686
msgid "2. Regression"
msgstr "2. å›å¸°"

#: ../../tutorials/05_torch_connector.ipynb:688
msgid "We use a model based on the ``TwoLayerQNN`` to also illustrate how to perform a regression task. The chosen dataset in this case is randomly generated following a sine wave."
msgstr "``TwoLayerQNN`` ã«åŸºã¥ã„ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã€å›å¸°ã‚¿ã‚¹ã‚¯ã®å®Ÿè¡Œæ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚ ä»Šå›é¸æŠã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€æ­£å¼¦æ³¢ã«æ²¿ã£ã¦ãƒ©ãƒ³ãƒ€ãƒ ã«ç”Ÿæˆã•ã‚ŒãŸã‚‚ã®ã§ã™ã€‚"

#: ../../tutorials/05_torch_connector.ipynb:730
msgid "A. Regression with PyTorch and ``OpflowQNN``"
msgstr "A. PyTorchã¨ ``OpflowQNN`` ã«ã‚ˆã‚‹å›å¸°"

#: ../../tutorials/05_torch_connector.ipynb:741
msgid "The network definition and training loop will be analogous to those of the classification task using ``TwoLayerQNN``. In this case, we define our own feature map and ansatz, instead of using the default values."
msgstr "ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å®šç¾©ã¨å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã¯ã€``TwoLayerQNN`` ã‚’ä½¿ç”¨ã—ãŸåˆ†é¡ã‚¿ã‚¹ã‚¯ã®ã‚‚ã®ã¨é¡ä¼¼ã—ã¦ã„ã¾ã™ã€‚ ã“ã®å ´åˆã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä½¿ç”¨ã™ã‚‹ã®ã§ã¯ãªãã€ç‹¬è‡ªã®ç‰¹å¾´ãƒãƒƒãƒ—ã¨ansatzã‚’å®šç¾©ã—ã¾ã™ã€‚"

#: ../../tutorials/05_torch_connector.ipynb:963
msgid "Part 2: MNIST Classification, Hybrid QNNs"
msgstr "ãƒ‘ãƒ¼ãƒˆ 2: MNIST åˆ†é¡ã€ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰QNN"

#: ../../tutorials/05_torch_connector.ipynb:965
msgid "In this second part, we show how to leverage a hybrid quantum-classical neural network using ``TorchConnector``, to perform a more complex image classification task on the MNIST handwritten digits dataset."
msgstr "2ç•ªç›®ã®éƒ¨åˆ†ã§ã¯ã€``TorchConnector`` ã‚’ä½¿ç”¨ã—ãŸãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã®é‡å­å¤å…¸çš„ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ´»ç”¨æ–¹æ³•ã‚’ç¤ºã—ã¾ã™ã€‚ ã‚ˆã‚Šè¤‡é›‘ãªç”»åƒåˆ†é¡ã‚¿ã‚¹ã‚¯ã‚’MNISTã®æ‰‹æ›¸ãã®æ•°å­—ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å®Ÿè¡Œã—ã¾ã™ã€‚"

#: ../../tutorials/05_torch_connector.ipynb:967
msgid "For a more detailed (pre-``TorchConnector``) explanation on hybrid quantum-classical neural networks, you can check out the corresponding section in the `Qiskit Textbook <https://qiskit.org/textbook/ch-machine-learning/machine-learning-qiskit-pytorch.html>`__."
msgstr "ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã®é‡å­å¤å…¸ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®è©³ç´°(``TorchConnector`` ã®å‰)ã«ã¤ã„ã¦ã¯ã€`Qiskit Textbook <https://qiskit.org/textbook/ch-machine-learning/machine-learning-qiskit-pytorch.html>`__ ã®å¯¾å¿œã™ã‚‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚"

#: ../../tutorials/05_torch_connector.ipynb:996
msgid "Step 1: Defining Data-loaders for train and test"
msgstr "ã‚¹ãƒ†ãƒƒãƒ— 1: å­¦ç¿’ã¨ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ‡ãƒ¼ã‚¿ãƒ»ãƒ­ãƒ¼ãƒ€ãƒ¼ã®å®šç¾©"

#: ../../tutorials/05_torch_connector.ipynb:1007
msgid "We take advantage of the ``torchvision`` `API <https://pytorch.org/vision/stable/datasets.html>`__ to directly load a subset of the `MNIST dataset <https://en.wikipedia.org/wiki/MNIST_database>`__ and define torch ``DataLoader``\\ s (`link <https://pytorch.org/docs/stable/data.html>`__) for train and test."
msgstr "``torchvision`` `API <https://pytorch.org/vision/stable/datasets.html>`__ ã‚’åˆ©ç”¨ã—ã¦ã€ `MNIST ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ <https://en.wikipedia.org/wiki/MNIST_database>`__ ã®ã‚µãƒ–ã‚»ãƒƒãƒˆã‚’ç›´æ¥ãƒ­ãƒ¼ãƒ‰ã—ã€å­¦ç¿’ã¨ãƒ†ã‚¹ãƒˆã®ãŸã‚ã® ``DataLoader`` (`ãƒªãƒ³ã‚¯ <https://pytorch.org/docs/stable/data.html>`__) ã‚’å®šç¾©ã—ã¾ã™ã€‚"

#: ../../tutorials/05_torch_connector.ipynb:1048
msgid "If we perform a quick visualization we can see that the train dataset consists of images of handwritten 0s and 1s."
msgstr "ç°¡å˜ãªå¯è¦–åŒ–ã‚’å®Ÿè¡Œã™ã‚‹ã¨ã€å­¦ç¿’ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯æ‰‹æ›¸ãã®0ã¨1ã®ç”»åƒã§æ§‹æˆã•ã‚Œã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚"

#: ../../tutorials/05_torch_connector.ipynb:1120
msgid "Step 2: Defining the QNN and Hybrid Model"
msgstr "ã‚¹ãƒ†ãƒƒãƒ— 2: QNNã¨ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ»ãƒ¢ãƒ‡ãƒ«ã®å®šç¾©"

#: ../../tutorials/05_torch_connector.ipynb:1131
msgid "This second step shows the power of the ``TorchConnector``. After defining our quantum neural network layer (in this case, a ``TwoLayerQNN``), we can embed it into a layer in our torch ``Module`` by initializing a torch connector as ``TorchConnector(qnn)``."
msgstr "ã“ã®2ç•ªç›®ã®ã‚¹ãƒ†ãƒƒãƒ—ã¯ã€ ``TorchConnector`` ã®å®ŸåŠ›ã‚’ç¤ºã—ã¾ã™ã€‚ é‡å­ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å±¤ã‚’å®šç¾©ã—ãŸå¾Œ (ã“ã®å ´åˆã¯ ``TwoLayerQNN``) ã€torchã‚³ãƒã‚¯ã‚¿ãƒ¼ã‚’ ``TorchConnector(qnn)`` ã¨ã—ã¦åˆæœŸåŒ–ã™ã‚‹ã“ã¨ã§ã€torch ``Module`` ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã«åŸ‹ã‚è¾¼ã‚€ã“ã¨ãŒã§ãã¾ã™ã€‚"

#: ../../tutorials/05_torch_connector.ipynb:1133
msgid "**âš ï¸ Attention:** In order to have an adequate gradient backpropagation in hybrid models, we MUST set the initial parameter ``input_gradients`` to TRUE during the qnn initialization."
msgstr "**âš ï¸ æ³¨æ„:** ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ»ãƒ¢ãƒ‡ãƒ«ã§ã€é©åˆ‡ãªå‹¾é…ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã†ãŸã‚ã«ã¯ã€QNNã®åˆæœŸåŒ–ä¸­ã«ã€åˆæœŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ ``input_gradients`` ã‚’ TRUE ã«è¨­å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚"

#: ../../tutorials/05_torch_connector.ipynb:1235
msgid "Step 3: Training"
msgstr "ã‚¹ãƒ†ãƒƒãƒ— 3: å­¦ç¿’"

#: ../../tutorials/05_torch_connector.ipynb:1337
msgid "Step 4: Evaluation"
msgstr "ã‚¹ãƒ†ãƒƒãƒ— 4: è©•ä¾¡"

#: ../../tutorials/05_torch_connector.ipynb:1440
msgid "ğŸ‰ğŸ‰ğŸ‰ğŸ‰ **You are now able to experiment with your own hybrid datasets and architectures using Qiskit Machine Learning.** **Good Luck!**"
msgstr "ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ **ã“ã‚Œã§ã€Qiskit æ©Ÿæ¢°å­¦ç¿’ã‚’ä½¿ç”¨ã—ã¦ã€ç‹¬è‡ªã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ»ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’è©¦ã™ã“ã¨ãŒã§ãã¾ã™ã€‚** **é ‘å¼µã£ã¦ãã ã•ã„!**"

#: ../../tutorials/index.rst:3
msgid "Machine Learning Tutorials"
msgstr "æ©Ÿæ¢°å­¦ç¿’ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«"

