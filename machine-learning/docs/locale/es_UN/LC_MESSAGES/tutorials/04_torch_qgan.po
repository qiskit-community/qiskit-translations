msgid ""
msgstr ""
"Project-Id-Version: qiskit-docs\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-03-28 08:39+0000\n"
"PO-Revision-Date: 2023-04-03 23:40\n"
"Last-Translator: \n"
"Language: es_UN\n"
"Language-Team: Spanish (United)\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"
"X-Crowdin-Project: qiskit-docs\n"
"X-Crowdin-Project-ID: 369271\n"
"X-Crowdin-Language: es-un\n"
"X-Crowdin-File: /master/machine-learning/docs/locale/en/LC_MESSAGES/tutorials/04_torch_qgan.po\n"
"X-Crowdin-File-ID: 9885\n"

#: ../../tutorials/04_torch_qgan.ipynb:9
msgid "This page was generated from `docs/tutorials/04_torch_qgan.ipynb`__."
msgstr "Esta página fue generada a partir de `docs/tutorials/04_torch_qgan.ipynb`__."

#: ../../tutorials/04_torch_qgan.ipynb:9
msgid "PyTorch qGAN Implementation"
msgstr "Implementación de PyTorch qGAN"

#: ../../tutorials/04_torch_qgan.ipynb:12
msgid "Overview"
msgstr "Descripción General"

#: ../../tutorials/04_torch_qgan.ipynb:14
msgid "This tutorial introduces step-by-step how to build a PyTorch-based Quantum Generative Adversarial Network algorithm."
msgstr "Este tutorial presenta paso a paso cómo construir un algoritmo de Red de Adversarios Generativos Cuántica basado en PyTorch."

#: ../../tutorials/04_torch_qgan.ipynb:16
msgid "The tutorial is structured as follows:"
msgstr "El tutorial está estructurado de la siguiente manera:"

#: ../../tutorials/04_torch_qgan.ipynb:18
msgid "`Introduction <#1.-Introduction>`__"
msgstr "`Introducción <#1.-Introducción>`__"

#: ../../tutorials/04_torch_qgan.ipynb:19
msgid "`Data and Represtation <#2.-Data-and-Representation>`__"
msgstr "`Datos y Representación <#2.-Datos-y-Representación>`__"

#: ../../tutorials/04_torch_qgan.ipynb:20
msgid "`Definitions of the Neural Networks <#3.-Definitions-of-the-Neural-Networks>`__"
msgstr "`Definiciones de las Redes Neuronales <#3.-Definiciones-de-las-Redes-Neurales>`__"

#: ../../tutorials/04_torch_qgan.ipynb:21
msgid "`Setting up Parameters for the Training Loop <#4.-Setting-up-Parameters-for-the-Training-Loop>`__"
msgstr "`Configuración de Parámetros para el Ciclo de Entrenamiento <#4.-Configuración-de-Parámetros-para-el-Ciclo-de-Entrenamiento>`__"

#: ../../tutorials/04_torch_qgan.ipynb:22
msgid "`Model Training <#5.-Model-Training>`__"
msgstr "`Entrenamiento del Modelo <#5.-Entrenamiento-del-Modelo>`__"

#: ../../tutorials/04_torch_qgan.ipynb:23
msgid "`Results: Cumulative Density Functions <#6.-Results:-Cumulative-Density-Functions>`__"
msgstr "`Resultados: Funciones de Densidad Acumulativa <#6.-Resultados:-Funciones-de-Densidad-Acumulativa>`__"

#: ../../tutorials/04_torch_qgan.ipynb:24
msgid "`Conclusion <#7.-Conclusion>`__"
msgstr "`Conclusión <#7.-Conclusión>`__"

#: ../../tutorials/04_torch_qgan.ipynb:27
msgid "1. Introduction"
msgstr "1. Introducción"

#: ../../tutorials/04_torch_qgan.ipynb:29
msgid "The qGAN [1] is a hybrid quantum-classical algorithm used for generative modeling tasks. The algorithm uses the interplay of a quantum generator :math:`G_{\\theta}`, i.e., an ansatz (parametrized quantum circuit), and a classical discriminator :math:`D_{\\phi}`, a neural network, to learn the underlying probability distribution given training data."
msgstr "El qGAN [1] es un algoritmo híbrido cuántico-clásico utilizado para tareas de modelado generativo. El algoritmo utiliza la interacción de un generador cuántico :math:`G_{\\theta}`, es decir, un ansatz (circuito cuántico parametrizado), y un discriminador clásico :math:`D_{\\phi}`, una red neuronal, para aprender la distribución de probabilidad subyacente dados los datos de entrenamiento."

#: ../../tutorials/04_torch_qgan.ipynb:31
msgid "The generator and discriminator are trained in alternating optimization steps, where the generator aims at generating probabilities that will be classified by the discriminator as training data values (i.e, probabilities from the real training distribution), and the discriminator tries to differentiate between original distribution and probabilities from the generator (in other words, telling apart the real and generated distributions). The final goal is for the quantum generator to learn a representation for the target probability distribution. The trained quantum generator can, thus, be used to load a quantum state which is an approximate model of the target distribution."
msgstr "El generador y el discriminador se entrenan en pasos de optimización alternantes, donde el generador apunta a generar probabilidades que serán clasificadas por el discriminador como valores de datos de entrenamiento (es decir, probabilidades de la distribución de entrenamiento real), y el discriminador trata de diferenciar entre distribución original y probabilidades del generador (en otras palabras, distinguir las distribuciones real y generada). El objetivo final es que el generador cuántico aprenda una representación para la distribución de probabilidad objetivo. El generador cuántico entrenado puede, por lo tanto, usarse para cargar un estado cuántico que es un modelo aproximado de la distribución objetivo."

#: ../../tutorials/04_torch_qgan.ipynb:34
msgid "**References:**"
msgstr "**Referencias:**"

#: ../../tutorials/04_torch_qgan.ipynb:36
msgid "[1] Zoufal et al., `Quantum Generative Adversarial Networks for learning and loading random distributions <https://www.nature.com/articles/s41534-019-0223-2>`__"
msgstr "[1] Zoufal et al., `Quantum Generative Adversarial Networks for learning and loading random distributions <https://www.nature.com/articles/s41534-019-0223-2>`__"

#: ../../tutorials/04_torch_qgan.ipynb:39
msgid "1.1. qGANs for Loading Random Distributions"
msgstr "1.1. qGANs para Cargar Distribuciones Aleatorias"

#: ../../tutorials/04_torch_qgan.ipynb:41
msgid "Given :math:`k`-dimensional data samples, we employ a quantum Generative Adversarial Network (qGAN) to learn a random distribution and to load it directly into a quantum state:"
msgstr "Dadas las muestras de datos :math:`k`-dimensionales, empleamos una Red de Adversarios Generativos cuántica (quantum Generative Adversarial Network, qGAN) para conocer una distribución aleatoria y cargarla directamente en un estado cuántico:"

#: ../../tutorials/04_torch_qgan.ipynb:43
msgid "\\big| g_{\\theta}\\rangle = \\sum_{j=0}^{2^n-1} \\sqrt{p_{\\theta}^{j}}\\big| j \\rangle\n\n"
msgstr "\\big| g_{\\theta}\\rangle = \\sum_{j=0}^{2^n-1} \\sqrt{p_{\\theta}^{j}}\\big| j \\rangle\n\n"

#: ../../tutorials/04_torch_qgan.ipynb:45
msgid "where :math:`p_{\\theta}^{j}` describe the occurrence probabilities of the basis states :math:`\\big| j\\rangle`."
msgstr "donde :math:`p_{\\theta}^{j}` describen las probabilidades de ocurrencia de los estados base :math:`\\big| j\\rangle`."

#: ../../tutorials/04_torch_qgan.ipynb:47
msgid "The aim of the qGAN training is to generate a state :math:`\\big| g_{\\theta}\\rangle` where :math:`p_{\\theta}^{j}`, for :math:`j\\in \\left\\{0, \\ldots, {2^n-1} \\right\\}`, describe a probability distribution that is close to the distribution underlying the training data :math:`X=\\left\\{x^0, \\ldots, x^{k-1} \\right\\}`."
msgstr "El objetivo del entrenamiento qGAN es generar un estado :math:`\\big| g_{\\theta}\\rangle` donde :math:`p_{\\theta}^{j}`, para :math:`j\\in \\left\\{0, \\ldots, {2^n-1} \\right\\}`, describe una distribución de probabilidad cercana a la distribución subyacente a los datos de entrenamiento :math:`X=\\left\\{x^0, \\ldots, x^{k-1} \\right\\}`."

#: ../../tutorials/04_torch_qgan.ipynb:49
msgid "For further details please refer to `Quantum Generative Adversarial Networks for Learning and Loading Random Distributions <https://arxiv.org/abs/1904.00043>`__ *Zoufal, Lucchi, Woerner* [2019]."
msgstr "Para obtener más detalles, consulta `Quantum Generative Adversarial Networks for Learning and Loading Random Distributions <https://arxiv.org/abs/1904.00043>`__ *Zoufal, Lucchi, Woerner* [2019]."

#: ../../tutorials/04_torch_qgan.ipynb:51
msgid "For an example of how to use a trained qGAN in an application, the pricing of financial derivatives, please see the `Option Pricing with qGANs <https://github.com/Qiskit/qiskit-finance/tree/main/docs/tutorials/10_qgan_option_pricing.ipynb>`__ tutorial."
msgstr "Para un ejemplo de cómo utilizar una qGAN entrenada en una aplicación, la fijación de precios de derivados financieros, consulta el tutorial `Fijación de Precios de Opciones con qGANs <https://github.com/Qiskit/qiskit-finance/tree/main/docs/tutorials/10_qgan_option_pricing.ipynb>`__."

#: ../../tutorials/04_torch_qgan.ipynb:54
msgid "2. Data and Representation"
msgstr "2. Datos y Representación"

#: ../../tutorials/04_torch_qgan.ipynb:56
msgid "First, we need to load our training data :math:`X`."
msgstr "Primero, necesitamos cargar nuestros datos de entrenamiento :math:`X`."

#: ../../tutorials/04_torch_qgan.ipynb:58
msgid "In this tutorial, the training data is given by a 2D multivariate normal distribution."
msgstr "En este tutorial, los datos de entrenamiento están dados por una distribución normal multivariante 2D."

#: ../../tutorials/04_torch_qgan.ipynb:60
msgid "The goal of the generator is to learn how to represent such distribution, and the trained generator should correspond to an :math:`n`-qubit quantum state :nbsphinx-math:`\\begin{equation} |g_{\\text{trained}}\\rangle=\\sum\\limits_{j=0}^{k-1}\\sqrt{p_{j}}|x_{j}\\rangle, \\end{equation}` where the basis states :math:`|x_{j}\\rangle` represent the data items in the training data set :math:`X={x_0, \\ldots, x_{k-1}}` with :math:`k\\leq 2^n` and :math:`p_j` refers to the sampling probability of :math:`|x_{j}\\rangle`."
msgstr "El objetivo del generador es aprender a representar dicha distribución, y el generador entrenado debe corresponder a un estado cuántico de :math:`n` qubits :nbsphinx-math:`\\begin{equation} |g_{\\text{trained}}\\rangle=\\sum\\limits_{j=0}^{k-1}\\sqrt{p_{j}}|x_{j}\\rangle, \\end{equation}` donde el estado base :math:`|x_{j}\\rangle` representa los elementos de datos en el conjunto de datos de entrenamiento :math:`X={x_0, \\ldots, x_{k-1}}` con :math:`k\\leq 2^n` y :math:`p_j` se refiere a la probabilidad de muestreo de :math:`|x_{j}\\rangle`."

#: ../../tutorials/04_torch_qgan.ipynb:64
msgid "To facilitate this representation, we need to map the samples from the multivariate normal distribution to discrete values. The number of values that can be represented depends on the number of qubits used for the mapping. Hence, the data resolution is defined by the number of qubits. If we use :math:`3` qubits to represent one feature, we have :math:`2^3 = 8` discrete values."
msgstr "Para facilitar esta representación, necesitamos mapear las muestras de la distribución normal multivariante a valores discretos. La cantidad de valores que se pueden representar depende de la cantidad de qubits utilizados para el mapeo. Por lo tanto, la resolución de datos se define por el número de qubits. Si usamos :math:`3` qubits para representar una característica, tenemos :math:`2^3 = 8` valores discretos."

#: ../../tutorials/04_torch_qgan.ipynb:66
msgid "We first begin by fixing seeds in the random number generators for reproducibility of the outcome in this tutorial."
msgstr "Primero comenzamos fijando semillas en los generadores de números aleatorios para la reproducibilidad del resultado en este tutorial."

#: ../../tutorials/04_torch_qgan.ipynb:91
msgid "We fix the number of dimensions, the discretization number and compute the number of qubits required as :math:`2^3 = 8`."
msgstr "Fijamos el número de dimensiones, el número de discretización y calculamos el número de qubits requeridos como :math:`2^3 = 8`."

#: ../../tutorials/04_torch_qgan.ipynb:116
msgid "Then, we prepare a discrete distribution from the continuous 2D normal distribution. We evaluate the continuous probability density function (PDF) on the grid :math:`(-2, 2)^2` with a discretization of :math:`8` values per feature. Thus, we have :math:`64` values of the PDF. Since this will be a discrete distribution we normalize the obtained probabilities."
msgstr "Luego, preparamos una distribución discreta a partir de la distribución normal 2D continua. Evaluamos la función de densidad de probabilidad continua (probability density function, PDF) en la cuadrícula :math:`(-2, 2)^2` con una discretización de :math:`8` valores por función. Por lo tanto, tenemos :math:`64` valores de la PDF. Como esta será una distribución discreta, normalizamos las probabilidades obtenidas."

#: ../../tutorials/04_torch_qgan.ipynb:143
msgid "Let's visualize our distribution. It is a nice bell-shaped bivariate normal distribution on a discrete grid."
msgstr "Visualicemos nuestra distribución. Es una buena distribución normal bivariada en forma de campana en una cuadrícula discreta."

#: ../../tutorials/04_torch_qgan.ipynb:183
msgid "3. Definitions of the Neural Networks"
msgstr "3. Definiciones de las Redes Neuronales"

#: ../../tutorials/04_torch_qgan.ipynb:185
msgid "In this section we define two neural networks as described above:"
msgstr "En esta sección definimos dos redes neuronales como se describe anteriormente:"

#: ../../tutorials/04_torch_qgan.ipynb:187
msgid "A quantum generator as a quantum neural network."
msgstr "Un generador cuántico como una red neuronal cuántica."

#: ../../tutorials/04_torch_qgan.ipynb:188
msgid "A classical discriminator as a PyTorch-based neural network."
msgstr "Un discriminador clásico como una red neuronal basada en PyTorch."

#: ../../tutorials/04_torch_qgan.ipynb:191
msgid "3.1. Definition of the quantum neural network ansatz"
msgstr "3.1. Definición de la red neuronal cuántica ansatz"

#: ../../tutorials/04_torch_qgan.ipynb:193
msgid "Now, we define the parameterized quantum circuit :math:`G\\left(\\boldsymbol{\\theta}\\right)` with :math:`\\boldsymbol{\\theta} = {\\theta_1, ..., \\theta_k}` which will be used in our quantum generator."
msgstr "Ahora, definimos el circuito cuántico parametrizado :math:`G\\left(\\boldsymbol{\\theta}\\right)` con :math:`\\boldsymbol{\\theta} = {\\theta_1, ..., \\theta_k}` que será uilizado en nuestro generador cuántico."

#: ../../tutorials/04_torch_qgan.ipynb:195
msgid "To implement the quantum generator, we choose a hardware efficient ansatz with :math:`6` repetitions. The ansatz implements :math:`R_Y`, :math:`R_Z` rotations and :math:`CX` gates which takes a uniform distribution as an input state. Notably, for :math:`k>1` the generator's parameters must be chosen carefully. For example, the circuit depth should be more than :math:`1` because higher circuit depths enable the representation of more complex structures. Here, we construct quite a deep circuit with a large number of parameters to be able to adequately capture and represent the distribution."
msgstr "Para implementar el generador cuántico, elegimos un ansatz eficiente en hardware con :math:`6` repeticiones. El ansatz implementa rotaciones :math:`R_Y`, :math:`R_Z` y compuertas :math:`CX` que toman una distribución uniforme como estado de entrada. En particular, para :math:`k>1` los parámetros del generador deben elegirse con cuidado. Por ejemplo, la profundidad del circuito debe ser mayor que :math:`1` porque las profundidades de circuito más altas permiten la representación de estructuras más complejas. Aquí, construimos un circuito bastante profundo con una gran cantidad de parámetros para poder capturar y representar adecuadamente la distribución."

#: ../../tutorials/04_torch_qgan.ipynb:224
msgid "Let's draw our circuit and see what it looks like. On the plot we may notice a pattern that appears :math:`6` times."
msgstr "Dibujemos nuestro circuito y veamos cómo se ve. En la gráfica podemos notar un patrón que aparece :math:`6` veces."

#: ../../tutorials/04_torch_qgan.ipynb:254
msgid "Let's print the number of trainable parameters."
msgstr "Imprimamos el número de parámetros entrenables."

#: ../../tutorials/04_torch_qgan.ipynb:301
msgid "3.2. Definition of the quantum generator"
msgstr "3.2. Definición del generador cuántico"

#: ../../tutorials/04_torch_qgan.ipynb:303
msgid "We start defining the generator by creating a sampler for the ansatz. The reference implementation is a statevector-based implementation, thus it returns exact probabilities as a result of circuit execution. We add the ``shots`` parameter to add some noise to the results. In this case the implementation samples probabilities from the multinomial distribution constructed from the measured quasi probabilities. And as usual we fix the seed for reproducibility purposes."
msgstr "Comenzamos definiendo el generador creando un muestreador para el ansatz. La implementación de referencia está basada en vector de estado, por lo que devuelve probabilidades exactas como resultado de la ejecución del circuito. Agregamos el parámetro ``shots`` para agregar algo de ruido a los resultados. En este caso, la implementación muestra probabilidades de la distribución multinomial construida a partir de las cuasi probabilidades medidas. Y, como de costumbre, fijamos la semilla con fines de reproducibilidad."

#: ../../tutorials/04_torch_qgan.ipynb:327
msgid "Next, we define a function that creates the quantum generator from a given parameterized quantum circuit. Inside this function we create a neural network that returns the quasi probability distribution evaluated by the underlying Sampler. We fix ``initial_weights`` for reproducibility purposes. In the end we wrap the created quantum neural network in ``TorchConnector`` to make use of PyTorch-based training."
msgstr "A continuación, definimos una función que crea el generador cuántico a partir de un circuito cuántico parametrizado dado. Dentro de esta función, creamos una red neuronal que devuelve la distribución de cuasi probabilidad evaluada por el Sampler subyacente. Fijamos ``initial_weights`` con fines de reproducibilidad. Al final, envolvemos la red neuronal cuántica creada en ``TorchConnector`` para hacer uso del entrenamiento basado en PyTorch."

#: ../../tutorials/04_torch_qgan.ipynb:363
msgid "3.3. Definition of the classical discriminator"
msgstr "3.3. Definición del discriminador clásico"

#: ../../tutorials/04_torch_qgan.ipynb:365
msgid "Next, we define a PyTorch-based classical neural network that represents the classical discriminator. The underlying gradients can be automatically computed with PyTorch."
msgstr "Después, definimos una red neuronal clásica basada en PyTorch que representa el discriminador clásico. Los gradientes subyacentes se pueden calcular automáticamente con PyTorch."

#: ../../tutorials/04_torch_qgan.ipynb:404
msgid "3.4. Create a generator and a discriminator"
msgstr "3.4. Crear un generador y un discriminador"

#: ../../tutorials/04_torch_qgan.ipynb:406
msgid "Now we create a generator and a discriminator."
msgstr "Ahora creamos un generador y un discriminador."

#: ../../tutorials/04_torch_qgan.ipynb:429
msgid "4. Setting up the Training Loop"
msgstr "4. Configuración del Ciclo de Entrenamiento"

#: ../../tutorials/04_torch_qgan.ipynb:431
msgid "In this section we set up:"
msgstr "En esta sección configuramos:"

#: ../../tutorials/04_torch_qgan.ipynb:433
msgid "A loss function for the generator and discriminator."
msgstr "Una función de pérdida para el generador y el discriminador."

#: ../../tutorials/04_torch_qgan.ipynb:434
msgid "Optimizers for both."
msgstr "Optimizadores para ambos."

#: ../../tutorials/04_torch_qgan.ipynb:435
msgid "A utility plotting function to visualize training process."
msgstr "Una función de utilidad de graficado para visualizar el proceso de entrenamiento."

#: ../../tutorials/04_torch_qgan.ipynb:438
msgid "4.1. Definition of the loss functions"
msgstr "4.1. Definición de las funciones de pérdida"

#: ../../tutorials/04_torch_qgan.ipynb:440
msgid "We want to train the generator and the discriminator with binary cross entropy as the loss function:"
msgstr "Queremos entrenar el generador y el discriminador con entropía cruzada binaria como la función de pérdida:"

#: ../../tutorials/04_torch_qgan.ipynb:442
msgid "L\\left(\\boldsymbol{\\theta}\\right)=\\sum_jp_j\\left(\\boldsymbol{\\theta}\\right)\\left[y_j\\log(x_j) + (1-y_j)\\log(1-x_j)\\right],\n\n"
msgstr "L\\left(\\boldsymbol{\\theta}\\right)=\\sum_jp_j\\left(\\boldsymbol{\\theta}\\right)\\left[y_j\\log(x_j) + (1-y_j)\\log(1-x_j)\\right],\n\n"

#: ../../tutorials/04_torch_qgan.ipynb:444
msgid "where :math:`x_j` refers to a data sample and :math:`y_j` to the corresponding label."
msgstr "donde :math:`x_j` se refiere a una muestra de datos, mientras que :math:`y_j` a la etiqueta correspondiente."

#: ../../tutorials/04_torch_qgan.ipynb:446
msgid "Since PyTorch's ``binary_cross_entropy`` is not differentiable with respect to weights, we implement the loss function manually to be able to evaluate gradients."
msgstr "Dado que ``binary_cross_entropy`` de PyTorch no es diferenciable con respecto a los pesos, implementamos la función de pérdida manualmente para poder evaluar gradientes."

#: ../../tutorials/04_torch_qgan.ipynb:472
msgid "4.2. Definition of the optimizers"
msgstr "4.2. Definición de los optimizadores"

#: ../../tutorials/04_torch_qgan.ipynb:474
msgid "In order to train the generator and discriminator, we need to define optimization schemes. In the following, we employ a momentum based optimizer called Adam, see `Kingma et al., Adam: A method for stochastic optimization <https://arxiv.org/abs/1412.6980>`__ for more details."
msgstr "Para entrenar el generador y el discriminador, necesitamos definir esquemas de optimización. A continuación, empleamos un optimizador basado en el momento llamado Adam, consulta `Kingma et al., Adam: A method for stochastic optimization <https://arxiv.org/abs/1412.6980>`__ para obtener más detalles."

#: ../../tutorials/04_torch_qgan.ipynb:505
msgid "4.3. Visualization of the training process"
msgstr "4.3. Visualización del proceso de entrenamiento"

#: ../../tutorials/04_torch_qgan.ipynb:507
msgid "We will visualize what is happening during the training by plotting the evolution of the generator's and the discriminator's loss functions during the training, as well as the progress in the relative entropy between the trained and the target distribution. We define a function that plots the loss functions and relative entropy. We call this function once an epoch of training is complete."
msgstr "Visualizaremos lo que sucede durante el entrenamiento graficando la evolución de las funciones de pérdida del generador y del discriminador durante el entrenamiento, así como el progreso en la entropía relativa entre la distribución entrenada y objetivo. Definimos una función que grafique las funciones de pérdida y la entropía relativa. Llamamos a esta función una vez que se completa una época de entrenamiento."

#: ../../tutorials/04_torch_qgan.ipynb:509
msgid "Visualization of the training process begins when training data is collected across two epochs."
msgstr "La visualización del proceso de entrenamiento comienza cuando los datos de entrenamiento se recopilan en dos épocas."

#: ../../tutorials/04_torch_qgan.ipynb:558
msgid "5. Model Training"
msgstr "5. Entrenamiento del Modelo"

#: ../../tutorials/04_torch_qgan.ipynb:560
msgid "In the training loop we monitor not only loss functions, but relative entropy as well. The relative entropy describes a distance metric for distributions. Hence, we can use it to benchmark how close/far away the trained distribution is from the target distribution."
msgstr "En el ciclo de entrenamiento monitoreamos no solo las funciones de pérdida, sino también la entropía relativa. La entropía relativa describe una métrica de distancia para las distribuciones. Por lo tanto, podemos usarla para comparar qué tan cerca/lejos está la distribución entrenada de la distribución objetivo."

#: ../../tutorials/04_torch_qgan.ipynb:562
msgid "Now, we are ready to train our model. It may take some time to train the model so be patient."
msgstr "Ahora, estamos listos para entrenar nuestro modelo. Puede llevar algo de tiempo entrenar al modelo, así que ten paciencia."

#: ../../tutorials/04_torch_qgan.ipynb:673
msgid "6. Results: Cumulative Density Functions"
msgstr "6. Resultados: Funciones de Densidad Acumulativa"

#: ../../tutorials/04_torch_qgan.ipynb:675
msgid "In this section we compare the cumulative distribution function (CDF) of the trained distribution to the CDF of the target distribution."
msgstr "En esta sección, comparamos la función de distribución acumulativa (cumulative distribution function, CDF) de la distribución entrenada con la CDF de la distribución objetivo."

#: ../../tutorials/04_torch_qgan.ipynb:677
msgid "First, we generate a new probability distribution with PyTorch autograd turned off as we are not going to train the model anymore."
msgstr "Primero, generamos una nueva distribución de probabilidad con el autograd de PyTorch desactivado, ya que no vamos a entrenar más el modelo."

#: ../../tutorials/04_torch_qgan.ipynb:699
msgid "And then, we plot the cumulative distribution functions of the generated distribution, original distribution, and the difference between them. Please, be careful, the scale on the third plot **is not the same** as on the first and second plot, and the actual difference between the two plotted CDFs is pretty small."
msgstr "Y luego, graficamos las funciones de distribución acumulativa de la distribución generada, la distribución original y la diferencia entre ellas. Ten cuidado, la escala en la tercera gráfica **no es la misma** que en la primera y segunda gráfica, y la diferencia real entre las dos CDF graficadas es bastante pequeña."

#: ../../tutorials/04_torch_qgan.ipynb:754
msgid "7. Conclusion"
msgstr "7. Conclusión"

#: ../../tutorials/04_torch_qgan.ipynb:756
msgid "Quantum generative adversarial networks employ the interplay of a generator and discriminator to map an approximate representation of a probability distribution underlying given data samples into a quantum channel. This tutorial presents a self-standing PyTorch-based qGAN implementation where the generator is given by a quantum channel, i.e., a variational quantum circuit, and the discriminator by a classical neural network, and discusses the application of efficient learning and loading of generic probability distributions into quantum states. The loading requires :math:`\\mathscr{O}\\left(poly\\left(n\\right)\\right)` gates and can thus enable the use of potentially advantageous quantum algorithms."
msgstr "Las redes de adversarios generativos cuánticas emplean la interacción de un generador y un discriminador para mapear una representación aproximada de una distribución de probabilidad subyacente a muestras de datos dadas en un canal cuántico. Este tutorial presenta una implementación autónoma de qGAN basada en PyTorch, donde el generador está dado por un canal cuántico, es decir, un circuito cuántico variacional, y el discriminador por una red neuronal clásica, y analiza la aplicación de aprendizaje eficiente y carga de distribuciones de probabilidad genéricas en estados cuánticos. La carga requiere compuertas :math:`\\mathscr{O}\\left(poly\\left(n\\right)\\right)` y, por lo tanto, puede permitir el uso de algoritmos cuánticos potencialmente ventajosos."

