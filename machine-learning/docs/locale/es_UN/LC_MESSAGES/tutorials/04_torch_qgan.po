msgid ""
msgstr ""
"Project-Id-Version: qiskit-docs\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-02-07 03:44+0000\n"
"PO-Revision-Date: 2023-03-06 16:50\n"
"Last-Translator: \n"
"Language: es_UN\n"
"Language-Team: Spanish (United)\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.11.0\n"
"X-Crowdin-Project: qiskit-docs\n"
"X-Crowdin-Project-ID: 369271\n"
"X-Crowdin-Language: es-un\n"
"X-Crowdin-File: /master/machine-learning/docs/locale/en/LC_MESSAGES/tutorials/04_torch_qgan.po\n"
"X-Crowdin-File-ID: 9885\n"

#: ../../tutorials/04_torch_qgan.ipynb:9
msgid "This page was generated from `docs/tutorials/04_torch_qgan.ipynb`__."
msgstr "Esta página fue generada a partir de `docs/tutorials/04_torch_qgan.ipynb`__."

#: ../../tutorials/04_torch_qgan.ipynb:9
msgid "PyTorch qGAN Implementation"
msgstr "Implementación de PyTorch qGAN"

#: ../../tutorials/04_torch_qgan.ipynb:12
msgid "Overview"
msgstr "Descripción General"

#: ../../tutorials/04_torch_qgan.ipynb:14
msgid "This tutorial introduces step-by-step how to build a PyTorch-based Quantum Generative Adversarial Network algorithm."
msgstr "Este tutorial presenta paso a paso cómo construir un algoritmo de Red de Adversarios Generativos Cuántica basado en PyTorch."

#: ../../tutorials/04_torch_qgan.ipynb:17
msgid "Context"
msgstr "Contexto"

#: ../../tutorials/04_torch_qgan.ipynb:19
msgid "The qGAN [1] is a hybrid quantum-classical algorithm used for generative modeling tasks. The algorithm uses the interplay of a quantum generator :math:`G_{\\theta}`, i.e., an ansatz, and a classical discriminator :math:`D_{\\phi}`, a neural network, to learn the underlying probability distribution given training data."
msgstr "El qGAN [1] es un algoritmo híbrido cuántico-clásico utilizado para tareas de modelado generativo. El algoritmo utiliza la interacción de un generador cuántico :math:`G_{\\theta}`, es decir, un ansatz, y un discriminador clásico :math:`D_{\\phi}`, una red neuronal, para aprender la distribución de probabilidad subyacente dados los datos de entrenamiento."

#: ../../tutorials/04_torch_qgan.ipynb:21
msgid "The generator and discriminator are trained in alternating optimization steps, where the generator aims at generating samples that will be classified by the discriminator as training data samples (i.e, samples extracted from the real training distribution), and the discriminator tries to differentiate between original training data samples and data samples from the generator (in other words, telling apart the real and generated distributions). The final goal is for the quantum generator to learn a representation for the training data’s underlying probability distribution. The trained quantum generator can, thus, be used to load a quantum state which is an approximate model of the target distribution."
msgstr "El generador y el discriminador se entrenan en pasos de optimización alternantes, donde el generador apunta a generar muestras que serán clasificadas por el discriminador como muestras de datos de entrenamiento (es decir, muestras extraídas de la distribución de entrenamiento real), y el discriminador trata de diferenciar entre muestras de datos del entrenamiento original y muestras de datos del generador (en otras palabras, distinguir las distribuciones reales y generadas). El objetivo final es que el generador cuántico aprenda una representación de la distribución de probabilidad subyacente de los datos de entrenamiento. El generador cuántico entrenado puede, por lo tanto, usarse para cargar un estado cuántico que es un modelo aproximado de la distribución objetivo."

#: ../../tutorials/04_torch_qgan.ipynb:24
msgid "**References:**"
msgstr "**Referencias:**"

#: ../../tutorials/04_torch_qgan.ipynb:26
msgid "[1] Zoufal et al., `Quantum Generative Adversarial Networks for learning and loading random distributions <https://www.nature.com/articles/s41534-019-0223-2>`__"
msgstr "[1] Zoufal et al., `Quantum Generative Adversarial Networks for learning and loading random distributions <https://www.nature.com/articles/s41534-019-0223-2>`__"

#: ../../tutorials/04_torch_qgan.ipynb:29
msgid "Application: qGANs for Loading Random Distributions"
msgstr "Aplicación: qGANs para Cargar Distribuciones Aleatorias"

#: ../../tutorials/04_torch_qgan.ipynb:31
msgid "Given :math:`k`-dimensional data samples, we employ a quantum Generative Adversarial Network (qGAN) to learn the data’s underlying random distribution and to load it directly into a quantum state:"
msgstr "Dadas las muestras de datos :math:`k`-dimensionales, empleamos una Red de Adversarios Generativos cuántica (quantum Generative Adversarial Network, qGAN) para conocer la distribución aleatoria subyacente de los datos y cargarlos directamente en un estado cuántico:"

#: ../../tutorials/04_torch_qgan.ipynb:33
msgid "\\big| g_{\\theta}\\rangle = \\sum_{j=0}^{2^n-1} \\sqrt{p_{\\theta}^{j}}\\big| j \\rangle\n\n"
msgstr "\\big| g_{\\theta}\\rangle = \\sum_{j=0}^{2^n-1} \\sqrt{p_{\\theta}^{j}}\\big| j \\rangle\n\n"

#: ../../tutorials/04_torch_qgan.ipynb:35
msgid "where :math:`p_{\\theta}^{j}` describe the occurrence probabilities of the basis states :math:`\\big| j\\rangle`."
msgstr "donde :math:`p_{\\theta}^{j}` describen las probabilidades de ocurrencia de los estados base :math:`\\big| j\\rangle`."

#: ../../tutorials/04_torch_qgan.ipynb:37
msgid "The aim of the qGAN training is to generate a state :math:`\\big| g_{\\theta}\\rangle` where :math:`p_{\\theta}^{j}`, for :math:`j\\in \\left\\{0, \\ldots, {2^n-1} \\right\\}`, describe a probability distribution that is close to the distribution underlying the training data :math:`X=\\left\\{x^0, \\ldots, x^{k-1} \\right\\}`."
msgstr "El objetivo del entrenamiento qGAN es generar un estado :math:`\\big| g_{\\theta}\\rangle` donde :math:`p_{\\theta}^{j}`, para :math:`j\\in \\left\\{0, \\ldots, {2^n-1} \\right\\}`, describe una distribución de probabilidad cercana a la distribución subyacente a los datos de entrenamiento :math:`X=\\left\\{x^0, \\ldots, x^{k-1} \\right\\}`."

#: ../../tutorials/04_torch_qgan.ipynb:39
msgid "For further details please refer to `Quantum Generative Adversarial Networks for Learning and Loading Random Distributions <https://arxiv.org/abs/1904.00043>`__ *Zoufal, Lucchi, Woerner* [2019]."
msgstr "Para obtener más detalles, consulta `Quantum Generative Adversarial Networks for Learning and Loading Random Distributions <https://arxiv.org/abs/1904.00043>`__ *Zoufal, Lucchi, Woerner* [2019]."

#: ../../tutorials/04_torch_qgan.ipynb:41
msgid "For an example of how to use a trained qGAN in an application, the pricing of financial derivatives, please see the `Option Pricing with qGANs <https://github.com/Qiskit/qiskit-finance/tree/main/docs/tutorials/10_qgan_option_pricing.ipynb>`__ tutorial."
msgstr "Para un ejemplo de cómo utilizar una qGAN entrenada en una aplicación, la fijación de precios de derivados financieros, consulta el tutorial `Fijación de Precios de Opciones con qGANs <https://github.com/Qiskit/qiskit-finance/tree/main/docs/tutorials/10_qgan_option_pricing.ipynb>`__."

#: ../../tutorials/04_torch_qgan.ipynb:44
msgid "Tutorial"
msgstr "Tutorial"

#: ../../tutorials/04_torch_qgan.ipynb:47
msgid "Data and Representation"
msgstr "Datos y Representación"

#: ../../tutorials/04_torch_qgan.ipynb:49
msgid "First, we need to load our training data :math:`X`."
msgstr "Primero, necesitamos cargar nuestros datos de entrenamiento :math:`X`."

#: ../../tutorials/04_torch_qgan.ipynb:51
msgid "In this tutorial, the training data is given by samples from a 2D multivariate normal distribution."
msgstr "En este tutorial, los datos de entrenamiento están dados por muestras de una distribución normal multivariante 2D."

#: ../../tutorials/04_torch_qgan.ipynb:53
msgid "The goal of the generator is to learn how to represent such distribution, and the trained generator should correspond to an :math:`n`-qubit quantum state :nbsphinx-math:`\\begin{equation} |g_{\\text{trained}}\\rangle=\\sum\\limits_{j=0}^{k-1}\\sqrt{p_{j}}|x_{j}\\rangle, \\end{equation}` where the basis states :math:`|x_{j}\\rangle` represent the data items in the training data set :math:`X={x_0, \\ldots, x_{k-1}}` with :math:`k\\leq 2^n` and :math:`p_j` refers to the sampling probability of :math:`|x_{j}\\rangle`."
msgstr "El objetivo del generador es aprender a representar dicha distribución, y el generador entrenado debe corresponder a un estado cuántico de :math:`n` qubits :nbsphinx-math:`\\begin{equation} |g_{\\text{trained}}\\rangle=\\sum\\limits_{j=0}^{k-1}\\sqrt{p_{j}}|x_{j}\\rangle, \\end{equation}` donde el estado base :math:`|x_{j}\\rangle` representa los elementos de datos en el conjunto de datos de entrenamiento :math:`X={x_0, \\ldots, x_{k-1}}` con :math:`k\\leq 2^n` y :math:`p_j` se refiere a la probabilidad de muestreo de :math:`|x_{j}\\rangle`."

#: ../../tutorials/04_torch_qgan.ipynb:57
msgid "To facilitate this representation, we need to map the samples from the multivariate normal distribution to discrete values. The number of values that can be represented depends on the number of qubits used for the mapping. Hence, the data resolution is defined by the number of qubits. If we use :math:`3` qubits to represent one feature, we have :math:`2^3 = 8` discrete values."
msgstr "Para facilitar esta representación, necesitamos mapear las muestras de la distribución normal multivariante a valores discretos. La cantidad de valores que se pueden representar depende de la cantidad de qubits utilizados para el mapeo. Por lo tanto, la resolución de datos se define por el número de qubits. Si usamos :math:`3` qubits para representar una característica, tenemos :math:`2^3 = 8` valores discretos."

#: ../../tutorials/04_torch_qgan.ipynb:59
msgid "We first begin by fixing seeds in the random number generators, then we will import the libraries and packages we will need for this tutorial."
msgstr "Primero comenzamos fijando semillas en los generadores de números aleatorios, luego importaremos las bibliotecas y los paquetes que necesitaremos para este tutorial."

#: ../../tutorials/04_torch_qgan.ipynb:84
msgid "Then, we sample some data from a 2D multivariate normal distribution as described above."
msgstr "Luego, muestreamos algunos datos de una distribución normal multivariante 2D como se describe anteriormente."

#: ../../tutorials/04_torch_qgan.ipynb:132
msgid "Let’s visualize our distribution and samples. The ``discretize_and_truncate`` function discretized the classical data we got from the multivariate normal so, a plot of the probability density function looks very coarse. But it is still a bell-shaped bivariate normal distribution."
msgstr "Visualicemos nuestra distribución y muestras. La función ``discretize_and_truncate`` discretizó los datos clásicos que obtuvimos de la normal multivariada, por lo que una gráfica de la función de densidad de probabilidad parece muy tosca. Pero sigue siendo una distribución normal bivariada en forma de campana."

#: ../../tutorials/04_torch_qgan.ipynb:194
msgid "On the next figure we prove that the training data was discretized. We passed data resolution as an array ``[3, 3]``, This array defined the number of qubits used to represent each data dimension. Thus, we can define :math:`2^3 = 8` discrete values and all training samples should fall under one of these discrete values. On the next figure there is a histogram for each random variable and to see actual discretization we increased the number of bins. Some of the bins are empty and there are 8 non-empty bins for each variable."
msgstr "En la siguiente figura demostramos que los datos de entrenamiento fueron discretizados. Pasamos la resolución de datos como un arreglo de ``[3, 3]``. Este arreglo definió la cantidad de qubits utilizados para representar cada dimensión de datos. Por lo tanto, podemos definir :math:`2^3 = 8` valores discretos y todas las muestras de entrenamiento deben caer bajo uno de estos valores discretos. En la siguiente figura hay un histograma para cada variable aleatoria y para ver la discretización real aumentamos el número de contenedores (bins). Algunos de los contenedores están vacíos y hay 8 contenedores no vacíos para cada variable."

#: ../../tutorials/04_torch_qgan.ipynb:260
msgid "We move to PyTorch modeling and start from converting data arrays into tensors and create a data loader from our training data."
msgstr "Pasamos al modelado con PyTorch y comenzamos convirtiendo arreglos de datos en tensores y creamos un cargador de datos a partir de nuestros datos de entrenamiento."

#: ../../tutorials/04_torch_qgan.ipynb:289
msgid "Backend Configurations"
msgstr "Configuraciones del Backend"

#: ../../tutorials/04_torch_qgan.ipynb:291
msgid "Next, we need to choose a backend that is used to run the quantum generator. The presented method is compatible with all shot-based backends (qasm, fake hardware, real hardware) provided by Qiskit."
msgstr "A continuación, debemos elegir un backend que se utilice para ejecutar el generador cuántico. El método presentado es compatible con todos los backends basados en iteraciones o shots (qasm, hardware falso, hardware real) proporcionados por Qiskit."

#: ../../tutorials/04_torch_qgan.ipynb:293
msgid "First, we create a quantum instance for the training where the batch size defines the number of shots."
msgstr "Primero, creamos una instancia cuántica (quantum instance) para el entrenamiento, donde el tamaño del lote (batch) define la cantidad de iteraciones (shots)."

#: ../../tutorials/04_torch_qgan.ipynb:318
msgid "Then we create a quantum instance for the evaluation purposes, we choose a higher number of shots to get better insights."
msgstr "Luego creamos una instancia cuántica para fines de evaluación, elegimos una mayor cantidad de iteraciones para obtener una mejor apreciación."

#: ../../tutorials/04_torch_qgan.ipynb:340
msgid "Initialize the quantum neural network ansatz"
msgstr "Inicializar la red neuronal cuántica ansatz"

#: ../../tutorials/04_torch_qgan.ipynb:342
msgid "Now, we define the parameterized quantum circuit :math:`G\\left(\\boldsymbol{\\theta}\\right)` with :math:`\\boldsymbol{\\theta} = {\\theta_1, ..., \\theta_k}` which will be used in our quantum generator."
msgstr "Ahora, definimos el circuito cuántico parametrizado :math:`G\\left(\\boldsymbol{\\theta}\\right)` con :math:`\\boldsymbol{\\theta} = {\\theta_1, ..., \\theta_k}` que será uilizado en nuestro generador cuántico."

#: ../../tutorials/04_torch_qgan.ipynb:344
msgid "To implement the quantum generator, we choose a depth-:math:`2` ansatz that implements :math:`R_Y` rotations and :math:`CX` gates which takes a uniform distribution as an input state. Notably, for :math:`k>1` the generator’s parameters must be chosen carefully. For example, the circuit depth should be more than :math:`1` because higher circuit depths enable the representation of more complex structures."
msgstr "Para implementar el generador cuántico, elegimos un ansatz de profundidad :math:`2` que implementa rotaciones :math:`R_Y` y compuertas :math:`CX` que toman una distribución uniforme como estado de entrada. En particular, para :math:`k>1` los parámetros del generador deben elegirse con cuidado. Por ejemplo, la profundidad del circuito debe ser mayor que :math:`1` porque las profundidades de circuito más altas permiten la representación de estructuras más complejas."

#: ../../tutorials/04_torch_qgan.ipynb:374
msgid "Let’s draw our circuit and see what it looks like."
msgstr "Dibujemos nuestro circuito y veamos cómo se ve."

#: ../../tutorials/04_torch_qgan.ipynb:405
msgid "Definition of the quantum generator"
msgstr "Definición del generador cuántico"

#: ../../tutorials/04_torch_qgan.ipynb:407
msgid "Next, we define a function that creates the quantum generator from a given parameterized quantum circuit. As parameters this function takes a quantum instance to be used for data sampling. We wrap a created quantum neural network in ``TorchConnector`` to make use of PyTorch-based training."
msgstr "A continuación, definimos una función que crea el generador cuántico a partir de un circuito cuántico parametrizado dado. Como parámetros, esta función toma una instancia cuántica que se utilizará para el muestreo de datos. Encapsulamos una red neuronal cuántica creada en ``TorchConnector`` para hacer uso del entrenamiento basado en PyTorch."

#: ../../tutorials/04_torch_qgan.ipynb:444
msgid "Definition of the classical discriminator"
msgstr "Definición del discriminador clásico"

#: ../../tutorials/04_torch_qgan.ipynb:446
msgid "Next, we define a PyTorch-based classical neural network that represents the classical discriminator. The underlying gradients can be automatically computed with PyTorch."
msgstr "Después, definimos una red neuronal clásica basada en PyTorch que representa el discriminador clásico. Los gradientes subyacentes se pueden calcular automáticamente con PyTorch."

#: ../../tutorials/04_torch_qgan.ipynb:485
msgid "Definition of the loss functions"
msgstr "Definición de las funciones de pérdida"

#: ../../tutorials/04_torch_qgan.ipynb:487
msgid "We want to train the generator and the discriminator with binary cross entropy as loss function:"
msgstr "Queremos entrenar el generador y el discriminador con entropía cruzada binaria como función de pérdida:"

#: ../../tutorials/04_torch_qgan.ipynb:489
msgid "L\\left(\\boldsymbol{\\theta}\\right)=\\sum_jp_j\\left(\\boldsymbol{\\theta}\\right)\\left[y_j\\log(x_j) + (1-y_j)\\log(1-x_j)\\right],\n\n"
msgstr "L\\left(\\boldsymbol{\\theta}\\right)=\\sum_jp_j\\left(\\boldsymbol{\\theta}\\right)\\left[y_j\\log(x_j) + (1-y_j)\\log(1-x_j)\\right],\n\n"

#: ../../tutorials/04_torch_qgan.ipynb:491
msgid "where :math:`x_j` refers to a data sample and :math:`y_j` to the corresponding label."
msgstr "donde :math:`x_j` se refiere a una muestra de datos, mientras que :math:`y_j` a la etiqueta correspondiente."

#: ../../tutorials/04_torch_qgan.ipynb:517
msgid "Evaluation of custom gradients for the generator BCE loss function"
msgstr "Evaluación de gradientes personalizados para la función de pérdida BCE del generador"

#: ../../tutorials/04_torch_qgan.ipynb:519
msgid "The evaluation of custom gradients for the quantum generator requires us to combine quantum gradients :math:`\\frac{\\partial p_j\\left(\\boldsymbol{\\theta}\\right)}{\\partial \\theta_l}` that we compute with Qiskit’s gradient framework with the binary cross entropy as follows:"
msgstr "La evaluación de gradientes personalizados para el generador cuántico requiere que combinemos gradientes cuánticos :math:`\\frac{\\partial p_j\\left(\\boldsymbol{\\theta}\\right)}{\\partial \\theta_l}` que calculamos con el framework de gradiente de Qiskit con la entropía cruzada binaria de la siguiente manera:"

#: ../../tutorials/04_torch_qgan.ipynb:521
msgid "\\frac{\\partial L\\left(\\boldsymbol{\\theta}\\right)}{\\partial \\theta_l} = \\sum_j \\frac{\\partial p_j\\left(\\boldsymbol{\\theta}\\right)}{\\partial \\theta_l} \\left[y_j\\log(x_j) + (1-y_j)\\log(1-x_j)\\right].\n\n"
msgstr "\\frac{\\partial L\\left(\\boldsymbol{\\theta}\\right)}{\\partial \\theta_l} = \\sum_j \\frac{\\partial p_j\\left(\\boldsymbol{\\theta}\\right)}{\\partial \\theta_l} \\left[y_j\\log(x_j) + (1-y_j)\\log(1-x_j)\\right].\n\n"

#: ../../tutorials/04_torch_qgan.ipynb:523
msgid "First, we need to define how gradients will be evaluated. Depending on the backend, the gradients may be returned in a sparse format. Since PyTorch provides only a limited support for sparse gradients, we need to write a custom function for gradient based training."
msgstr "Primero, necesitamos definir cómo se evaluarán los gradientes. Según el backend, los gradientes pueden devolverse en un formato disperso. Dado que PyTorch proporciona solo un soporte limitado para gradientes dispersos, necesitamos escribir una función personalizada para el entrenamiento basado en gradientes."

#: ../../tutorials/04_torch_qgan.ipynb:548
msgid "Here, we define the custom function that evaluates gradients of the generator loss function considering the custom gradients of the quantum generator. As the parameters the function takes a list of parameter values and the discriminator as a classical neural network. The function returns a list of gradient values."
msgstr "Aquí, definimos la función personalizada que evalúa los gradientes de la función de pérdida del generador considerando los gradientes personalizados del generador cuántico. Como los parámetros, la función toma una lista de valores de parámetros y el discriminador como una red neuronal clásica. La función devuelve una lista de valores de gradiente."

#: ../../tutorials/04_torch_qgan.ipynb:590
msgid "Relative entropy as benchmarking metric"
msgstr "Entropía relativa como métrica de evaluación comparativa"

#: ../../tutorials/04_torch_qgan.ipynb:592
msgid "The relative entropy describes a distance metric for distributions. Hence, we can use it to benchmark how close/far away the trained distribution is from the target distribution."
msgstr "La entropía relativa describe una métrica de distancia para las distribuciones. Por lo tanto, podemos usarla para comparar qué tan cerca/lejos está la distribución entrenada de la distribución objetivo."

#: ../../tutorials/04_torch_qgan.ipynb:594
msgid "In the next function we computes relative entropy between target and trained distribution."
msgstr "En la siguiente función, calculamos la entropía relativa entre la distribución objetivo y la entrenada."

#: ../../tutorials/04_torch_qgan.ipynb:624
msgid "Definition of the optimizers"
msgstr "Definición de los optimizadores"

#: ../../tutorials/04_torch_qgan.ipynb:626
msgid "In order to train the generator and discriminator, we need to define optimization schemes. In the following, we employ a momentum based optimizer called Adam, see `Kingma et al., Adam: A method for stochastic optimization <https://arxiv.org/abs/1412.6980>`__ for more details."
msgstr "Para entrenar el generador y el discriminador, necesitamos definir esquemas de optimización. A continuación, empleamos un optimizador basado en el momento llamado Adam, consulta `Kingma et al., Adam: A method for stochastic optimization <https://arxiv.org/abs/1412.6980>`__ para obtener más detalles."

#: ../../tutorials/04_torch_qgan.ipynb:662
msgid "Visualization of the training process"
msgstr "Visualización del proceso de entrenamiento"

#: ../../tutorials/04_torch_qgan.ipynb:664
msgid "We will visualize what is happening during the training by plotting the evolution of the generator’s and the discriminator’s loss functions during the training, as well as the progress in the relative entropy between the trained and the target distribution. We define a function that plots the loss functions and relative entropy. We call this function once an epoch of training is complete."
msgstr "Visualizaremos lo que sucede durante el entrenamiento graficando la evolución de las funciones de pérdida del generador y del discriminador durante el entrenamiento, así como el progreso en la entropía relativa entre la distribución entrenada y objetivo. Definimos una función que grafique las funciones de pérdida y la entropía relativa. Llamamos a esta función una vez que se completa una época de entrenamiento."

#: ../../tutorials/04_torch_qgan.ipynb:666
msgid "Visualization of the training process begins when training data is collected across two epochs."
msgstr "La visualización del proceso de entrenamiento comienza cuando los datos de entrenamiento se recopilan en dos épocas."

#: ../../tutorials/04_torch_qgan.ipynb:715
msgid "Training"
msgstr "Entrenamiento"

#: ../../tutorials/04_torch_qgan.ipynb:717
msgid "Now, we are ready to train our model. It may take some time to train the model so be patient."
msgstr "Ahora, estamos listos para entrenar nuestro modelo. Puede llevar algo de tiempo entrenar al modelo, así que ten paciencia."

#: ../../tutorials/04_torch_qgan.ipynb:804
msgid "Results: cumulative distribution functions"
msgstr "Resultados: funciones de distribución acumulativas"

#: ../../tutorials/04_torch_qgan.ipynb:806
msgid "In the final section we compare the cumulative distribution function (CDF) of the trained distribution to the CDF of the target distribution."
msgstr "En la sección final, comparamos la función de distribución acumulativa (cumulative distribution function, CDF) de la distribución entrenada con la CDF de la distribución objetivo."

#: ../../tutorials/04_torch_qgan.ipynb:808
msgid "We create a new generator for sampling that takes more shots and, hence, gives more information. Recall, the sampling quantum instance was created with a larger number of shots. Then, we set the weights of the sampling generator to the values obtained in the training process."
msgstr "Creamos un nuevo generador de muestreo que toma más iteraciones y, por lo tanto, brinda más información. Recuerda, la instancia cuántica de muestreo se creó con una mayor cantidad de iteraciones. Luego, ajustamos los pesos del generador de muestreo a los valores obtenidos en el proceso de entrenamiento."

#: ../../tutorials/04_torch_qgan.ipynb:830
msgid "Next, we wet the generator data samples, corresponding sampling probabilities, and plot the cumulative distribution functions."
msgstr "A continuación, obtenemos las muestras de datos del generador, las probabilidades de muestreo correspondientes y graficamos las funciones de distribución acumulativa."

#: ../../tutorials/04_torch_qgan.ipynb:898
msgid "On the plot above, in the blue color the generated distribution is shown and in the orange color the training one. You may find that both CDFs are similar to each other."
msgstr "En la gráfica de arriba, en color azul, se muestra la distribución generada y en color naranja la de entrenamiento. Puedes encontrar que ambos CDF son similares entre sí."

#: ../../tutorials/04_torch_qgan.ipynb:910
msgid "Conclusion"
msgstr "Conclusión"

#: ../../tutorials/04_torch_qgan.ipynb:912
msgid "Quantum generative adversarial networks employ the interplay of a generator and discriminator to map an approximate representation of a probability distribution underlying given data samples into a quantum channel. This tutorial presents a self-standing PyTorch-based qGAN implementation where the generator is given by a quantum channel, i.e., a variational quantum circuit, and the discriminator by a classical neural network, and discusses the application of efficient learning and loading of generic probability distributions – implicitly given by data samples – into quantum states. Since, this approximate loading requires only :math:`\\mathscr{O}\\left(poly\\left(n\\right)\\right)` gates and can enable the use of potentially advantageous quantum algorithms by offering an efficient data loading scheme."
msgstr "Las redes de adversarios generativos cuánticas emplean la interacción de un generador y un discriminador para mapear una representación aproximada de una distribución de probabilidad subyacente a muestras de datos dadas en un canal cuántico. Este tutorial presenta una implementación autónoma de qGAN basada en PyTorch, donde el generador está dado por un canal cuántico, es decir, un circuito cuántico variacional, y el discriminador por una red neuronal clásica, y analiza la aplicación de aprendizaje eficiente y carga de distribuciones de probabilidad genéricas, dadas implícitamente por muestras de datos, en estados cuánticos. Dado que esta carga aproximada requiere solo compuertas :math:`\\mathscr{O}\\left(poly\\left(n\\right)\\right)` y puede permitir el uso de algoritmos cuánticos potencialmente ventajosos al ofrecer un esquema de carga de datos eficiente."

