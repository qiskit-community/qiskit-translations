msgid ""
msgstr ""
"Project-Id-Version: qiskit-docs\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-02-07 03:44+0000\n"
"PO-Revision-Date: 2023-03-04 23:26\n"
"Last-Translator: \n"
"Language: es_UN\n"
"Language-Team: Spanish (United)\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.11.0\n"
"X-Crowdin-Project: qiskit-docs\n"
"X-Crowdin-Project-ID: 369271\n"
"X-Crowdin-Language: es-un\n"
"X-Crowdin-File: /master/machine-learning/docs/locale/en/LC_MESSAGES/tutorials/10_effective_dimension.po\n"
"X-Crowdin-File-ID: 9730\n"

#: ../../tutorials/10_effective_dimension.ipynb:9
msgid "This page was generated from `docs/tutorials/10_effective_dimension.ipynb`__."
msgstr "Esta página fue generada a partir de `docs/tutorials/10_effective_dimension.ipynb`__."

#: ../../tutorials/10_effective_dimension.ipynb:9
msgid "Effective Dimension of Qiskit Neural Networks"
msgstr "Dimensión Efectiva de las Redes Neuronales de Qiskit"

#: ../../tutorials/10_effective_dimension.ipynb:11
msgid "In this tutorial, we will take advantage of the ``EffectiveDimension`` and ``LocalEffectiveDimension`` classes to evaluate the power of Quantum Neural Network models. These are metrics based on information geometry that connect to notions such as trainability, expressibility or ability to generalize."
msgstr "En este tutorial, aprovecharemos las clases ``EffectiveDimension`` y ``LocalEffectiveDimension`` para evaluar el poder de los modelos de redes neuronales cuánticas. Son métricas basadas en la geometría de la información que conectan con nociones como la entrenabilidad, la expresibilidad o la capacidad de generalización."

#: ../../tutorials/10_effective_dimension.ipynb:13
msgid "Before diving into the code example, we will briefly explain what is the difference between these two metrics, and why are they relevant to the study of Quantum Neural Networks. More information about global effective dimension can be found in `this paper <https://arxiv.org/pdf/2011.00027.pdf>`__, while the local effective dimension was introduced in a `later work <https://arxiv.org/abs/2112.04807>`__."
msgstr "Antes de profundizar en el ejemplo de código, explicaremos brevemente cuál es la diferencia entre estas dos métricas y por qué son relevantes para el estudio de las redes neuronales cuánticas. Se puede encontrar más información sobre la dimensión efectiva global en `este artículo <https://arxiv.org/pdf/2011.00027.pdf>`__, mientras que la dimensión efectiva local se introdujo en un `trabajo posterior <https://arxiv.org/abs/2112.04807>`__."

#: ../../tutorials/10_effective_dimension.ipynb:25
msgid "1. Global vs. Local Effective Dimension"
msgstr "1. Dimensión Efectiva Global vs. Local"

#: ../../tutorials/10_effective_dimension.ipynb:27
msgid "Both classical and quantum machine learning models share a common goal: being good at **generalizing**, i.e. learning insights from data and applying them on unseen data."
msgstr "Tanto el modelo de machine learning clásico como el cuántico comparten un objetivo común: ser bueno para **generalizar**, es decir, aprender información de los datos y aplicarla en datos ocultos."

#: ../../tutorials/10_effective_dimension.ipynb:29
msgid "Finding a good metric to assess this ability is a non-trivial matter. In `The Power of Quantum Neural Networks <https://arxiv.org/pdf/2011.00027.pdf>`__, the authors introduce the **global** effective dimension as a useful indicator of how well a particular model will be able to perform on new data. In `Effective Dimension of Machine Learning Models <https://arxiv.org/pdf/2112.04807.pdf>`__, the **local** effective dimension is proposed as a new capacity measure that bounds the generalization error of machine learning models."
msgstr "Encontrar una buena métrica para evaluar esta capacidad no es un asunto trivial. En `The Power of Quantum Neural Networks <https://arxiv.org/pdf/2011.00027.pdf>`__, los autores presentan la dimensión efectiva **global** como un indicador útil de qué tan bien un modelo en particular podrá desempeñarse con nuevos datos. En `Effective Dimension of Machine Learning Models <https://arxiv.org/pdf/2112.04807.pdf>`__, la dimensión efectiva **local** se propone como una nueva medida de capacidad que limita el error de generalización de los modelos de machine learning."

#: ../../tutorials/10_effective_dimension.ipynb:32
msgid "The key difference between global (``EffectiveDimension`` class) and **local** effective dimension (``LocalEffectiveDimension`` class) is actually not in the way they are computed, but in the nature of the parameter space that is analyzed. The global effective dimension incorporates the **full parameter space** of the model, and is calculated from a **large number of parameter (weight) sets**. On the other hand, the local effective dimension focuses on how well the **trained** model can generalize to new data, and how **expressive** it can be. Therefore, the local effective dimension is calculated from **a single** set of weight samples (training result). This difference is small in terms of practical implementation, but quite relevant at a conceptual level."
msgstr "La diferencia clave entre la dimensión efectiva global (clase ``EffectiveDimension``) y **local** (clase ``LocalEffectiveDimension``) no está en la forma en que se calculan, sino en la naturaleza del espacio de parámetros que se analiza. La dimensión efectiva global incorpora el **espacio de parámetros completo** del modelo y se calcula a partir de una **gran cantidad de conjuntos de parámetros (peso)**. Por otro lado, la dimensión efectiva local se enfoca en qué tan bien el modelo **entrenado** puede generalizar a nuevos datos y qué tan **expresiva** puede ser. Por lo tanto, la dimensión efectiva local se calcula a partir de **un solo** conjunto de muestras de peso (resultado del entrenamiento). Esta diferencia es pequeña en términos de implementación práctica, pero bastante relevante a nivel conceptual."

#: ../../tutorials/10_effective_dimension.ipynb:45
msgid "2. The Effective Dimension Algorithm"
msgstr "2. El Algoritmo de Dimensión Efectiva"

#: ../../tutorials/10_effective_dimension.ipynb:47
msgid "Both the global and local effective dimension algorithms use the Fisher Information matrix to provide a measure of complexity. The details on how this matrix is calculated are provided in the `reference paper <https://arxiv.org/pdf/2011.00027.pdf>`__, but in general terms, this matrix captures how sensitive a neural network’s output is to changes in the network’s parameter space."
msgstr "Los algoritmos de dimensión efectiva global y local utilizan la matriz de Información de Fisher para proporcionar una medida de la complejidad. Los detalles sobre cómo se calcula esta matriz se proporcionan en el `artículo de referencia <https://arxiv.org/pdf/2011.00027.pdf>`__, pero en términos generales, esta matriz captura qué tan sensible es la salida de una red neuronal a los cambios en el espacio de parámetros de la red."

#: ../../tutorials/10_effective_dimension.ipynb:49
msgid "In particular, this algorithm follows 4 main steps:"
msgstr "En particular, este algoritmo sigue 4 pasos principales:"

#: ../../tutorials/10_effective_dimension.ipynb:51
msgid "**Monte Carlo simulation:** the forward and backward passes (gradients) of the neural network are computed for each pair of input and weight samples."
msgstr "**Simulación de Monte Carlo:** los pasos hacia adelante y hacia atrás (gradientes) de la red neuronal se calculan para cada par de muestras de entrada y peso."

#: ../../tutorials/10_effective_dimension.ipynb:52
msgid "**Fisher Matrix Computation:** these outputs and gradients are used to compute the Fisher Information Matrix."
msgstr "**Cálculo de Matriz de Fisher:** estos resultados y gradientes se utilizan para calcular la Matriz de Información de Fisher."

#: ../../tutorials/10_effective_dimension.ipynb:53
msgid "**Fisher Matrix Normalization:** averaging over all input samples and dividing by the matrix trace"
msgstr "**Normalización de la Matriz de Fisher:** promediando todas las muestras de entrada y dividiendo por la traza de la matriz"

#: ../../tutorials/10_effective_dimension.ipynb:54
msgid "**Effective Dimension Calculation:** according to the formula from `Abbas et al. <https://arxiv.org/pdf/2011.00027.pdf>`__"
msgstr "**Cálculo de la Dimensión Efectiva:** según la fórmula de `Abbas et al. <https://arxiv.org/pdf/2011.00027.pdf>`__"

#: ../../tutorials/10_effective_dimension.ipynb:66
msgid "3. Basic Example (SamplerQNN)"
msgstr "3. Ejemplo Básico (SamplerQNN)"

#: ../../tutorials/10_effective_dimension.ipynb:68
msgid "This example shows how to set up a QNN model problem and run the global effective dimension algorithm. Both Qiskit ``SamplerQNN`` (shown in this example) and ``EstimatorQNN`` (shown in a later example) can be used with the ``EffectiveDimension`` class."
msgstr "Este ejemplo muestra cómo configurar un problema de modelo QNN y ejecutar el algoritmo de dimensión efectiva global. Tanto ``SamplerQNN`` de Qiskit (que se muestra en este ejemplo) como ``EstimatorQNN`` (que se muestra en un ejemplo posterior) se pueden usar con la clase ``EffectiveDimension``."

#: ../../tutorials/10_effective_dimension.ipynb:70
msgid "We start off from the required imports and a fixed seed for the random number generator for reproducibility purposes."
msgstr "Partimos con las importaciones requeridas y una semilla fija para el generador de números aleatorios con fines de reproducibilidad."

#: ../../tutorials/10_effective_dimension.ipynb:108
msgid "3.1 Define QNN"
msgstr "3.1 Definir la QNN"

#: ../../tutorials/10_effective_dimension.ipynb:110
msgid "The first step to create a ``SamplerQNN`` is to define a parametrized feature map and ansatz. In this toy example, we will use 3 qubits, and we will define the circuit used in the ``SamplerQNN`` class."
msgstr "El primer paso para crear una ``SamplerQNN`` es definir un mapa de características parametrizado y un ansatz. En este ejemplo de juguete, usaremos 3 qubits y definiremos el circuito usado en la clase ``SamplerQNN``."

#: ../../tutorials/10_effective_dimension.ipynb:150
msgid "The parametrized circuit can then be sent together with an optional interpret map (parity in this case) to the ``SamplerQNN`` constructor."
msgstr "El circuito parametrizado se puede enviar junto con un mapa de interpretación opcional (paridad en este caso) al constructor de ``SamplerQNN``."

#: ../../tutorials/10_effective_dimension.ipynb:195
msgid "3.2 Set up Effective Dimension calculation"
msgstr "3.2 Configurar el cálculo de la Dimensión Efectiva"

#: ../../tutorials/10_effective_dimension.ipynb:197
msgid "In order to compute the effective dimension of our QNN using the ``EffectiveDimension`` class, we need a series of sets of input samples and weights, as well as the total number of data samples available in a dataset. The ``input_samples`` and ``weight_samples`` are set in the class constructor, while the number of data samples is given during the call to the effective dimension computation, to be able to test and compare how this measure changes with different dataset sizes."
msgstr "Para calcular la dimensión efectiva de nuestra QNN usando la clase ``EffectiveDimension``, necesitamos una serie de conjuntos de muestras y pesos de entrada, así como el número total de muestras de datos disponibles en un conjunto de datos. Las ``input_samples`` y ``weight_samples`` se establecen en el constructor de la clase, mientras que el número de muestras de datos se proporciona durante la llamada al cálculo de la dimensión efectiva, para poder probar y comparar cómo cambia esta medida con diferentes tamaños de conjuntos de datos."

#: ../../tutorials/10_effective_dimension.ipynb:208
msgid "We can define the number of input samples and weight samples and the class will randomly sample a corresponding array from a normal (for ``input_samples``) or a uniform (for ``weight_samples``) distribution. Instead of passing a number of samples we can pass an array, sampled manually."
msgstr "Podemos definir el número de muestras de entrada y muestras de peso y la clase muestreará aleatoriamente un arreglo correspondiente a una distribución normal (para ``input_samples``) o uniforme (para ``weight_samples``). En lugar de pasar una cantidad de muestras, podemos pasar un arreglo, muestreado manualmente."

#: ../../tutorials/10_effective_dimension.ipynb:235
msgid "If we want to test a specific set of input samples and weight samples, we can provide it directly to the ``EffectiveDimension`` class as shown in the following snippet:"
msgstr "Si queremos probar un conjunto específico de muestras de entrada y muestras de peso, podemos proporcionarlo directamente a la clase ``EffectiveDimension`` como se muestra en el siguiente fragmento de código:"

#: ../../tutorials/10_effective_dimension.ipynb:260
msgid "The effective dimension algorithm also requires a dataset size. In this example, we will define an array of sizes to later see how this input affects the result."
msgstr "El algoritmo de dimensión efectiva también requiere un tamaño de conjunto de datos. En este ejemplo, definiremos un arreglo de tamaños para luego ver cómo esta entrada afecta el resultado."

#: ../../tutorials/10_effective_dimension.ipynb:283
msgid "3.3 Compute Global Effective Dimension"
msgstr "3.3 Calcular la Dimensión Efectiva Global"

#: ../../tutorials/10_effective_dimension.ipynb:285
msgid "Let’s now calculate the effective dimension of our network for the previously defined set of input samples, weights, and a dataset size of 5000."
msgstr "Ahora calculemos la dimensión efectiva de nuestra red para el conjunto previamente definido de muestras de entrada, pesos y un tamaño de conjunto de datos de 5000."

#: ../../tutorials/10_effective_dimension.ipynb:306
msgid "The effective dimension values will range between 0 and ``d``, where ``d`` represents the dimension of the model, and it’s practically obtained from the number of weights of the QNN. By dividing the result by ``d``, we can obtain the normalized effective dimension, which correlates directly with the capacity of the model."
msgstr "Los valores efectivos de dimensión oscilarán entre 0 y ``d``, donde ``d`` representa la dimensión del modelo, y se obtiene prácticamente a partir del número de pesos de la QNN. Al dividir el resultado entre ``d``, podemos obtener la dimensión efectiva normalizada, que se correlaciona directamente con la capacidad del modelo."

#: ../../tutorials/10_effective_dimension.ipynb:359
msgid "By calling the ``EffectiveDimension`` class with an array if input sizes ``n``, we can monitor how the effective dimension changes with the dataset size."
msgstr "Al llamar a la clase ``EffectiveDimension`` con un arreglo de tamaños de entrada ``n``, podemos monitorear cómo cambia la dimensión efectiva con el tamaño del conjunto de datos."

#: ../../tutorials/10_effective_dimension.ipynb:444
msgid "4. Local Effective Dimension Example"
msgstr "4. Ejemplo de Dimensión Efectiva Local"

#: ../../tutorials/10_effective_dimension.ipynb:446
msgid "As explained in the introduction, the local effective dimension algorithm only uses **one** set of weights, and it can be used to monitor how training affects the expressiveness of a neural network. The ``LocalEffectiveDimension`` class enforces this constraint to ensure that these calculations are conceptually separate, but the rest of the implementation is shared with ``EffectiveDimension``."
msgstr "Como se explicó en la introducción, el algoritmo de dimensión efectiva local solo usa **un** conjunto de pesos y se puede usar para monitorear cómo el entrenamiento afecta la expresividad de una red neuronal. La clase ``LocalEffectiveDimension`` impone esta restricción para garantizar que estos cálculos sean conceptualmente independientes, pero el resto de la implementación se comparte con ``EffectiveDimension``."

#: ../../tutorials/10_effective_dimension.ipynb:448
msgid "This example shows how to leverage the ``LocalEffectiveDimension`` class to analyze the effect of training on QNN expressiveness."
msgstr "Este ejemplo muestra cómo aprovechar la clase ``LocalEffectiveDimension`` para analizar el efecto del entrenamiento en la expresividad de la QNN."

#: ../../tutorials/10_effective_dimension.ipynb:460
msgid "4.1 Define Dataset and QNN"
msgstr "4.1 Definir el Conjunto de Datos y la QNN"

#: ../../tutorials/10_effective_dimension.ipynb:462
msgid "We start by creating a 3D binary classification dataset using ``make_classification`` function from scikit-learn."
msgstr "Comenzamos creando un conjunto de datos de clasificación binaria 3D utilizando la función ``make_classification`` de scikit-learn."

#: ../../tutorials/10_effective_dimension.ipynb:495
msgid "The next step is to create a QNN, an instance of ``EstimatorQNN`` in our case in the same fashion we created an instance of ``SamplerQNN``."
msgstr "El siguiente paso es crear una QNN, una instancia de ``EstimatorQNN`` en nuestro caso de la misma manera que creamos una instancia de ``SamplerQNN``."

#: ../../tutorials/10_effective_dimension.ipynb:519
msgid "4.2 Train QNN"
msgstr "4.2 Entrenar la QNN"

#: ../../tutorials/10_effective_dimension.ipynb:521
msgid "We can now proceed to train the QNN. The training step may take some time, be patient. You can pass a callback to the classifier to observe how the training process is going on. We fix ``initial_point`` for reproducibility purposes as usual."
msgstr "Ahora podemos proceder a entrenar la QNN. El paso de entrenamiento puede tomar algún tiempo, sé paciente. Puedes pasar una devolución de llamada al clasificador para observar cómo se desarrolla el proceso de entrenamiento. Fijamos ``initial_point`` con fines de reproducibilidad, como de costumbre."

#: ../../tutorials/10_effective_dimension.ipynb:594
msgid "The classifier can now differentiate between classes with an accuracy of:"
msgstr "El clasificador ahora puede diferenciar entre clases con una precisión de:"

#: ../../tutorials/10_effective_dimension.ipynb:642
msgid "4.3 Compute Local Effective Dimension of trained QNN"
msgstr "4.3 Calcular la Dimensión Efectiva Local de QNN entrenadas"

#: ../../tutorials/10_effective_dimension.ipynb:644
msgid "Now that we have trained our network, let’s evaluate the local effective dimension based on the trained weights. To do that we access the trained weights directly from the classifier."
msgstr "Ahora que hemos entrenado nuestra red, evaluemos la dimensión efectiva local en función de los pesos entrenados. Para ello accedemos a los pesos entrenados directamente desde el clasificador."

#: ../../tutorials/10_effective_dimension.ipynb:705
msgid "4.4 Compute Local Effective Dimension of untrained QNN"
msgstr "4.4 Calcular la Dimensión Efectiva Local de QNN no entrenadas"

#: ../../tutorials/10_effective_dimension.ipynb:707
msgid "We can compare this result with the effective dimension of the untrained network, using the ``initial_point`` as our weight sample:"
msgstr "Podemos comparar este resultado con la dimensión efectiva de la red no entrenada, usando el ``initial_point`` como nuestra muestra de peso:"

#: ../../tutorials/10_effective_dimension.ipynb:766
msgid "4.5 Plot and analyze results"
msgstr "4.5 Graficar y analizar resultados"

#: ../../tutorials/10_effective_dimension.ipynb:768
msgid "If we plot the effective dimension values before and after training, we can see the following result:"
msgstr "Si graficamos los valores de las dimensiones efectivas antes y después del entrenamiento, podemos ver el siguiente resultado:"

#: ../../tutorials/10_effective_dimension.ipynb:806
msgid "In general, we should expect the value of the local effective dimension to decrease after training. This can be understood by looking back into the main goal of machine learning, which is to pick a model that is expressive enough to fit your data, but not too expressive that it overfits and performs badly on new data samples."
msgstr "En general, deberíamos esperar que el valor de la dimensión efectiva local disminuya después del entrenamiento. Esto se puede entender mirando hacia atrás en el objetivo principal del machine learning, que es elegir un modelo que sea lo suficientemente expresivo como para ajustarse a tus datos, pero no demasiado expresivo como para que se sobreajuste y funcione mal en nuevas muestras de datos."

#: ../../tutorials/10_effective_dimension.ipynb:808
msgid "Certain optimizers help regularize the overfitting of a model by learning parameters, and this action of learning inherently reduces a model’s expressiveness, as measured by the local effective dimension. Following this logic, a randomly initialized parameter set will most likely produce a higher effective dimension that the final set of trained weights, because that model with that particular parameterization is “using more parameters” unnecessarily to fit the data. After training (with the implicit regularization), a trained model will not need to use so many parameters and thus have more “inactive parameters” and a lower effective dimension."
msgstr "Ciertos optimizadores ayudan a regularizar el sobreajuste de un modelo mediante el aprendizaje de parámetros, y esta acción de aprendizaje reduce inherentemente la expresividad de un modelo, medida por la dimensión efectiva local. Siguiendo esta lógica, lo más probable es que un conjunto de parámetros inicializados aleatoriamente produzca una dimensión efectiva más alta que el conjunto final de pesos entrenados, porque ese modelo con esa parametrización particular está “usando más parámetros” innecesariamente para ajustar los datos. Después del entrenamiento (con la regularización implícita), un modelo entrenado no necesitará usar tantos parámetros y, por lo tanto, tendrá más “parámetros inactivos” y una dimensión efectiva menor."

#: ../../tutorials/10_effective_dimension.ipynb:811
msgid "We must keep in mind though that this is the general intuition, and there might be cases where a randomly selected set of weights happens to provide a lower effective dimension than the trained weights for a specific model."
msgstr "Sin embargo, debemos tener en cuenta que esta es la intuición general, y puede haber casos en los que un conjunto de pesos seleccionados al azar proporcione una dimensión efectiva más baja que los pesos entrenados para un modelo específico."

