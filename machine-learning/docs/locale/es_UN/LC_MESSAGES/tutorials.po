msgid ""
msgstr ""
"Project-Id-Version: qiskit-docs\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-06-29 14:17+0000\n"
"PO-Revision-Date: 2021-07-02 07:12\n"
"Last-Translator: \n"
"Language-Team: Spanish (United)\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: qiskit-docs\n"
"X-Crowdin-Project-ID: 369271\n"
"X-Crowdin-Language: es-un\n"
"X-Crowdin-File: /master/machine-learning/docs/locale/en/LC_MESSAGES/tutorials.po\n"
"X-Crowdin-File-ID: 9528\n"
"Language: es_UN\n"

#: ../../tutorials/01_neural_networks.ipynb:13
#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:13
#: ../../tutorials/03_quantum_kernel.ipynb:13
#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:13
#: ../../tutorials/05_torch_connector.ipynb:13
msgid "Run interactively in jupyter notebook."
msgstr "Ejecutar de forma interactiva en jupyter notebook."

#: ../../tutorials/01_neural_networks.ipynb:9
msgid "Quantum Neural Networks"
msgstr "Redes Neuronales Cuánticas"

#: ../../tutorials/01_neural_networks.ipynb:11
msgid "This notebook demonstrates the different generic quantum neural network (QNN) implementations provided in Qiskit Machine Learning. The networks are meant as application-agnostic computational units that can be used for many different use cases. Depending on the application, a particular type of network might more or less suitable and might require to be set up in a particular way. The following different available neural networks will now be discussed in more detail:"
msgstr "Este cuaderno muestra las diferentes implementaciones genéricas de redes neuronales cuánticas (quantum neural network, QNN) proporcionadas en Qiskit Machine Learning. Las redes están pensadas como unidades computacionales independientes de la aplicación en las que se pueden utilizar para muchos casos de uso diferentes. Dependiendo de la aplicación, un tipo particular de red puede ser más o menos adecuado y puede ser necesario configurarlo de una manera particular. Las siguientes redes neuronales disponibles diferentes se discutirán ahora con más detalle:"

#: ../../tutorials/01_neural_networks.ipynb:13
msgid "``NeuralNetwork``: The interface for neural networks."
msgstr "``NeuralNetwork``: La interfaz para redes neuronales."

#: ../../tutorials/01_neural_networks.ipynb:14
msgid "``OpflowQNN``: A network based on the evaluation of quantum mechanical observables."
msgstr "``OpflowQNN``: Una red basada en la evaluación de observables de la mecánica cuántica."

#: ../../tutorials/01_neural_networks.ipynb:15
msgid "``TwoLayerQNN``: A special ``OpflowQNN`` implementation for convenience."
msgstr "``TwoLayerQNN``: Una implementación especial de ``OpflowQNN`` para mayor conveniencia."

#: ../../tutorials/01_neural_networks.ipynb:16
msgid "``CircuitQNN``: A network based on the samples resulting from measuring a quantum circuit."
msgstr "``CircuitQNN``: Una red basada en los ejemplos resultantes de medir un circuito cuántico."

#: ../../tutorials/01_neural_networks.ipynb:64
msgid "1. ``NeuralNetwork``"
msgstr "1. ``NeuralNetwork``"

#: ../../tutorials/01_neural_networks.ipynb:66
msgid "The ``NeuralNetwork`` represents the interface for all neural networks available in Qiskit Machine Learning. It exposes a forward and a backward pass taking the data samples and trainable weights as input. A ``NeuralNetwork`` does not contain any training capabilities, these are pushed to the actual algorithms / applications. Thus, a ``NeuralNetwork`` also does not store the values for trainable weights. In the following, different implementations of this interfaces are introduced."
msgstr "El ``NeuralNetwork`` representa la interface para todas las redes neuronales disponibles en Qiskit Machine Learning. Este muestra un pase hacia adelante y un pase hacia atrás tomando las muestras y ponderaciones entrenables como entrada. Un ``NeuralNetwork`` no contiene ninguna capacidad de entrenamiento, esto se le asigna a los algoritmos y a las aplicaciones. Por ello, un ``NeuralNetwork`` tampoco guarda los valores de las ponderaciones entrenables. Enseguida, se introducen las diferentes implementaciones de estas interfaces."

#: ../../tutorials/01_neural_networks.ipynb:68
msgid "Suppose a ``NeuralNetwork`` called ``nn``. Then, the ``nn.forward(input, weights)`` pass takes either flat inputs for the data and weights of size ``nn.num_inputs`` and ``nn.num_weights``, respectively. ``NeuralNetwork`` supports batching of inputs and returns batches of output of the corresponding shape."
msgstr "Supongamos que tenemos un ``NeuralNetwork`` llamado ``nn``. A continuación, el paso ``nn.forward (input, weights)`` toma cualquiera de las entradas planas para los datos y las ponderaciones del tamaño ``nn.num_inputs`` y ``nn.num_weights``, respectivamente. ``NeuralNetwork`` es compatible con la carga de entradas y devuelve lotes de salida de la forma correspondiente."

#: ../../tutorials/01_neural_networks.ipynb:80
msgid "2. ``OpflowQNN``"
msgstr "2. ``OpflowQNN``"

#: ../../tutorials/01_neural_networks.ipynb:82
msgid "The ``OpflowQNN`` takes a (parametrized) operator from Qiskit and leverages Qiskit’s gradient framework to provide the backward pass. Such an operator can for instance be an expected value of a quantum mechanical observable with respect to a parametrized quantum state. The Parameters can be used to load classical data as well as represent trainable weights. The ``OpflowQNN`` also allows lists of operators and more complex structures to construct more complex QNNs."
msgstr ""

#: ../../tutorials/01_neural_networks.ipynb:321
msgid "Combining multiple observables in a ``ListOp`` also allows to create more complex QNNs"
msgstr "La combinación de multiples observables en un ``ListOp`` también permite crear QNNs más complejos"

#: ../../tutorials/01_neural_networks.ipynb:412
msgid "3. ``TwoLayerQNN``"
msgstr "3. ``TwoLayerQNN``"

#: ../../tutorials/01_neural_networks.ipynb:414
msgid "The ``TwoLayerQNN`` is a special ``OpflowQNN`` on :math:`n` qubits that consists of first a feature map to insert data and second an ansatz that is trained. The default observable is :math:`Z^{\\otimes n}`, i.e., parity."
msgstr ""

#: ../../tutorials/01_neural_networks.ipynb:612
msgid "4. ``CircuitQNN``"
msgstr "4. ``CircuitQNN``"

#: ../../tutorials/01_neural_networks.ipynb:614
msgid "The ``CircuitQNN`` is based on a (parametrized) ``QuantumCircuit``. This can take input as well as weight parameters and produces samples from the measurement. The samples can either be interpreted as probabilities of measuring the integer index corresponding to a bitstring or directly as a batch of binary output. In the case of probabilities, gradients can be estimated efficiently and the ``CircuitQNN`` provides a backward pass as well. In case of samples, differentiation is not possible and the backward pass returns ``(None, None)``."
msgstr ""

#: ../../tutorials/01_neural_networks.ipynb:617
msgid "Further, the ``CircuitQNN`` allows to specify an ``interpret`` function to post-process the samples. This is expected to take a measured integer (from a bitstring) and map it to a new index, i.e. non-negative integer. In this case, the output shape needs to be provided and the probabilities are agregated accordingly."
msgstr "Además, el ``CircuitQNN`` permite especificar una función ``interpret`` para procesar las muestras. Se espera que esto tome un entero medido (de una cadena de bits) y que se mapee con un nuevo índice, es decir, un entero no negativo. En este caso, es necesario proporcionar la forma de la salida y las probabilidades se agregan en consecuencia."

#: ../../tutorials/01_neural_networks.ipynb:619
msgid "A ``CircuitQNN`` can be configured to return sparse as well as dense probability vectors. If no ``interpret`` function is used, the dimension of the probability vector scales exponentially with the number of qubits and a sparse recommendation is usually recommended. In case of an ``interpret`` function it depends on the expected outcome. If, for instance, an index is mapped to the parity of the corresponding bitstring, i.e., to 0 or 1, a dense output makes sense and the result will be a probability vector of length 2."
msgstr "Un ``CircuitQNN``puede ser configurado para devolver los vectores de probabilidad dispersos, así como densos. Si no se utiliza la función ``interpret``, la dimensión del vector de probabilidad escala exponencialmente con el número de qubits y normalmente se sugiere una recomendación dispersa. En caso de que una función ``interpret`` se utilice, va a depender del resultado esperado. Si, por ejemplo, un índice se correlaciona con la paridad de la cadena de bits correspondiente, es decir, a 0 o 1, una salida densa tiene sentido y el resultado será un vector de probabilidad de longitud 2."

#: ../../tutorials/01_neural_networks.ipynb:662
msgid "4.1 Output: sparse integer probabilities"
msgstr "4.1 Salida: probabilidades de enteros dispersos"

#: ../../tutorials/01_neural_networks.ipynb:761
msgid "4.2 Output: dense parity probabilities"
msgstr "4.2 Salida: probabilidades de paridad densa"

#: ../../tutorials/01_neural_networks.ipynb:869
msgid "4.3 Output: Samples"
msgstr "4.3 Salida: Muestras"

#: ../../tutorials/01_neural_networks.ipynb:985
msgid "4.4 Output: Parity Samples"
msgstr "4.4 Salida: Muestras de Paridad"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:9
msgid "Neural Network Classifier & Regressor"
msgstr "Clasificador y Regresor de Redes Neuronales"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:11
msgid "In this tutorial we show how the ``NeuralNetworkClassifier`` and ``NeuralNetworkRegressor`` are used. Both take as an input a (Quantum) ``NeuralNetwork`` and leverage it in a specific context. In both cases we also provide a pre-configured variant for convenience, the Variational Quantum Classifier (``VQC``) and Variational Quantum Regressor (``VQR``). The tutorial is structured as follows:"
msgstr "En este tutorial mostramos cómo se utiliza el ``NeuralNetworkClassifier`` y el ``NeuralNetworkRegressor``. Ambos toman como entrada una ``NeuralNetwork`` (Cuántica) y la aprovechan en un contexto específico. En ambos casos también ofrecemos una variante preconfigurada para mayor comodidad, el Clasificador Cuántico Variacional (Variational Quantum Classifier, ``VQC``) y el Regresor Cuántico Variacional (Variational Quantum Regressor, ``VQR``). El tutorial está estructurado de la siguiente manera:"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:13
msgid "`Classification <#Classification>`__"
msgstr "`Clasificación <#Classification>`__"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:15
msgid "Classification with an ``OpflowQNN``"
msgstr "Clasificación con un ``OpflowQNN``"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:16
#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:249
msgid "Classification with a ``CircuitQNN``"
msgstr "Clasificación con un ``CircuitQNN``"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:17
#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:398
msgid "Variational Quantum Classifier (``VQC``)"
msgstr "Clasificador Cuántico Variacional (Variational Quantum Classifier, ``VQC``)"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:19
msgid "`Regression <#Regression>`__"
msgstr "`Regresión <#Regression>`__"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:21
#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:539
msgid "Regression with an ``OpflowQNN``"
msgstr "Regresión con un ``OpflowQNN``"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:22
msgid "Variational Quantum Regressor (``VQR``)"
msgstr "Regresor Cuántico Variacional (Variational Quantum Regressor, ``VQR``)"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:70
#: ../../tutorials/03_quantum_kernel.ipynb:53
#: ../../tutorials/05_torch_connector.ipynb:69
msgid "Classification"
msgstr "Clasificación"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:72
msgid "We prepare a simple classification dataset to illustrate the following algorithms."
msgstr "Preparamos un conjunto de datos de clasificación simple para ilustrar los siguientes algoritmos."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:117
msgid "Classification with the an ``OpflowQNN``"
msgstr "Clasificación con un ``OpflowQNN``"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:119
msgid "First we show how an ``OpflowQNN`` can be used for classification within a ``NeuralNetworkClassifier``. In this context, the ``OpflowQNN`` is expected to return one-dimensional output in :math:`[-1, +1]`. This only works for binary classification and we assign the two classes to :math:`\\{-1, +1\\}`. For convenience, we use the ``TwoLayerQNN``, which is a special type of ``OpflowQNN`` defined via a feature map and an ansatz."
msgstr "Primero mostramos cómo se puede usar un ``OpflowQNN`` para la clasificación dentro de un ``NeuralNetworkClassifier``. En este contexto, se espera que el ``OpflowQNN`` devuelva una salida unidimensional en :math:`[-1, +1]`. Esto solo funciona para la clasificación binaria y asignamos las dos clases a :math:`\\{-1, +1\\}`. Para mayor comodidad, utilizamos el ``TwoLayerQNN``, que es un tipo especial de ``OpflowQNN`` definido a través de un mapa de características y un ansatz."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:251
msgid "Next we show how a ``CircuitQNN`` can be used for classification within a ``NeuralNetworkClassifier``. In this context, the ``CircuitQNN`` is expected to return :math:`d`-dimensional probability vector as output, where :math:`d` denotes the number of classes. Sampling from a ``QuantumCircuit`` automatically results in a probability distribution and we just need to define a mapping from the measured bitstrings to the different classes. For binary classification we use the parity mapping."
msgstr "A continuación, mostramos cómo se puede utilizar un ``CircuitQNN`` para la clasificación dentro de un ``NeuralNetworkClassifier``. En este contexto, se espera que el ``CircuitQNN`` devuelva un vector de probabilidad :math:`d`-dimensional como salida, donde :math:`d` denota el número de clases. El muestreo de un ``QuantumCircuit`` da como resultado automáticamente una distribución de probabilidad y solo necesitamos definir un mapeo de las cadenas de bits medidas a las diferentes clases. Para la clasificación binaria usamos el mapeo de paridad."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:400
msgid "The ``VQC`` is a special variant of the ``NeuralNetworkClassifier`` with a ``CircuitQNN``. It applies a parity mapping (or extensions to multiple classes) to map from the bitstring to the classification, which results in a probability vector, which is interpreted as a one-hot encoded result. By default, it applies this the ``CrossEntropyLoss`` function that expects labels given in one-hot encoded format and will return predictions in that format too."
msgstr "El ``VQC`` es una variante especial del ``NeuralNetworkClassifier`` con un ``CircuitQNN``. Aplica un mapeo de paridad (o extensiones a múltiples clases) para mapear desde la cadena de bits hasta la clasificación, lo que da como resultado un vector de probabilidad, que se interpreta como un resultado codificado one-hot. De forma predeterminada, aplica esta función ``CrossEntropyLoss`` que espera que las etiquetas se proporcionen en un formato codificado one-hot y también devolverá predicciones en ese formato."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:496
#: ../../tutorials/05_torch_connector.ipynb:524
msgid "Regression"
msgstr "Regresión"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:498
msgid "We prepare a simple regression dataset to illustrate the following algorithms."
msgstr "Preparamos un conjunto de datos de regresión simple para ilustrar los siguientes algoritmos."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:541
msgid "Here we restrict to regression with an ``OpflowQNN`` that returns values in :math:`[-1, +1]`. More complex and also multi-dimensional models could be constructed, also based on ``CircuitQNN`` but that exceeds the scope of this tutorial."
msgstr "Aquí restringimos la regresión con un ``OpflowQNN`` que devuelve valores entre :math:`[-1, +1]`. Se podrían construir modelos más complejos y también multidimensionales, también basados en ``CircuitQNN`` pero eso excede el alcance de este tutorial."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:648
msgid "Regression with the Variational Quantum Regressor (``VQR``)"
msgstr "Regresión con el Regresor Cuántico Variacional (Variational Quantum Regressor, ``VQR``)"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:650
msgid "Similar to the ``VQC`` for classification, the ``VQR`` is a special variant of the ``NeuralNetworkRegressor`` with a ``OpflowQNN``. By default it considers the ``L2Loss`` function to minimize the mean squared error between predictions and targets."
msgstr "Similar al ``VQC`` para la clasificación, el ``VQR`` es una variante especial del ``NeuralNetworkRegressor`` con un ``OpflowQNN``. De forma predeterminada, considera la función ``L2Loss`` para minimizar el error cuadrático medio entre las predicciones y los objetivos."

#: ../../tutorials/03_quantum_kernel.ipynb:9
msgid "Quantum Kernel Machine Learning"
msgstr "Machine Learning con Kernel Cuántico"

#: ../../tutorials/03_quantum_kernel.ipynb:11
msgid "The general task of machine learning is to find and study patterns in data. For many datasets, the datapoints are better understood in a higher dimensional feature space, through the use of a kernel function: :math:`k(\\vec{x}_i, \\vec{x}_j) = \\langle f(\\vec{x}_i), f(\\vec{x}_j) \\rangle` where :math:`k` is the kernel function, :math:`\\vec{x}_i, \\vec{x}_j` are :math:`n` dimensional inputs, :math:`f` is a map from :math:`n`-dimension to :math:`m`-dimension space and :math:`\\langle a,b \\rangle` denotes the dot product. When considering finite data, a kernel function can be represented as a matrix: :math:`K_{ij} = k(\\vec{x}_i,\\vec{x}_j)`."
msgstr ""

#: ../../tutorials/03_quantum_kernel.ipynb:14
msgid "In quantum kernel machine learning, a quantum feature map :math:`\\phi(\\vec{x})` is used to map a classical feature vector :math:`\\vec{x}` to a quantum Hilbert space, :math:`| \\phi(\\vec{x})\\rangle \\langle \\phi(\\vec{x})|`, such that :math:`K_{ij} = \\left| \\langle \\phi^\\dagger(\\vec{x}_j)| \\phi(\\vec{x}_i) \\rangle \\right|^{2}`. See `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__ for more details."
msgstr "En machine learning con kernel cuántico, se utiliza un mapa de características cuánticas :math:`\\phi(\\vec{x})` para asignar un vector de características clásicas :math:`\\vec{x}` a un espacio cuántico de Hilbert,:math:`| \\phi(\\vec{x})\\rangle \\langle \\phi(\\vec{x})|`, tal que :math:`K_{ij} = \\left| \\langle \\phi^\\dagger(\\vec{x}_j)| \\phi(\\vec{x}_i) \\rangle \\right|^{2}`. Consulte `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__ para obtener más detalles."

#: ../../tutorials/03_quantum_kernel.ipynb:16
msgid "In this notebook, we use ``qiskit`` to calculate a kernel matrix using a quantum feature map, then use this kernel matrix in ``scikit-learn`` classification and clustering algorithms."
msgstr "En este cuaderno, usamos ``qiskit`` para calcular una matriz de kernel usando un mapa de características cuánticas, luego usamos esta matriz de kernel en algoritmos de clasificación y agrupamiento de ``scikit-learn``."

#: ../../tutorials/03_quantum_kernel.ipynb:55
msgid "For our classification example, we will use the *ad hoc dataset* as described in `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__, and the ``scikit-learn`` `support vector machine <https://scikit-learn.org/stable/modules/svm.html>`__ classification (``svc``) algorithm."
msgstr "Para nuestro ejemplo de clasificación, usaremos el *dataset ad hoc* como se describe en `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__, y el algoritmo de clasificación `support vector machine <https://scikit-learn.org/stable/modules/svm.html>`__ (``svc``) de ``scikit-learn``."

#: ../../tutorials/03_quantum_kernel.ipynb:111
msgid "With our training and testing datasets ready, we set up the ``QuantumKernel`` class to calculate a kernel matrix using the `ZZFeatureMap <https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZZFeatureMap.html>`__, and the ``BasicAer`` ``qasm_simulator`` using 1024 shots."
msgstr "Con nuestros conjuntos de datos de entrenamiento y prueba listos, configuramos la clase ``QuantumKernel`` para calcular una matriz de kernel usando el `ZZFeatureMap <https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZZFeatureMap.html>`__, y el ``qasm_simulator`` de ``BasicAer``usando 1024 iteraciones."

#: ../../tutorials/03_quantum_kernel.ipynb:138
msgid "The ``scikit-learn`` ``svc`` algorithm allows us to define a `custom kernel <https://scikit-learn.org/stable/modules/svm.html#custom-kernels>`__ in two ways: by providing the kernel as a callable function or by precomputing the kernel matrix. We can do either of these using the ``QuantumKernel`` class in ``qiskit``."
msgstr "El algoritmo ``svc`` de ``scikit-learn`` nos permite definir un `custom kernel <https://scikit-learn.org/stable/modules/svm.html#custom-kernels>`__ de dos maneras: proporcionando el kernel como una función invocable o precomputando la matriz del kernel. Podemos hacer cualquiera de estos usando la clase ``QuantumKernel`` en ``qiskit``."

#: ../../tutorials/03_quantum_kernel.ipynb:140
msgid "The following code gives the kernel as a callable function:"
msgstr ""

#: ../../tutorials/03_quantum_kernel.ipynb:184
msgid "The following code precomputes and plots the training and testing kernel matrices before providing them to the ``scikit-learn`` ``svc`` algorithm:"
msgstr "El siguiente código precalcula y traza las matrices del kernel de entrenamiento y prueba antes de proporcionarlas al algoritmo ``svc`` de ``scikit-learn``:"

#: ../../tutorials/03_quantum_kernel.ipynb:250
msgid "``qiskit`` also contains the ``qsvc`` class that extends the ``sklearn svc`` class, that can be used as follows:"
msgstr "``qiskit`` también contiene la clase ``qsvc`` que extiende la clase ``sklearn svc`, que se puede usar de la siguiente manera:"

#: ../../tutorials/03_quantum_kernel.ipynb:295
msgid "Clustering"
msgstr "Agrupación (Clustering)"

#: ../../tutorials/03_quantum_kernel.ipynb:297
msgid "For our clustering example, we will again use the *ad hoc dataset* as described in `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__, and the ``scikit-learn`` ``spectral`` clustering algorithm."
msgstr "Para nuestro ejemplo de agrupamiento, usaremos nuevamente el *dataset ad hoc* como se describe en `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__, y el algoritmo de agrupación ``spectral`` de ``scikit-learn``."

#: ../../tutorials/03_quantum_kernel.ipynb:299
msgid "We will regenerate the dataset with a larger gap between the two classes, and as clustering is an unsupervised machine learning task, we don’t need a test sample."
msgstr "Regeneraremos el conjunto de datos con una brecha mayor entre las dos clases y, como la agrupación en clústeres es una tarea de machine learning no supervisada, no necesitamos una muestra de prueba."

#: ../../tutorials/03_quantum_kernel.ipynb:350
msgid "We again set up the ``QuantumKernel`` class to calculate a kernel matrix using the `ZZFeatureMap <https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZZFeatureMap.html>`__, and the BasicAer ``qasm_simulator`` using 1024 shots."
msgstr "Nuevamente configuramos la clase ``QuantumKernel`` para calcular una matriz de kernel usando el `ZZFeatureMap <https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZZFeatureMap.html>`__, y el ``qasm_simulator`` de BasicAer usando 1024 iteraciones."

#: ../../tutorials/03_quantum_kernel.ipynb:377
msgid "The scikit-learn spectral clustering algorithm allows us to define a [custom kernel] in two ways: by providing the kernel as a callable function or by precomputing the kernel matrix. Using the QuantumKernel class in qiskit, we can only use the latter."
msgstr "El algoritmo de agrupamiento espectral scikit-learn nos permite definir un [kernel personalizado] de dos formas: proporcionando el kernel como una función invocable o precomputando la matriz del kernel. Usando la clase QuantumKernel en qiskit, solo podemos usar este último."

#: ../../tutorials/03_quantum_kernel.ipynb:379
msgid "The following code precomputes and plots the kernel matrices before providing it to the scikit-learn spectral clustering algorithm, and scoring the labels using normalized mutual information, since we apriori know the class labels."
msgstr "El siguiente código precalcula y traza las matrices del kernel antes de proporcionarlo al algoritmo de agrupamiento espectral scikit-learn y puntuar las etiquetas utilizando información mutua normalizada, ya que conocemos las etiquetas de clase a priori."

#: ../../tutorials/03_quantum_kernel.ipynb:439
msgid "``scikit-learn`` has other algorithms that can use a precomputed kernel matrix, here are a few:"
msgstr "``scikit-learn`` tiene otros algoritmos que pueden usar una matriz de kernel precalculada, aquí hay algunos:"

#: ../../tutorials/03_quantum_kernel.ipynb:441
msgid "`Agglomerative clustering <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html>`__"
msgstr "`Agglomerative clustering <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html>`__"

#: ../../tutorials/03_quantum_kernel.ipynb:442
msgid "`Support vector regression <https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html>`__"
msgstr "`Support vector regression <https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html>`__"

#: ../../tutorials/03_quantum_kernel.ipynb:443
msgid "`Ridge regression <https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html>`__"
msgstr "`Ridge regression <https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html>`__"

#: ../../tutorials/03_quantum_kernel.ipynb:444
msgid "`Guassian process regression <https://scikit-learn.org/stable/modules/gaussian_process.html>`__"
msgstr "`Guassian process regression <https://scikit-learn.org/stable/modules/gaussian_process.html>`__"

#: ../../tutorials/03_quantum_kernel.ipynb:445
msgid "`Principal component analysis <https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html>`__"
msgstr "`Principal component analysis <https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html>`__"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:9
msgid "qGANs for Loading Random Distributions"
msgstr "qGANs para Cargar Distribuciones Aleatorias"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:11
msgid "Given :math:`k`-dimensional data samples, we employ a quantum Generative Adversarial Network (qGAN) to learn the data’s underlying random distribution and to load it directly into a quantum state:"
msgstr "Dadas muestras de datos :math:`k`-dimensionales, empleamos una Red Generativa Antagónica cuántica (quantum Generative Adversarial Network, qGAN) para conocer la distribución aleatoria subyacente de los datos y cargarlos directamente en un estado cuántico:"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:13
msgid "\\big| g_{\\theta}\\rangle = \\sum_{j=0}^{2^n-1} \\sqrt{p_{\\theta}^{j}}\\big| j \\rangle\n\n"
msgstr "\\big| g_{\\theta}\\rangle = \\sum_{j=0}^{2^n-1} \\sqrt{p_{\\theta}^{j}}\\big| j \\rangle\n\n"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:15
msgid "where :math:`p_{\\theta}^{j}` describe the occurrence probabilities of the basis states :math:`\\big| j\\rangle`."
msgstr "donde :math:`p_{\\theta}^{j}` describen las probabilidades de ocurrencia de los estados base :math:`\\big| j\\rangle`."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:17
msgid "The aim of the qGAN training is to generate a state :math:`\\big| g_{\\theta}\\rangle` where :math:`p_{\\theta}^{j}`, for :math:`j\\in \\left\\{0, \\ldots, {2^n-1} \\right\\}`, describe a probability distribution that is close to the distribution underlying the training data :math:`X=\\left\\{x^0, \\ldots, x^{k-1} \\right\\}`."
msgstr "El objetivo del entrenamiento qGAN es generar un estado :math:`\\big| g_{\\theta}\\rangle` donde :math:`p_{\\theta}^{j}`, para :math:`j\\in \\left\\{0, \\ldots, {2^n-1} \\right\\}`, describe una distribución de probabilidad cercana a la distribución subyacente a los datos de entrenamiento :math:`X=\\left\\{x^0, \\ldots, x^{k-1} \\right\\}`."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:19
msgid "For further details please refer to `Quantum Generative Adversarial Networks for Learning and Loading Random Distributions <https://arxiv.org/abs/1904.00043>`__ *Zoufal, Lucchi, Woerner* [2019]."
msgstr "Para obtener más detalles, consulta `Quantum Generative Adversarial Networks for Learning and Loading Random Distributions <https://arxiv.org/abs/1904.00043>`__ *Zoufal, Lucchi, Woerner* [2019]."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:21
msgid "For an example of how to use a trained qGAN in an application, the pricing of financial derivatives, please see the `Option Pricing with qGANs <https://github.com/Qiskit/qiskit-finance/tree/main/docs/tutorials/10_qgan_option_pricing.ipynb>`__ tutorial."
msgstr "Para ver un ejemplo de cómo utilizar un qGAN capacitado en una aplicación, el precio de los derivados financieros, consulta el tutorial `Option Pricing with qGANs <https://github.com/Qiskit/qiskit-finance/tree/main/docs/tutorials/10_qgan_option_pricing.ipynb>`__."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:56
msgid "Load the Training Data"
msgstr "Cargar los Datos de Entrenamiento"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:58
msgid "First, we need to load the :math:`k`-dimensional training data samples (here k=1)."
msgstr "Primero, necesitamos cargar las muestras de datos de entrenamiento :math:`k`-dimensionales (aquí k=1)."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:60
msgid "Next, the data resolution is set, i.e. the min/max data values and the number of qubits used to represent each data dimension."
msgstr "A continuación, se establece la resolución de datos, es decir, los valores de datos mínimo/máximo y el número de qubits utilizados para representar cada dimensión de datos."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:95
msgid "Initialize the qGAN"
msgstr "Inicializar la qGAN"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:97
msgid "The qGAN consists of a quantum generator :math:`G_{\\theta}`, i.e., an ansatz, and a classical discriminator :math:`D_{\\phi}`, a neural network."
msgstr "La qGAN consta de un generador cuántico :math:`G_{\\theta}`, es decir, un ansatz, y un discriminador clásico :math:`D_{\\phi}`, una red neuronal."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:99
msgid "To implement the quantum generator, we choose a depth-\\ :math:`1` ansatz that implements :math:`R_Y` rotations and :math:`CZ` gates which takes a uniform distribution as an input state. Notably, for :math:`k>1` the generator’s parameters must be chosen carefully. For example, the circuit depth should be :math:`>1` because higher circuit depths enable the representation of more complex structures."
msgstr "Para implementar el generador cuántico, elegimos un ansatz de profundidad :math:`1` que implementa las rotaciones :math:`R_Y` y las compuertas :math:`CZ` que toma una distribución uniforme como estado de entrada. En particular, para :math:`k>1`, los parámetros del generador deben elegirse con cuidado. Por ejemplo, la profundidad del circuito debería ser :math:`>1` porque profundidades de circuito más grandes permiten la representación de estructuras más complejas."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:101
msgid "The classical discriminator used here is based on a neural network implementation using NumPy. There is also a discriminator based on PyTorch which is not installed by default when installing Qiskit - see `Optional Install <https://github.com/Qiskit/qiskit-machine-learning#optional-installs>`__ for more information."
msgstr "El discriminador clásico que se utiliza aquí se basa en una implementación de red neuronal que utiliza NumPy. También hay un discriminador basado en PyTorch que no se instala de forma predeterminada al instalar Qiskit; consulta `Optional Install <https://github.com/Qiskit/qiskit-machine-learning#optional-installs>`__ para obtener más información."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:103
msgid "Here, both networks are updated with the ADAM optimization algorithm (ADAM is qGAN optimizer default)."
msgstr "Aquí, ambas redes se actualizan con el algoritmo de optimización de ADAM (ADAM es el optimizador qGAN predeterminado)."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:164
msgid "Run the qGAN Training"
msgstr "Ejecuta el Entrenamiento qGAN"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:166
msgid "During the training the discriminator’s and the generator’s parameters are updated alternately w.r.t the following loss functions:"
msgstr "Durante el entrenamiento, los parámetros del discriminador y del generador se actualizan alternativamente con las siguientes funciones de pérdida:"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:168
msgid "L_G\\left(\\phi, \\theta\\right) = -\\frac{1}{m}\\sum\\limits_{l=1}^{m}\\left[\\log\\left(D_{\\phi}\\left(g^{l}\\right)\\right)\\right]\n\n"
msgstr "L_G\\left(\\phi, \\theta\\right) = -\\frac{1}{m}\\sum\\limits_{l=1}^{m}\\left[\\log\\left(D_{\\phi}\\left(g^{l}\\right)\\right)\\right]\n\n"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:170
msgid "and"
msgstr "y"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:172
msgid "L_D\\left(\\phi, \\theta\\right) =\n"
"  \\frac{1}{m}\\sum\\limits_{l=1}^{m}\\left[\\log D_{\\phi}\\left(x^{l}\\right) + \\log\\left(1-D_{\\phi}\\left(g^{l}\\right)\\right)\\right],"
msgstr "L_D\\left(\\phi, \\theta\\right) =\n"
"  \\frac{1}{m}\\sum\\limits_{l=1}^{m}\\left[\\log D_{\\phi}\\left(x^{l}\\right) + \\log\\left(1-D_{\\phi}\\left(g^{l}\\right)\\right)\\right],"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:177
msgid "with :math:`m` denoting the batch size and :math:`g^l` describing the data samples generated by the quantum generator."
msgstr "con :math:`m` que indica el tamaño del lote y :math:`g^l` que describe las muestras de datos generadas por el generador cuántico."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:179
msgid "Please note that the training, for the purpose of this notebook, has been kept briefer by the selection of a known initial point (``init_params``). Without such prior knowledge be aware training may take some while."
msgstr "Ten en cuenta que el entrenamiento, para los propósitos de este cuaderno, se ha mantenido más breve mediante la selección de un punto inicial conocido (``init_params``). Sin estos conocimientos previos, ten en cuenta que el entrenamiento puede llevar algún tiempo."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:245
msgid "Training Progress & Outcome"
msgstr "Progreso y Resultado del Entrenamiento"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:247
msgid "Now, we plot the evolution of the generator’s and the discriminator’s loss functions during the training, as well as the progress in the relative entropy between the trained and the target distribution."
msgstr "Ahora, graficamos la evolución de las funciones de pérdida del generador y el discriminador durante el entrenamiento, así como el progreso en la entropía relativa entre la distribución entrenada y la distribución objetivo."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:249
msgid "Finally, we also compare the cumulative distribution function (CDF) of the trained distribution to the CDF of the target distribution."
msgstr "Finalmente, también comparamos la función de distribución acumulada (CDF) de la distribución entrenada con la CDF de la distribución objetivo."

#: ../../tutorials/05_torch_connector.ipynb:9
msgid "Torch Connector"
msgstr "Conector Torch"

#: ../../tutorials/05_torch_connector.ipynb:11
msgid "This tutorial shows how the ``TorchConnector`` allows to use any ``NeuralNetwork`` from Qiskit Machine Learning and integrate it in a PyTorch workflow. The ``TorchConnector`` takes any ``NeuralNetwork`` and makes it available as a PyTorch ``Module``."
msgstr "Este tutorial muestra cómo el ``TorchConnector`` permite usar cualquier ``NeuralNetwork`` de Qiskit Machine Learning e integrarlo en un flujo de trabajo de PyTorch. El ``TorchConnector`` toma cualquier ``NeuralNetwork`` y la pone disponible como un ``Module`` de PyTorch."

#: ../../tutorials/05_torch_connector.ipynb:14
msgid "Content:"
msgstr "Contenido:"

#: ../../tutorials/05_torch_connector.ipynb:16
msgid "`Part 1: Simple Classification & Regression <#Part-1:-Simple-Classification-&-Regression>`__ - Classification - Classification with PyTorch and the ``OpflowQNN`` - Classification with PyTorch and the ``CircuitQNN`` - Regression - Regression with PyTorch and the ``OpflowQNN``"
msgstr "`Parte 1: Clasificación y Regresión Simples <#Part-1:-Simple-Classification-&-Regression>`__ - Clasificación - Clasificación con PyTorch y el ``OpflowQNN`` - Clasificación con PyTorch y el ``CircuitQNN`` - Regresión - Regresión con PyTorch y el ``OpflowQNN``"

#: ../../tutorials/05_torch_connector.ipynb:18
msgid "`Part 2: MNIST Classification <#Part-2:-MNIST-Classification>`__"
msgstr "`Parte 2: Clasificación MNIST <#Part-2:-MNIST-Classification>`__"

#: ../../tutorials/05_torch_connector.ipynb:20
msgid "Illustrates how to embed a (Quantum) ``NeuralNetwork`` into a target PyTorch workflow to classify MNIST data."
msgstr "Ilustra cómo incrustar una ``NeuralNetwork`` (Cuántica) en un flujo de trabajo de PyTorch de destino para clasificar los datos MNIST."

#: ../../tutorials/05_torch_connector.ipynb:57
msgid "Part 1: Simple Classification & Regression"
msgstr "Parte 1: Clasificación y Regresión Simples"

#: ../../tutorials/05_torch_connector.ipynb:71
msgid "First, we show how the ``TorchConnector`` can be used to use a Quantum ``NeuralNetwork`` to solve a classification tasks. Therefore, we generate a simple random data set."
msgstr "Primero, mostramos cómo el ``TorchConnector`` se puede usar con una ``NeuralNetwork`` Cuántica para resolver tareas de clasificación. Por lo tanto, generamos un conjunto de datos aleatorios simple."

#: ../../tutorials/05_torch_connector.ipynb:117
msgid "Classification with PyTorch and the ``OpflowQNN``"
msgstr "Clasificación con PyTorch y el ``OpflowQNN``"

#: ../../tutorials/05_torch_connector.ipynb:119
msgid "Linking an ``OpflowQNN`` to PyTorch is relatively straight-forward. Here we illustrate this using the ``TwoLayerQNN``."
msgstr "Vincular un ``OpflowQNN`` a PyTorch es relativamente sencillo. Aquí ilustramos esto usando el ``TwoLayerQNN``."

#: ../../tutorials/05_torch_connector.ipynb:330
msgid "The red circles indicate wrongly classified data points."
msgstr "Los círculos rojos indican puntos de datos clasificados incorrectamente."

#: ../../tutorials/05_torch_connector.ipynb:342
msgid "Classification with PyTorch and the ``CircuitQNN``"
msgstr "Clasificación con PyTorch y el ``CircuitQNN``"

#: ../../tutorials/05_torch_connector.ipynb:344
msgid "Linking an ``CircuitQNN`` to PyTorch requires the correct setup, otherwise backpropagation is not possible."
msgstr "Vincular un ``CircuitQNN`` a PyTorch requiere la configuración correcta; de lo contrario, la retropropagación no es posible."

#: ../../tutorials/05_torch_connector.ipynb:526
msgid "We use a model based on the ``TwoLayerQNN`` to also illustrate an regression task."
msgstr "Usamos un modelo basado en el ``TwoLayerQNN`` para ilustrar también una tarea de regresión."

#: ../../tutorials/05_torch_connector.ipynb:758
msgid "Part 2: MNIST Classification"
msgstr "Part 2: Clasifiación MNIST"

#: ../../tutorials/05_torch_connector.ipynb:760
msgid "Also see Qiskit Textbook: https://qiskit.org/textbook/ch-machine-learning/machine-learning-qiskit-pytorch.html"
msgstr "Consulta también el libro de texto de Qiskit: https://qiskit.org/textbook/ch-machine-learning/machine-learning-qiskit-pytorch.html"

#: ../../tutorials/index.rst:3
msgid "Machine Learning Tutorials"
msgstr "Tutoriales de Machine Learning"

