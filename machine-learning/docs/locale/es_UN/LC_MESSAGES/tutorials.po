msgid ""
msgstr ""
"Project-Id-Version: qiskit-docs\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-07-12 22:21+0000\n"
"PO-Revision-Date: 2021-07-13 15:47\n"
"Last-Translator: \n"
"Language-Team: Spanish (United)\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: qiskit-docs\n"
"X-Crowdin-Project-ID: 369271\n"
"X-Crowdin-Language: es-un\n"
"X-Crowdin-File: /master/machine-learning/docs/locale/en/LC_MESSAGES/tutorials.po\n"
"X-Crowdin-File-ID: 9528\n"
"Language: es_UN\n"

#: ../../tutorials/01_neural_networks.ipynb:13
#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:13
#: ../../tutorials/03_quantum_kernel.ipynb:13
#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:13
#: ../../tutorials/05_torch_connector.ipynb:13
msgid "Run interactively in jupyter notebook."
msgstr "Ejecutar de forma interactiva en jupyter notebook."

#: ../../tutorials/01_neural_networks.ipynb:9
msgid "Quantum Neural Networks"
msgstr "Redes Neuronales Cu谩nticas"

#: ../../tutorials/01_neural_networks.ipynb:11
msgid "This notebook demonstrates the different generic quantum neural network (QNN) implementations provided in Qiskit Machine Learning. The networks are meant as application-agnostic computational units that can be used for many different use cases. Depending on the application, a particular type of network might more or less suitable and might require to be set up in a particular way. The following different available neural networks will now be discussed in more detail:"
msgstr "Este cuaderno muestra las diferentes implementaciones gen茅ricas de redes neuronales cu谩nticas (quantum neural network, QNN) proporcionadas en Qiskit Machine Learning. Las redes est谩n pensadas como unidades computacionales independientes de la aplicaci贸n en las que se pueden utilizar para muchos casos de uso diferentes. Dependiendo de la aplicaci贸n, un tipo particular de red puede ser m谩s o menos adecuado y puede ser necesario configurarlo de una manera particular. Las siguientes redes neuronales disponibles diferentes se discutir谩n ahora con m谩s detalle:"

#: ../../tutorials/01_neural_networks.ipynb:13
msgid "``NeuralNetwork``: The interface for neural networks."
msgstr "``NeuralNetwork``: La interfaz para redes neuronales."

#: ../../tutorials/01_neural_networks.ipynb:14
msgid "``OpflowQNN``: A network based on the evaluation of quantum mechanical observables."
msgstr "``OpflowQNN``: Una red basada en la evaluaci贸n de observables de la mec谩nica cu谩ntica."

#: ../../tutorials/01_neural_networks.ipynb:15
msgid "``TwoLayerQNN``: A special ``OpflowQNN`` implementation for convenience."
msgstr "``TwoLayerQNN``: Una implementaci贸n especial de ``OpflowQNN`` para mayor conveniencia."

#: ../../tutorials/01_neural_networks.ipynb:16
msgid "``CircuitQNN``: A network based on the samples resulting from measuring a quantum circuit."
msgstr "``CircuitQNN``: Una red basada en los ejemplos resultantes de medir un circuito cu谩ntico."

#: ../../tutorials/01_neural_networks.ipynb:64
msgid "1. ``NeuralNetwork``"
msgstr "1. ``NeuralNetwork``"

#: ../../tutorials/01_neural_networks.ipynb:66
msgid "The ``NeuralNetwork`` represents the interface for all neural networks available in Qiskit Machine Learning. It exposes a forward and a backward pass taking the data samples and trainable weights as input. A ``NeuralNetwork`` does not contain any training capabilities, these are pushed to the actual algorithms / applications. Thus, a ``NeuralNetwork`` also does not store the values for trainable weights. In the following, different implementations of this interfaces are introduced."
msgstr "El ``NeuralNetwork`` representa la interface para todas las redes neuronales disponibles en Qiskit Machine Learning. Este muestra un pase hacia adelante y un pase hacia atr谩s tomando las muestras y ponderaciones entrenables como entrada. Un ``NeuralNetwork`` no contiene ninguna capacidad de entrenamiento, esto se le asigna a los algoritmos y a las aplicaciones. Por ello, un ``NeuralNetwork`` tampoco guarda los valores de las ponderaciones entrenables. Enseguida, se introducen las diferentes implementaciones de estas interfaces."

#: ../../tutorials/01_neural_networks.ipynb:68
msgid "Suppose a ``NeuralNetwork`` called ``nn``. Then, the ``nn.forward(input, weights)`` pass takes either flat inputs for the data and weights of size ``nn.num_inputs`` and ``nn.num_weights``, respectively. ``NeuralNetwork`` supports batching of inputs and returns batches of output of the corresponding shape."
msgstr "Supongamos que tenemos un ``NeuralNetwork`` llamado ``nn``. A continuaci贸n, el paso ``nn.forward (input, weights)`` toma cualquiera de las entradas planas para los datos y las ponderaciones del tama帽o ``nn.num_inputs`` y ``nn.num_weights``, respectivamente. ``NeuralNetwork`` es compatible con la carga de entradas y devuelve lotes de salida de la forma correspondiente."

#: ../../tutorials/01_neural_networks.ipynb:80
msgid "2. ``OpflowQNN``"
msgstr "2. ``OpflowQNN``"

#: ../../tutorials/01_neural_networks.ipynb:82
msgid "The ``OpflowQNN`` takes a (parametrized) operator from Qiskit and leverages Qiskits gradient framework to provide the backward pass. Such an operator can for instance be an expected value of a quantum mechanical observable with respect to a parametrized quantum state. The Parameters can be used to load classical data as well as represent trainable weights. The ``OpflowQNN`` also allows lists of operators and more complex structures to construct more complex QNNs."
msgstr "El ``OpflowQNN`` toma un operador (parametrizado) de Qiskit y aprovecha el entorno de trabajo de gradiente de Qiskit para proporcionar el pase hacia atr谩s. Tal operador puede ser, por ejemplo, un valor esperado de un observable en la mec谩nica cu谩ntica con respecto a un estado cu谩ntico parametrizado. Los par谩metros se pueden utilizar para cargar datos cl谩sicos y representar pesos entrenables. El ``OpflowQNN`` tambi茅n permite listas de operadores y estructuras m谩s complejas para construir QNN m谩s complejas."

#: ../../tutorials/01_neural_networks.ipynb:321
msgid "Combining multiple observables in a ``ListOp`` also allows to create more complex QNNs"
msgstr "La combinaci贸n de multiples observables en un ``ListOp`` tambi茅n permite crear QNNs m谩s complejos"

#: ../../tutorials/01_neural_networks.ipynb:412
msgid "3. ``TwoLayerQNN``"
msgstr "3. ``TwoLayerQNN``"

#: ../../tutorials/01_neural_networks.ipynb:414
msgid "The ``TwoLayerQNN`` is a special ``OpflowQNN`` on :math:`n` qubits that consists of first a feature map to insert data and second an ansatz that is trained. The default observable is :math:`Z^{\\otimes n}`, i.e., parity."
msgstr "El ``TwoLayerQNN`` es un ``OpflowQNN`` especial en :math:`n` qubits qu茅 consiste en primer lugar de un mapa de caracter铆sticas para insertar datos y en segundo de un ansatz entrenado. El observable predeterminado es :math:`Z^{\\otimes n}`, es decir, paridad."

#: ../../tutorials/01_neural_networks.ipynb:612
msgid "4. ``CircuitQNN``"
msgstr "4. ``CircuitQNN``"

#: ../../tutorials/01_neural_networks.ipynb:614
msgid "The ``CircuitQNN`` is based on a (parametrized) ``QuantumCircuit``. This can take input as well as weight parameters and produces samples from the measurement. The samples can either be interpreted as probabilities of measuring the integer index corresponding to a bitstring or directly as a batch of binary output. In the case of probabilities, gradients can be estimated efficiently and the ``CircuitQNN`` provides a backward pass as well. In case of samples, differentiation is not possible and the backward pass returns ``(None, None)``."
msgstr "El ``CircuitQNN`` est谩 basado en un ``QuantumCircuit`` (parametrizado). Esto puede tomar tanto par谩metros de entrada como de ponderaci贸n y producir muestras a partir de la medici贸n. Las muestras pueden interpretarse como probabilidades de medir el 铆ndice entero correspondiente a una cadena de bits o directamente como un lote de salida binaria. En el caso de las probabilidades, los gradientes se pueden estimar de manera eficiente y el ``CircuitQNN`` tambi茅n proporciona un pase hacia atr谩s. En el caso de las muestras, la diferenciaci贸n no es posible y el pase hacia atr谩s devuelve ``(None, None)``."

#: ../../tutorials/01_neural_networks.ipynb:617
msgid "Further, the ``CircuitQNN`` allows to specify an ``interpret`` function to post-process the samples. This is expected to take a measured integer (from a bitstring) and map it to a new index, i.e.non-negative integer. In this case, the output shape needs to be provided and the probabilities are aggregated accordingly."
msgstr "Adem谩s, el ``CircuitQNN`` permite especificar una funci贸n ``interpret`` para procesar las muestras. Se espera que esto tome un entero medido (de una cadena de bits) y que se mapee con un nuevo 铆ndice, es decir,un entero no negativo. En este caso, es necesario proporcionar la forma de la salida y las probabilidades se agregan en consecuencia."

#: ../../tutorials/01_neural_networks.ipynb:619
msgid "A ``CircuitQNN`` can be configured to return sparse as well as dense probability vectors. If no ``interpret`` function is used, the dimension of the probability vector scales exponentially with the number of qubits and a sparse recommendation is usually recommended. In case of an ``interpret`` function it depends on the expected outcome. If, for instance, an index is mapped to the parity of the corresponding bitstring, i.e., to 0 or 1, a dense output makes sense and the result will be a probability vector of length 2."
msgstr "Un ``CircuitQNN``puede ser configurado para devolver los vectores de probabilidad dispersos, as铆 como densos. Si no se utiliza la funci贸n ``interpret``, la dimensi贸n del vector de probabilidad escala exponencialmente con el n煤mero de qubits y normalmente se sugiere una recomendaci贸n dispersa. En caso de que una funci贸n ``interpret`` se utilice, va a depender del resultado esperado. Si, por ejemplo, un 铆ndice se correlaciona con la paridad de la cadena de bits correspondiente, es decir, a 0 o 1, una salida densa tiene sentido y el resultado ser谩 un vector de probabilidad de longitud 2."

#: ../../tutorials/01_neural_networks.ipynb:662
msgid "4.1 Output: sparse integer probabilities"
msgstr "4.1 Salida: probabilidades de enteros dispersos"

#: ../../tutorials/01_neural_networks.ipynb:761
msgid "4.2 Output: dense parity probabilities"
msgstr "4.2 Salida: probabilidades de paridad densa"

#: ../../tutorials/01_neural_networks.ipynb:869
msgid "4.3 Output: Samples"
msgstr "4.3 Salida: Muestras"

#: ../../tutorials/01_neural_networks.ipynb:985
msgid "4.4 Output: Parity Samples"
msgstr "4.4 Salida: Muestras de Paridad"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:9
msgid "Neural Network Classifier & Regressor"
msgstr "Clasificador y Regresor de Redes Neuronales"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:11
msgid "In this tutorial we show how the ``NeuralNetworkClassifier`` and ``NeuralNetworkRegressor`` are used. Both take as an input a (Quantum) ``NeuralNetwork`` and leverage it in a specific context. In both cases we also provide a pre-configured variant for convenience, the Variational Quantum Classifier (``VQC``) and Variational Quantum Regressor (``VQR``). The tutorial is structured as follows:"
msgstr "En este tutorial mostramos c贸mo se utiliza el ``NeuralNetworkClassifier`` y el ``NeuralNetworkRegressor``. Ambos toman como entrada una ``NeuralNetwork`` (Cu谩ntica) y la aprovechan en un contexto espec铆fico. En ambos casos tambi茅n ofrecemos una variante preconfigurada para mayor comodidad, el Clasificador Cu谩ntico Variacional (Variational Quantum Classifier, ``VQC``) y el Regresor Cu谩ntico Variacional (Variational Quantum Regressor, ``VQR``). El tutorial est谩 estructurado de la siguiente manera:"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:13
msgid "`Classification <#Classification>`__"
msgstr "`Clasificaci贸n <#Classification>`__"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:15
msgid "Classification with an ``OpflowQNN``"
msgstr "Clasificaci贸n con un ``OpflowQNN``"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:16
#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:249
msgid "Classification with a ``CircuitQNN``"
msgstr "Clasificaci贸n con un ``CircuitQNN``"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:17
#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:398
msgid "Variational Quantum Classifier (``VQC``)"
msgstr "Clasificador Cu谩ntico Variacional (Variational Quantum Classifier, ``VQC``)"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:19
msgid "`Regression <#Regression>`__"
msgstr "`Regresi贸n <#Regression>`__"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:21
#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:539
msgid "Regression with an ``OpflowQNN``"
msgstr "Regresi贸n con un ``OpflowQNN``"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:22
msgid "Variational Quantum Regressor (``VQR``)"
msgstr "Regresor Cu谩ntico Variacional (Variational Quantum Regressor, ``VQR``)"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:70
#: ../../tutorials/03_quantum_kernel.ipynb:53
msgid "Classification"
msgstr "Clasificaci贸n"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:72
msgid "We prepare a simple classification dataset to illustrate the following algorithms."
msgstr "Preparamos un conjunto de datos de clasificaci贸n simple para ilustrar los siguientes algoritmos."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:117
msgid "Classification with the an ``OpflowQNN``"
msgstr "Clasificaci贸n con un ``OpflowQNN``"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:119
msgid "First we show how an ``OpflowQNN`` can be used for classification within a ``NeuralNetworkClassifier``. In this context, the ``OpflowQNN`` is expected to return one-dimensional output in :math:`[-1, +1]`. This only works for binary classification and we assign the two classes to :math:`\\{-1, +1\\}`. For convenience, we use the ``TwoLayerQNN``, which is a special type of ``OpflowQNN`` defined via a feature map and an ansatz."
msgstr "Primero mostramos c贸mo se puede usar un ``OpflowQNN`` para la clasificaci贸n dentro de un ``NeuralNetworkClassifier``. En este contexto, se espera que el ``OpflowQNN`` devuelva una salida unidimensional en :math:`[-1, +1]`. Esto solo funciona para la clasificaci贸n binaria y asignamos las dos clases a :math:`\\{-1, +1\\}`. Para mayor comodidad, utilizamos el ``TwoLayerQNN``, que es un tipo especial de ``OpflowQNN`` definido a trav茅s de un mapa de caracter铆sticas y un ansatz."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:251
msgid "Next we show how a ``CircuitQNN`` can be used for classification within a ``NeuralNetworkClassifier``. In this context, the ``CircuitQNN`` is expected to return :math:`d`-dimensional probability vector as output, where :math:`d` denotes the number of classes. Sampling from a ``QuantumCircuit`` automatically results in a probability distribution and we just need to define a mapping from the measured bitstrings to the different classes. For binary classification we use the parity mapping."
msgstr "A continuaci贸n, mostramos c贸mo se puede utilizar un ``CircuitQNN`` para la clasificaci贸n dentro de un ``NeuralNetworkClassifier``. En este contexto, se espera que el ``CircuitQNN`` devuelva un vector de probabilidad :math:`d`-dimensional como salida, donde :math:`d` denota el n煤mero de clases. El muestreo de un ``QuantumCircuit`` da como resultado autom谩ticamente una distribuci贸n de probabilidad y solo necesitamos definir un mapeo de las cadenas de bits medidas a las diferentes clases. Para la clasificaci贸n binaria usamos el mapeo de paridad."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:400
msgid "The ``VQC`` is a special variant of the ``NeuralNetworkClassifier`` with a ``CircuitQNN``. It applies a parity mapping (or extensions to multiple classes) to map from the bitstring to the classification, which results in a probability vector, which is interpreted as a one-hot encoded result. By default, it applies this the ``CrossEntropyLoss`` function that expects labels given in one-hot encoded format and will return predictions in that format too."
msgstr "El ``VQC`` es una variante especial del ``NeuralNetworkClassifier`` con un ``CircuitQNN``. Aplica un mapeo de paridad (o extensiones a m煤ltiples clases) para mapear desde la cadena de bits hasta la clasificaci贸n, lo que da como resultado un vector de probabilidad, que se interpreta como un resultado codificado one-hot. De forma predeterminada, aplica esta funci贸n ``CrossEntropyLoss`` que espera que las etiquetas se proporcionen en un formato codificado one-hot y tambi茅n devolver谩 predicciones en ese formato."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:496
msgid "Regression"
msgstr "Regresi贸n"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:498
msgid "We prepare a simple regression dataset to illustrate the following algorithms."
msgstr "Preparamos un conjunto de datos de regresi贸n simple para ilustrar los siguientes algoritmos."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:541
msgid "Here we restrict to regression with an ``OpflowQNN`` that returns values in :math:`[-1, +1]`. More complex and also multi-dimensional models could be constructed, also based on ``CircuitQNN`` but that exceeds the scope of this tutorial."
msgstr "Aqu铆 restringimos la regresi贸n con un ``OpflowQNN`` que devuelve valores entre :math:`[-1, +1]`. Se podr铆an construir modelos m谩s complejos y tambi茅n multidimensionales, tambi茅n basados en ``CircuitQNN`` pero eso excede el alcance de este tutorial."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:648
msgid "Regression with the Variational Quantum Regressor (``VQR``)"
msgstr "Regresi贸n con el Regresor Cu谩ntico Variacional (Variational Quantum Regressor, ``VQR``)"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:650
msgid "Similar to the ``VQC`` for classification, the ``VQR`` is a special variant of the ``NeuralNetworkRegressor`` with a ``OpflowQNN``. By default it considers the ``L2Loss`` function to minimize the mean squared error between predictions and targets."
msgstr "Similar al ``VQC`` para la clasificaci贸n, el ``VQR`` es una variante especial del ``NeuralNetworkRegressor`` con un ``OpflowQNN``. De forma predeterminada, considera la funci贸n ``L2Loss`` para minimizar el error cuadr谩tico medio entre las predicciones y los objetivos."

#: ../../tutorials/03_quantum_kernel.ipynb:9
msgid "Quantum Kernel Machine Learning"
msgstr "Machine Learning con Kernel Cu谩ntico"

#: ../../tutorials/03_quantum_kernel.ipynb:11
msgid "The general task of machine learning is to find and study patterns in data. For many datasets, the datapoints are better understood in a higher dimensional feature space, through the use of a kernel function: :math:`k(\\vec{x}_i, \\vec{x}_j) = \\langle f(\\vec{x}_i), f(\\vec{x}_j) \\rangle` where :math:`k` is the kernel function, :math:`\\vec{x}_i, \\vec{x}_j` are :math:`n` dimensional inputs, :math:`f` is a map from :math:`n`-dimension to :math:`m`-dimension space and :math:`\\langle a,b \\rangle` denotes the dot product. When considering finite data, a kernel function can be represented as a matrix: :math:`K_{ij} = k(\\vec{x}_i,\\vec{x}_j)`."
msgstr "La tarea general del machine learning es encontrar y estudiar patrones en los datos. Para muchos conjuntos de datos, los puntos de datos se comprenden mejor en un espacio de caracter铆sticas de mayor dimensi贸n, mediante el uso de una funci贸n de kernel: :math:`k(\\vec{x}_i, \\vec{x}_j) = \\langle f(\\vec{x}_i), f(\\vec{x}_j) \\rangle` donde :math:`k` es la funci贸n de kernel, :math:`\\vec{x}_i, \\vec{x}_j`son entradas :math:`n` dimensionales, :math:`f` es un mapa del espacio de dimensi贸n :math:`n` al espacio de dimensi贸n :math:`m` y :math:`\\langle a,b \\rangle` denota el producto escalar. Cuando se consideran datos finitos, una funci贸n de kernel se puede representar como una matriz: :math:`K_{ij} = k(\\vec{x}_i,\\vec{x}_j)`."

#: ../../tutorials/03_quantum_kernel.ipynb:14
msgid "In quantum kernel machine learning, a quantum feature map :math:`\\phi(\\vec{x})` is used to map a classical feature vector :math:`\\vec{x}` to a quantum Hilbert space, :math:`| \\phi(\\vec{x})\\rangle \\langle \\phi(\\vec{x})|`, such that :math:`K_{ij} = \\left| \\langle \\phi^\\dagger(\\vec{x}_j)| \\phi(\\vec{x}_i) \\rangle \\right|^{2}`. See `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__ for more details."
msgstr "En machine learning con kernel cu谩ntico, se utiliza un mapa de caracter铆sticas cu谩nticas :math:`\\phi(\\vec{x})` para asignar un vector de caracter铆sticas cl谩sicas :math:`\\vec{x}` a un espacio cu谩ntico de Hilbert,:math:`| \\phi(\\vec{x})\\rangle \\langle \\phi(\\vec{x})|`, tal que :math:`K_{ij} = \\left| \\langle \\phi^\\dagger(\\vec{x}_j)| \\phi(\\vec{x}_i) \\rangle \\right|^{2}`. Consulte `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__ para obtener m谩s detalles."

#: ../../tutorials/03_quantum_kernel.ipynb:16
msgid "In this notebook, we use ``qiskit`` to calculate a kernel matrix using a quantum feature map, then use this kernel matrix in ``scikit-learn`` classification and clustering algorithms."
msgstr "En este cuaderno, usamos ``qiskit`` para calcular una matriz de kernel usando un mapa de caracter铆sticas cu谩nticas, luego usamos esta matriz de kernel en algoritmos de clasificaci贸n y agrupamiento de ``scikit-learn``."

#: ../../tutorials/03_quantum_kernel.ipynb:55
msgid "For our classification example, we will use the *ad hoc dataset* as described in `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__, and the ``scikit-learn`` `support vector machine <https://scikit-learn.org/stable/modules/svm.html>`__ classification (``svc``) algorithm."
msgstr "Para nuestro ejemplo de clasificaci贸n, usaremos el *dataset ad hoc* como se describe en `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__, y el algoritmo de clasificaci贸n `support vector machine <https://scikit-learn.org/stable/modules/svm.html>`__ (``svc``) de ``scikit-learn``."

#: ../../tutorials/03_quantum_kernel.ipynb:111
msgid "With our training and testing datasets ready, we set up the ``QuantumKernel`` class to calculate a kernel matrix using the `ZZFeatureMap <https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZZFeatureMap.html>`__, and the ``BasicAer`` ``qasm_simulator`` using 1024 shots."
msgstr "Con nuestros conjuntos de datos de entrenamiento y prueba listos, configuramos la clase ``QuantumKernel`` para calcular una matriz de kernel usando el `ZZFeatureMap <https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZZFeatureMap.html>`__, y el ``qasm_simulator`` de ``BasicAer``usando 1024 iteraciones."

#: ../../tutorials/03_quantum_kernel.ipynb:138
msgid "The ``scikit-learn`` ``svc`` algorithm allows us to define a `custom kernel <https://scikit-learn.org/stable/modules/svm.html#custom-kernels>`__ in two ways: by providing the kernel as a callable function or by precomputing the kernel matrix. We can do either of these using the ``QuantumKernel`` class in ``qiskit``."
msgstr "El algoritmo ``svc`` de ``scikit-learn`` nos permite definir un `custom kernel <https://scikit-learn.org/stable/modules/svm.html#custom-kernels>`__ de dos maneras: proporcionando el kernel como una funci贸n invocable o precomputando la matriz del kernel. Podemos hacer cualquiera de estos usando la clase ``QuantumKernel`` en ``qiskit``."

#: ../../tutorials/03_quantum_kernel.ipynb:140
msgid "The following code gives the kernel as a callable function:"
msgstr "El siguiente c贸digo da el kernel como una funci贸n invocable:"

#: ../../tutorials/03_quantum_kernel.ipynb:184
msgid "The following code precomputes and plots the training and testing kernel matrices before providing them to the ``scikit-learn`` ``svc`` algorithm:"
msgstr "El siguiente c贸digo precalcula y traza las matrices del kernel de entrenamiento y prueba antes de proporcionarlas al algoritmo ``svc`` de ``scikit-learn``:"

#: ../../tutorials/03_quantum_kernel.ipynb:250
msgid "``qiskit`` also contains the ``qsvc`` class that extends the ``sklearn svc`` class, that can be used as follows:"
msgstr "``qiskit`` tambi茅n contiene la clase ``qsvc`` que extiende la clase ``sklearn svc`, que se puede usar de la siguiente manera:"

#: ../../tutorials/03_quantum_kernel.ipynb:295
msgid "Clustering"
msgstr "Agrupaci贸n (Clustering)"

#: ../../tutorials/03_quantum_kernel.ipynb:297
msgid "For our clustering example, we will again use the *ad hoc dataset* as described in `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__, and the ``scikit-learn`` ``spectral`` clustering algorithm."
msgstr "Para nuestro ejemplo de agrupamiento, usaremos nuevamente el *dataset ad hoc* como se describe en `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__, y el algoritmo de agrupaci贸n ``spectral`` de ``scikit-learn``."

#: ../../tutorials/03_quantum_kernel.ipynb:299
msgid "We will regenerate the dataset with a larger gap between the two classes, and as clustering is an unsupervised machine learning task, we dont need a test sample."
msgstr "Regeneraremos el conjunto de datos con una brecha mayor entre las dos clases y, como la agrupaci贸n en cl煤steres es una tarea de machine learning no supervisada, no necesitamos una muestra de prueba."

#: ../../tutorials/03_quantum_kernel.ipynb:350
msgid "We again set up the ``QuantumKernel`` class to calculate a kernel matrix using the `ZZFeatureMap <https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZZFeatureMap.html>`__, and the BasicAer ``qasm_simulator`` using 1024 shots."
msgstr "Nuevamente configuramos la clase ``QuantumKernel`` para calcular una matriz de kernel usando el `ZZFeatureMap <https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZZFeatureMap.html>`__, y el ``qasm_simulator`` de BasicAer usando 1024 iteraciones."

#: ../../tutorials/03_quantum_kernel.ipynb:377
msgid "The scikit-learn spectral clustering algorithm allows us to define a [custom kernel] in two ways: by providing the kernel as a callable function or by precomputing the kernel matrix. Using the QuantumKernel class in qiskit, we can only use the latter."
msgstr "El algoritmo de agrupamiento espectral scikit-learn nos permite definir un [kernel personalizado] de dos formas: proporcionando el kernel como una funci贸n invocable o precomputando la matriz del kernel. Usando la clase QuantumKernel en qiskit, solo podemos usar este 煤ltimo."

#: ../../tutorials/03_quantum_kernel.ipynb:379
msgid "The following code precomputes and plots the kernel matrices before providing it to the scikit-learn spectral clustering algorithm, and scoring the labels using normalized mutual information, since we a priori know the class labels."
msgstr "El siguiente c贸digo precalcula y traza las matrices del kernel antes de proporcionarlo al algoritmo de agrupamiento espectral scikit-learn y puntuar las etiquetas utilizando informaci贸n mutua normalizada, ya que conocemos las etiquetas de clase a priori."

#: ../../tutorials/03_quantum_kernel.ipynb:439
msgid "``scikit-learn`` has other algorithms that can use a precomputed kernel matrix, here are a few:"
msgstr "``scikit-learn`` tiene otros algoritmos que pueden usar una matriz de kernel precalculada, aqu铆 hay algunos:"

#: ../../tutorials/03_quantum_kernel.ipynb:441
msgid "`Agglomerative clustering <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html>`__"
msgstr "`Agglomerative clustering <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html>`__"

#: ../../tutorials/03_quantum_kernel.ipynb:442
msgid "`Support vector regression <https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html>`__"
msgstr "`Support vector regression <https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html>`__"

#: ../../tutorials/03_quantum_kernel.ipynb:443
msgid "`Ridge regression <https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html>`__"
msgstr "`Ridge regression <https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html>`__"

#: ../../tutorials/03_quantum_kernel.ipynb:444
msgid "`Gaussian process regression <https://scikit-learn.org/stable/modules/gaussian_process.html>`__"
msgstr "`Guassian process regression <https://scikit-learn.org/stable/modules/gaussian_process.html>`__"

#: ../../tutorials/03_quantum_kernel.ipynb:445
msgid "`Principal component analysis <https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html>`__"
msgstr "`Principal component analysis <https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html>`__"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:9
msgid "qGANs for Loading Random Distributions"
msgstr "qGANs para Cargar Distribuciones Aleatorias"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:11
msgid "Given :math:`k`-dimensional data samples, we employ a quantum Generative Adversarial Network (qGAN) to learn the datas underlying random distribution and to load it directly into a quantum state:"
msgstr "Dadas muestras de datos :math:`k`-dimensionales, empleamos una Red Generativa Antag贸nica cu谩ntica (quantum Generative Adversarial Network, qGAN) para conocer la distribuci贸n aleatoria subyacente de los datos y cargarlos directamente en un estado cu谩ntico:"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:13
msgid "\\big| g_{\\theta}\\rangle = \\sum_{j=0}^{2^n-1} \\sqrt{p_{\\theta}^{j}}\\big| j \\rangle\n\n"
msgstr "\\big| g_{\\theta}\\rangle = \\sum_{j=0}^{2^n-1} \\sqrt{p_{\\theta}^{j}}\\big| j \\rangle\n\n"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:15
msgid "where :math:`p_{\\theta}^{j}` describe the occurrence probabilities of the basis states :math:`\\big| j\\rangle`."
msgstr "donde :math:`p_{\\theta}^{j}` describen las probabilidades de ocurrencia de los estados base :math:`\\big| j\\rangle`."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:17
msgid "The aim of the qGAN training is to generate a state :math:`\\big| g_{\\theta}\\rangle` where :math:`p_{\\theta}^{j}`, for :math:`j\\in \\left\\{0, \\ldots, {2^n-1} \\right\\}`, describe a probability distribution that is close to the distribution underlying the training data :math:`X=\\left\\{x^0, \\ldots, x^{k-1} \\right\\}`."
msgstr "El objetivo del entrenamiento qGAN es generar un estado :math:`\\big| g_{\\theta}\\rangle` donde :math:`p_{\\theta}^{j}`, para :math:`j\\in \\left\\{0, \\ldots, {2^n-1} \\right\\}`, describe una distribuci贸n de probabilidad cercana a la distribuci贸n subyacente a los datos de entrenamiento :math:`X=\\left\\{x^0, \\ldots, x^{k-1} \\right\\}`."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:19
msgid "For further details please refer to `Quantum Generative Adversarial Networks for Learning and Loading Random Distributions <https://arxiv.org/abs/1904.00043>`__ *Zoufal, Lucchi, Woerner* [2019]."
msgstr "Para obtener m谩s detalles, consulta `Quantum Generative Adversarial Networks for Learning and Loading Random Distributions <https://arxiv.org/abs/1904.00043>`__ *Zoufal, Lucchi, Woerner* [2019]."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:21
msgid "For an example of how to use a trained qGAN in an application, the pricing of financial derivatives, please see the `Option Pricing with qGANs <https://github.com/Qiskit/qiskit-finance/tree/main/docs/tutorials/10_qgan_option_pricing.ipynb>`__ tutorial."
msgstr "Para ver un ejemplo de c贸mo utilizar un qGAN capacitado en una aplicaci贸n, el precio de los derivados financieros, consulta el tutorial `Option Pricing with qGANs <https://github.com/Qiskit/qiskit-finance/tree/main/docs/tutorials/10_qgan_option_pricing.ipynb>`__."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:56
msgid "Load the Training Data"
msgstr "Cargar los Datos de Entrenamiento"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:58
msgid "First, we need to load the :math:`k`-dimensional training data samples (here k=1)."
msgstr "Primero, necesitamos cargar las muestras de datos de entrenamiento :math:`k`-dimensionales (aqu铆 k=1)."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:60
msgid "Next, the data resolution is set, i.e.the min/max data values and the number of qubits used to represent each data dimension."
msgstr "A continuaci贸n, se establece la resoluci贸n de datos, es decir,los valores de datos m铆nimo/m谩ximo y el n煤mero de qubits utilizados para representar cada dimensi贸n de datos."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:95
msgid "Initialize the qGAN"
msgstr "Inicializar la qGAN"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:97
msgid "The qGAN consists of a quantum generator :math:`G_{\\theta}`, i.e., an ansatz, and a classical discriminator :math:`D_{\\phi}`, a neural network."
msgstr "La qGAN consta de un generador cu谩ntico :math:`G_{\\theta}`, es decir, un ansatz, y un discriminador cl谩sico :math:`D_{\\phi}`, una red neuronal."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:99
msgid "To implement the quantum generator, we choose a depth-\\ :math:`1` ansatz that implements :math:`R_Y` rotations and :math:`CZ` gates which takes a uniform distribution as an input state. Notably, for :math:`k>1` the generators parameters must be chosen carefully. For example, the circuit depth should be :math:`>1` because higher circuit depths enable the representation of more complex structures."
msgstr "Para implementar el generador cu谩ntico, elegimos un ansatz de profundidad :math:`1` que implementa las rotaciones :math:`R_Y` y las compuertas :math:`CZ` que toma una distribuci贸n uniforme como estado de entrada. En particular, para :math:`k>1`, los par谩metros del generador deben elegirse con cuidado. Por ejemplo, la profundidad del circuito deber铆a ser :math:`>1` porque profundidades de circuito m谩s grandes permiten la representaci贸n de estructuras m谩s complejas."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:101
msgid "The classical discriminator used here is based on a neural network implementation using NumPy. There is also a discriminator based on PyTorch which is not installed by default when installing Qiskit - see `Optional Install <https://github.com/Qiskit/qiskit-machine-learning#optional-installs>`__ for more information."
msgstr "El discriminador cl谩sico que se utiliza aqu铆 se basa en una implementaci贸n de red neuronal que utiliza NumPy. Tambi茅n hay un discriminador basado en PyTorch que no se instala de forma predeterminada al instalar Qiskit; consulta `Optional Install <https://github.com/Qiskit/qiskit-machine-learning#optional-installs>`__ para obtener m谩s informaci贸n."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:103
msgid "Here, both networks are updated with the ADAM optimization algorithm (ADAM is qGAN optimizer default)."
msgstr "Aqu铆, ambas redes se actualizan con el algoritmo de optimizaci贸n de ADAM (ADAM es el optimizador qGAN predeterminado)."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:164
msgid "Run the qGAN Training"
msgstr "Ejecuta el Entrenamiento qGAN"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:166
msgid "During the training the discriminators and the generators parameters are updated alternately w.r.t the following loss functions:"
msgstr "Durante el entrenamiento, los par谩metros del discriminador y del generador se actualizan alternativamente con las siguientes funciones de p茅rdida:"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:168
msgid "L_G\\left(\\phi, \\theta\\right) = -\\frac{1}{m}\\sum\\limits_{l=1}^{m}\\left[\\log\\left(D_{\\phi}\\left(g^{l}\\right)\\right)\\right]\n\n"
msgstr "L_G\\left(\\phi, \\theta\\right) = -\\frac{1}{m}\\sum\\limits_{l=1}^{m}\\left[\\log\\left(D_{\\phi}\\left(g^{l}\\right)\\right)\\right]\n\n"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:170
msgid "and"
msgstr "y"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:172
msgid "L_D\\left(\\phi, \\theta\\right) =\n"
"  \\frac{1}{m}\\sum\\limits_{l=1}^{m}\\left[\\log D_{\\phi}\\left(x^{l}\\right) + \\log\\left(1-D_{\\phi}\\left(g^{l}\\right)\\right)\\right],"
msgstr "L_D\\left(\\phi, \\theta\\right) =\n"
"  \\frac{1}{m}\\sum\\limits_{l=1}^{m}\\left[\\log D_{\\phi}\\left(x^{l}\\right) + \\log\\left(1-D_{\\phi}\\left(g^{l}\\right)\\right)\\right],"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:177
msgid "with :math:`m` denoting the batch size and :math:`g^l` describing the data samples generated by the quantum generator."
msgstr "con :math:`m` que indica el tama帽o del lote y :math:`g^l` que describe las muestras de datos generadas por el generador cu谩ntico."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:179
msgid "Please note that the training, for the purpose of this notebook, has been kept briefer by the selection of a known initial point (``init_params``). Without such prior knowledge be aware training may take some while."
msgstr "Ten en cuenta que el entrenamiento, para los prop贸sitos de este cuaderno, se ha mantenido m谩s breve mediante la selecci贸n de un punto inicial conocido (``init_params``). Sin estos conocimientos previos, ten en cuenta que el entrenamiento puede llevar alg煤n tiempo."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:245
msgid "Training Progress & Outcome"
msgstr "Progreso y Resultado del Entrenamiento"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:247
msgid "Now, we plot the evolution of the generators and the discriminators loss functions during the training, as well as the progress in the relative entropy between the trained and the target distribution."
msgstr "Ahora, graficamos la evoluci贸n de las funciones de p茅rdida del generador y el discriminador durante el entrenamiento, as铆 como el progreso en la entrop铆a relativa entre la distribuci贸n entrenada y la distribuci贸n objetivo."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:249
msgid "Finally, we also compare the cumulative distribution function (CDF) of the trained distribution to the CDF of the target distribution."
msgstr "Finalmente, tambi茅n comparamos la funci贸n de distribuci贸n acumulada (CDF) de la distribuci贸n entrenada con la CDF de la distribuci贸n objetivo."

#: ../../tutorials/05_torch_connector.ipynb:9
msgid "Torch Connector and Hybrid QNNs"
msgstr "Conector Torch y QNNs H铆bridas"

#: ../../tutorials/05_torch_connector.ipynb:11
msgid "This tutorial introduces Qiskits ``TorchConnector`` class, and demonstrates how the ``TorchConnector`` allows for a natural integration of any ``NeuralNetwork`` from Qiskit Machine Learning into a PyTorch workflow. ``TorchConnector`` takes a Qiskit ``NeuralNetwork`` and makes it available as a PyTorch ``Module``. The resulting module can be seamlessly incorporated into PyTorch classical architectures and trained jointly without additional considerations, enabling the development and testing of novel **hybrid quantum-classical** machine learning architectures."
msgstr "Este tutorial presenta la clase ``TorchConnector`` de Qiskit y demuestra c贸mo el ``TorchConnector`` permite una integraci贸n natural de cualquier ``NeuralNetwork`` de Qiskit Machine Learning en un flujo de trabajo PyTorch. ``TorchConnector`` toma un ``NeuralNetwork`` de Qiskit y lo pone a disposici贸n como un ``Module`` de PyTorch. El m贸dulo resultante se puede incorporar a la perfecci贸n a las arquitecturas cl谩sicas de PyTorch y se puede entrenar de forma conjunta sin consideraciones adicionales, permitiendo el desarrollo y pruebas de arquitecturas de machine learning **h铆bridas cu谩nticas-cl谩sicas** novedosas."

#: ../../tutorials/05_torch_connector.ipynb:15
msgid "Content:"
msgstr "Contenido:"

#: ../../tutorials/05_torch_connector.ipynb:17
msgid "`Part 1: Simple Classification & Regression <#Part-1:-Simple-Classification-&-Regression>`__"
msgstr "`Parte 1: Clasificaci贸n y Regresi贸n Simples <#Part-1:-Simple-Classification-&-Regression>`__"

#: ../../tutorials/05_torch_connector.ipynb:19
msgid "The first part of this tutorial shows how quantum neural networks can be trained using PyTorchs automatic differentiation engine (``torch.autograd``, `link <https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html>`__) for simple classification and regression tasks."
msgstr "La primera parte de este tutorial muestra c贸mo se pueden entrenar las redes neuronales cu谩nticas utilizando el motor de diferenciaci贸n autom谩tica de PyTorch (`torch.autograd``, `enlace \n"
"<https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html>`__) para tareas simples de clasificaci贸n y regresi贸n."

#: ../../tutorials/05_torch_connector.ipynb:21
msgid "`Classification <#1.-Classification>`__"
msgstr "`Clasificaci贸n <#1.-Classification>`__"

#: ../../tutorials/05_torch_connector.ipynb:23
msgid "Classification with PyTorch and ``OpflowQNN``"
msgstr "Clasificaci贸n con PyTorch y ``OpflowQNN``"

#: ../../tutorials/05_torch_connector.ipynb:24
msgid "Classification with PyTorch and ``CircuitQNN``"
msgstr "Clasificaci贸n con PyTorch y ``CircuitQNN``"

#: ../../tutorials/05_torch_connector.ipynb:26
msgid "`Regression <#2.-Regression>`__"
msgstr "`Regresi贸n <#2.-Regression>`__"

#: ../../tutorials/05_torch_connector.ipynb:28
msgid "Regression with PyTorch and ``OpflowQNN``"
msgstr "Regresi贸n con PyTorch y ``OpflowQNN``"

#: ../../tutorials/05_torch_connector.ipynb:30
msgid "`Part 2: MNIST Classification, Hybrid QNNs <#Part-2:-MNIST-Classification,-Hybrid-QNNs>`__"
msgstr "`Parte 2: Clasificaci贸n MNIST, QNNs H铆bridas <#Part-2:-MNIST-Classification,-Hybrid-QNNs>`__"

#: ../../tutorials/05_torch_connector.ipynb:32
msgid "The second part of this tutorial illustrates how to embed a (Quantum) ``NeuralNetwork`` into a target PyTorch workflow (in this case, a typical CNN architecture) to classify MNIST data in a hybrid quantum-classical manner."
msgstr "La segunda parte de este tutorial ilustra c贸mo incrustar una ``NeuralNetwork`` (Cu谩ntica) en un flujo de trabajo PyTorch de destino (en este caso, una arquitectura CNN t铆pica) para clasificar los datos MNIST de una manera h铆brida cu谩ntica-cl谩sica."

#: ../../tutorials/05_torch_connector.ipynb:74
msgid "Part 1: Simple Classification & Regression"
msgstr "Parte 1: Clasificaci贸n y Regresi贸n Simples"

#: ../../tutorials/05_torch_connector.ipynb:86
msgid "1. Classification"
msgstr "1. Clasificaci贸n"

#: ../../tutorials/05_torch_connector.ipynb:88
msgid "First, we show how ``TorchConnector`` allows to train a Quantum ``NeuralNetwork`` to solve a classification tasks using PyTorchs automatic differentiation engine. In order to illustrate this, we will perform **binary classification** on a randomly generated dataset."
msgstr "Primero, mostramos c贸mo ``TorchConnector`` permite entrenar una ``NeuralNetwork`` Cu谩ntica para resolver tareas de clasificaci贸n utilizando el motor de diferenciaci贸n autom谩tica de PyTorch. Para ilustrar esto, realizaremos una **clasificaci贸n binaria** en un conjunto de datos generado aleatoriamente."

#: ../../tutorials/05_torch_connector.ipynb:144
msgid "A. Classification with PyTorch and ``OpflowQNN``"
msgstr "A. Clasificaci贸n con PyTorch y ``OpflowQNN``"

#: ../../tutorials/05_torch_connector.ipynb:146
msgid "Linking an ``OpflowQNN`` to PyTorch is relatively straightforward. Here we illustrate this using the ``TwoLayerQNN``, a sub-case of ``OpflowQNN`` introduced in previous tutorials."
msgstr "Vincular un ``OpflowQNN`` a PyTorch es relativamente sencillo. Aqu铆 ilustramos esto usando el ``TwoLayerQNN``, un sub-caso de ``OpflowQNN`` introducido en tutoriales anteriores."

#: ../../tutorials/05_torch_connector.ipynb:254
msgid "Optimizer"
msgstr "Optimizador"

#: ../../tutorials/05_torch_connector.ipynb:256
msgid "The choice of optimizer for training any machine learning model can be crucial in determining the success of our trainings outcome. When using ``TorchConnector``, we get access to all of the optimizer algorithms defined in the [``torch.optim``] package (`link <https://pytorch.org/docs/stable/optim.html>`__). Some of the most famous algorithms used in popular machine learning architectures include *Adam*, *SGD*, or *Adagrad*. However, for this tutorial we will be using the L-BFGS algorithm (``torch.optim.LBFGS``), one of the most well know second-order optimization algorithms for numerical optimization."
msgstr "La elecci贸n del optimizador para entrenar cualquier modelo de machine learning puede ser crucial para determinar el 茅xito del resultado de nuestro entrenamiento. Cuando usamos ``TorchConnector``, obtenemos acceso a todos los algoritmos del optimizador definidos en el paquete [``torch.optim``] (`enlace <https://pytorch.org/docs/stable/optim.html>`__). Algunos de los algoritmos m谩s famosos utilizados en las arquitecturas de machine learning populares incluyen *Adam*, *SGD*, o *Adagrad*. Sin embargo, para este tutorial usaremos el algoritmo L-BFGS (``torch.optim.LBFGS``), uno de los algoritmos de optimizaci贸n de segundo orden m谩s conocidos para la optimizaci贸n num茅rica."

#: ../../tutorials/05_torch_connector.ipynb:260
msgid "Loss Function"
msgstr "Funci贸n de P茅rdida"

#: ../../tutorials/05_torch_connector.ipynb:262
msgid "As for the loss function, we can also take advantage of PyTorchs pre-defined modules from ``torch.nn``, such as the `Cross-Entropy <https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html>`__ or `Mean Squared Error <https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html>`__ losses."
msgstr "En cuanto a la funci贸n de p茅rdida, tambi茅n podemos aprovechar los m贸dulos predefinidos de PyTorch desde ``torch.nn``, como las p茅rdidas `Cross-Entropy <https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html>`__ o `Mean Squared Error <https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html>`__."

#: ../../tutorials/05_torch_connector.ipynb:264
msgid "** Clarification :** In classical machine learning, the general rule of thumb is to apply a Cross-Entropy loss to classification tasks, and MSE loss to regression tasks. However, this recommendation is given under the assumption that the output of the classification network is a class probability value in the [0,1] range (usually this is achieved through a Softmax layer). Because the following example for ``TwoLayerQNN`` does not include such layer, and we dont apply any mapping to the output (the following section shows an example of application of parity mapping with ``CircuitQNNs``), the QNNs output can take any value in the range [-1,1]. In case you were wondering, this is the reason why this particular example uses MSELoss for classification despite it not being the norm (but we encourage you to experiment with different loss functions and see how they can impact training results)."
msgstr "** Aclaraci贸n:** En machine learning cl谩sico, la regla general es aplicar una p茅rdida de entrop铆a cruzada (Cross-Entropy) a las tareas de clasificaci贸n y una p茅rdida de MSE a las tareas de regresi贸n. Sin embargo, esta recomendaci贸n se da bajo el supuesto de que la salida de la red de clasificaci贸n es un valor de probabilidad de clase en el rango [0,1] (generalmente esto se logra a trav茅s de una capa Softmax). Debido a que el siguiente ejemplo para ``TwoLayerQNN`` no incluye dicha capa, y no aplicamos ning煤n mapeo a la salida (la siguiente secci贸n muestra un ejemplo de aplicaci贸n de mapeo de paridad con ``CircuitQNNs``), la salida de la QNN puede tomar cualquier valor en el rango [-1,1]. En caso de que te lo preguntes, esta es la raz贸n por la que este ejemplo en particular usa MSELoss para la clasificaci贸n a pesar de que no es la norma (pero te alentamos a experimentar con diferentes funciones de p茅rdida y ver c贸mo pueden afectar los resultados del entrenamiento)."

#: ../../tutorials/05_torch_connector.ipynb:442
#: ../../tutorials/05_torch_connector.ipynb:674
msgid "The red circles indicate wrongly classified data points."
msgstr "Los c铆rculos rojos indican puntos de datos clasificados incorrectamente."

#: ../../tutorials/05_torch_connector.ipynb:454
msgid "B. Classification with PyTorch and ``CircuitQNN``"
msgstr "B. Clasificaci贸n con PyTorch y ``CircuitQNN``"

#: ../../tutorials/05_torch_connector.ipynb:456
msgid "Linking an ``CircuitQNN`` to PyTorch requires a bit more attention than ``OpflowQNN``. Without the correct setup, backpropagation is not possible."
msgstr "Vincular un ``CircuitQNN`` a PyTorch requiere un poco m谩s de atenci贸n que con ``OpflowQNN``. Sin la configuraci贸n correcta, la propagaci贸n hacia atr谩s (backpropagation) no es posible."

#: ../../tutorials/05_torch_connector.ipynb:458
msgid "In particular, we must make sure that we are returning a dense array of probabilities in the networks forward pass (``sparse=False``). This parameter is set up to ``False`` by default, so we just have to make sure that it has not been changed."
msgstr "En particular, debemos asegurarnos de que estamos devolviendo un arreglo denso de probabilidades en el paso hacia adelante de la red (``sparse=False``). Este par谩metro est谩 configurado en ``False`` de forma predeterminada, por lo que solo debemos asegurarnos de que no haya sido modificado."

#: ../../tutorials/05_torch_connector.ipynb:460
msgid "**锔 Attention:** If we define a custom interpret function ( in the example: ``parity``), we must remember to explicitly provide the desired output shape ( in the example: ``2``). For more info on the initial parameter setup for ``CircuitQNN``, please check out the `official qiskit documentation <https://qiskit.org/documentation/machine-learning/stubs/qiskit_machine_learning.neural_networks.CircuitQNN.html>`__."
msgstr "**锔 Atenci贸n:** Si definimos una funci贸n de interpretaci贸n personalizada (en el ejemplo: ``parity``), debemos recordar proporcionar expl铆citamente la forma de salida deseada (en el ejemplo: ``2``). Para obtener m谩s informaci贸n sobre la configuraci贸n inicial de par谩metros para ``CircuitQNN``, consulta la `documentaci贸n oficial de qiskit <https://qiskit.org/documentation/machine-learning/stubs/qiskit_machine_learning.neural_networks.CircuitQNN.html>`__."

#: ../../tutorials/05_torch_connector.ipynb:523
#: ../../tutorials/05_torch_connector.ipynb:815
msgid "For a reminder on optimizer and loss function choices, you can go back to `this section <#Optimizer>`__."
msgstr "Para obtener un recordatorio sobre el optimizador y las opciones de funciones de p茅rdida, puedes volver a `esta secci贸n <#Optimizer>`__."

#: ../../tutorials/05_torch_connector.ipynb:686
msgid "2. Regression"
msgstr "2. Regresi贸n"

#: ../../tutorials/05_torch_connector.ipynb:688
msgid "We use a model based on the ``TwoLayerQNN`` to also illustrate how to perform a regression task. The chosen dataset in this case is randomly generated following a sine wave."
msgstr "Usamos un modelo basado en el ``TwoLayerQNN`` para ilustrar tambi茅n c贸mo realizar una tarea de regresi贸n. El conjunto de datos elegido en este caso se genera aleatoriamente siguiendo una onda sinusoidal."

#: ../../tutorials/05_torch_connector.ipynb:730
msgid "A. Regression with PyTorch and ``OpflowQNN``"
msgstr "A. Regresi贸n con PyTorch y ``OpflowQNN``"

#: ../../tutorials/05_torch_connector.ipynb:741
msgid "The network definition and training loop will be analogous to those of the classification task using ``TwoLayerQNN``. In this case, we define our own feature map and ansatz, instead of using the default values."
msgstr "La definici贸n de red y el ciclo de entrenamiento ser谩n an谩logos a los de la tarea de clasificaci贸n usando ``TwoLayerQNN``. En este caso, definimos nuestro propio mapa de caracter铆sticas y ansatz, en lugar de usar los valores predeterminados."

#: ../../tutorials/05_torch_connector.ipynb:963
msgid "Part 2: MNIST Classification, Hybrid QNNs"
msgstr "Parte 2: Clasificaci贸n MNIST, QNNs H铆bridas"

#: ../../tutorials/05_torch_connector.ipynb:965
msgid "In this second part, we show how to leverage a hybrid quantum-classical neural network using ``TorchConnector``, to perform a more complex image classification task on the MNIST handwritten digits dataset."
msgstr "En esta segunda parte, mostramos c贸mo aprovechar una red neuronal h铆brida cu谩ntica-cl谩sica utilizando ``TorchConnector``, para realizar una tarea de clasificaci贸n de im谩genes m谩s compleja en el conjunto de datos de d铆gitos manuscritos del MNIST."

#: ../../tutorials/05_torch_connector.ipynb:967
msgid "For a more detailed (pre-``TorchConnector``) explanation on hybrid quantum-classical neural networks, you can check out the corresponding section in the `Qiskit Textbook <https://qiskit.org/textbook/ch-machine-learning/machine-learning-qiskit-pytorch.html>`__."
msgstr "Para obtener una explicaci贸n m谩s detallada (pre-``TorchConnector``) sobre las redes neuronales h铆bridas cu谩nticas-cl谩sicas, puedes consultar la secci贸n correspondiente en el `Libro de texto de Qiskit <https://qiskit.org/textbook/ch-machine-learning/machine-learning-qiskit-pytorch.html>`__."

#: ../../tutorials/05_torch_connector.ipynb:996
msgid "Step 1: Defining Data-loaders for train and test"
msgstr "Paso 1: Definici贸n de cargadores de datos para entrenamiento y prueba"

#: ../../tutorials/05_torch_connector.ipynb:1007
msgid "We take advantage of the ``torchvision`` `API <https://pytorch.org/vision/stable/datasets.html>`__ to directly load a subset of the `MNIST dataset <https://en.wikipedia.org/wiki/MNIST_database>`__ and define torch ``DataLoader``\\ s (`link <https://pytorch.org/docs/stable/data.html>`__) for train and test."
msgstr "Aprovechamos la `API <https://pytorch.org/vision/stable/datasets.html>`__ de ``torchvision`` para cargar directamente un subconjunto del `conjunto de datos MNIST<https://en.wikipedia.org/wiki/MNIST_database>`__ y definir los ``DataLoader``\\ s de torch (`enlace <https://pytorch.org/docs/stable/data.html>`__) para entrenar y probar."

#: ../../tutorials/05_torch_connector.ipynb:1048
msgid "If we perform a quick visualization we can see that the train dataset consists of images of handwritten 0s and 1s."
msgstr "Si realizamos una visualizaci贸n r谩pida, podemos ver que el conjunto de datos de entrenamiento consta de im谩genes de 0s y 1s escritos a mano."

#: ../../tutorials/05_torch_connector.ipynb:1120
msgid "Step 2: Defining the QNN and Hybrid Model"
msgstr "Paso 2: Definici贸n del Modelo H铆brido y de la QNN"

#: ../../tutorials/05_torch_connector.ipynb:1131
msgid "This second step shows the power of the ``TorchConnector``. After defining our quantum neural network layer (in this case, a ``TwoLayerQNN``), we can embed it into a layer in our torch ``Module`` by initializing a torch connector as ``TorchConnector(qnn)``."
msgstr "Este segundo paso muestra el poder del ``TorchConnector``. Despu茅s de definir nuestra capa de red neuronal cu谩ntica (en este caso, una ``TwoLayerQNN``), podemos incrustarla en una capa en nuestro ``Module`` de torch, al inicializar un conector torch como ``TorchConnector(qnn)``."

#: ../../tutorials/05_torch_connector.ipynb:1133
msgid "**锔 Attention:** In order to have an adequate gradient backpropagation in hybrid models, we MUST set the initial parameter ``input_gradients`` to TRUE during the qnn initialization."
msgstr "**锔 Atenci贸n:** Para tener un gradiente de propagaci贸n hacia atr谩s (backpropagation) adecuado en modelos h铆bridos, DEBEMOS establecer el par谩metro inicial ``input_gradients`` en TRUE durante la inicializaci贸n de la qnn."

#: ../../tutorials/05_torch_connector.ipynb:1235
msgid "Step 3: Training"
msgstr "Paso 3: Entrenamiento"

#: ../../tutorials/05_torch_connector.ipynb:1337
msgid "Step 4: Evaluation"
msgstr "Paso 4: Evaluaci贸n"

#: ../../tutorials/05_torch_connector.ipynb:1440
msgid " **You are now able to experiment with your own hybrid datasets and architectures using Qiskit Machine Learning.** **Good Luck!**"
msgstr " **Ahora puedes experimentar con tus propios conjuntos de datos y arquitecturas h铆bridas utilizando Qiskit Machine Learning.** **隆Buena suerte!**"

#: ../../tutorials/index.rst:3
msgid "Machine Learning Tutorials"
msgstr "Tutoriales de Machine Learning"

