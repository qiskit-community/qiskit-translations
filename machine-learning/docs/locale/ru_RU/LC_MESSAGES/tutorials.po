msgid ""
msgstr ""
"Project-Id-Version: qiskit-docs\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-07-12 22:21+0000\n"
"PO-Revision-Date: 2021-07-12 23:07\n"
"Last-Translator: \n"
"Language-Team: Russian\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"
"Plural-Forms: nplurals=4; plural=((n%10==1 && n%100!=11) ? 0 : ((n%10 >= 2 && n%10 <=4 && (n%100 < 12 || n%100 > 14)) ? 1 : ((n%10 == 0 || (n%10 >= 5 && n%10 <=9)) || (n%100 >= 11 && n%100 <= 14)) ? 2 : 3));\n"
"X-Crowdin-Project: qiskit-docs\n"
"X-Crowdin-Project-ID: 369271\n"
"X-Crowdin-Language: ru\n"
"X-Crowdin-File: /master/machine-learning/docs/locale/en/LC_MESSAGES/tutorials.po\n"
"X-Crowdin-File-ID: 9528\n"
"Language: ru_RU\n"

#: ../../tutorials/01_neural_networks.ipynb:13
#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:13
#: ../../tutorials/03_quantum_kernel.ipynb:13
#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:13
#: ../../tutorials/05_torch_connector.ipynb:13
msgid "Run interactively in jupyter notebook."
msgstr "Запускается интерактивно в jupyter notebook."

#: ../../tutorials/01_neural_networks.ipynb:9
msgid "Quantum Neural Networks"
msgstr "Квантовые нейронные сети"

#: ../../tutorials/01_neural_networks.ipynb:11
msgid "This notebook demonstrates the different generic quantum neural network (QNN) implementations provided in Qiskit Machine Learning. The networks are meant as application-agnostic computational units that can be used for many different use cases. Depending on the application, a particular type of network might more or less suitable and might require to be set up in a particular way. The following different available neural networks will now be discussed in more detail:"
msgstr "Этот блокнот демонстрирует различные общие реализации квантовых нейронных сетей (QNN), предоставляемые в Qiskit Machine Learning. Сети предназначены для использования в качестве вычислительных единиц, не зависящих от области применения, которые могут быть использованы для множества различных случаев. В зависимости от задачи, конкретный тип сети может быть более или менее подходящим и может потребовать определенной настройки. Ниже будут более подробно рассмотрены различные доступные нейронные сети:"

#: ../../tutorials/01_neural_networks.ipynb:13
msgid "``NeuralNetwork``: The interface for neural networks."
msgstr "` ` NeuralNetwork ` `: Интерфейс для нейронных сетей."

#: ../../tutorials/01_neural_networks.ipynb:14
msgid "``OpflowQNN``: A network based on the evaluation of quantum mechanical observables."
msgstr "``OpflowQN``: Сеть, основанная на оценке квантово-механических наблюдений."

#: ../../tutorials/01_neural_networks.ipynb:15
msgid "``TwoLayerQNN``: A special ``OpflowQNN`` implementation for convenience."
msgstr "``TwoLayerQNN``: Специальная реализация ``OpflowQNN`` для удобства."

#: ../../tutorials/01_neural_networks.ipynb:16
msgid "``CircuitQNN``: A network based on the samples resulting from measuring a quantum circuit."
msgstr "\" CircuitQNN ` `: сеть, основанная на образцах, полученных в результате измерения квантовой цепи."

#: ../../tutorials/01_neural_networks.ipynb:64
msgid "1. ``NeuralNetwork``"
msgstr "1. ` ` NeuralNetwork ` `"

#: ../../tutorials/01_neural_networks.ipynb:66
msgid "The ``NeuralNetwork`` represents the interface for all neural networks available in Qiskit Machine Learning. It exposes a forward and a backward pass taking the data samples and trainable weights as input. A ``NeuralNetwork`` does not contain any training capabilities, these are pushed to the actual algorithms / applications. Thus, a ``NeuralNetwork`` also does not store the values for trainable weights. In the following, different implementations of this interfaces are introduced."
msgstr "``NeuralNetwork`` это интерфейс для всех нейронных сетей, доступных в Qiskit Machine Learning. Он предоставляет собой прямой и обратный проход, принимающий на вход образцы данных и обучаемые веса. ``NeuralNetwork`` не содержит никаких возможностей для обучения, они предоставляются конкретными алгоритмами / приложениями. Таким образом, ``NeuralNetwork`` также не хранит значения обучаемых весов. Далее будут представлены различные реализации этих интерфейсов."

#: ../../tutorials/01_neural_networks.ipynb:68
msgid "Suppose a ``NeuralNetwork`` called ``nn``. Then, the ``nn.forward(input, weights)`` pass takes either flat inputs for the data and weights of size ``nn.num_inputs`` and ``nn.num_weights``, respectively. ``NeuralNetwork`` supports batching of inputs and returns batches of output of the corresponding shape."
msgstr "Предположим, что ``NeuralNetwork`` называется ``nn``. Затем проход ``nn.forward(input, weights)`` принимает либо одинаковые входные значения для данных и весов размера ``nn. um_inputs`` и ``nn.num_weights``, соответственно. ``NeuralNetwork`` поддерживает группировку входных данных и возвращает набор выходных данных соответствующей формы."

#: ../../tutorials/01_neural_networks.ipynb:80
msgid "2. ``OpflowQNN``"
msgstr "2. ` ` OpflowQNN ` `"

#: ../../tutorials/01_neural_networks.ipynb:82
msgid "The ``OpflowQNN`` takes a (parametrized) operator from Qiskit and leverages Qiskit’s gradient framework to provide the backward pass. Such an operator can for instance be an expected value of a quantum mechanical observable with respect to a parametrized quantum state. The Parameters can be used to load classical data as well as represent trainable weights. The ``OpflowQNN`` also allows lists of operators and more complex structures to construct more complex QNNs."
msgstr "Функция ``OpflowQNN`` принимает (параметризованный) оператор из Qiskit и использует градиентную схему Qiskit для обеспечения обратного прохода. Таким оператором может быть, например, математическое ожидание квантово-механической наблюдаемой величины относительно параметризованного квантового состояния. Параметры могут быть использованы для загрузки классических данных, а также для представления обучаемых весов. В ``OpflowQNN`` также допускаются списки операторов и более сложные структуры для построения более сложных QNN."

#: ../../tutorials/01_neural_networks.ipynb:321
msgid "Combining multiple observables in a ``ListOp`` also allows to create more complex QNNs"
msgstr "Объединение нескольких наблюдаемых в ``ListOp`` также позволяет создавать более сложные QNN"

#: ../../tutorials/01_neural_networks.ipynb:412
msgid "3. ``TwoLayerQNN``"
msgstr "3. ` ` TwoLayerQNN ` `"

#: ../../tutorials/01_neural_networks.ipynb:414
msgid "The ``TwoLayerQNN`` is a special ``OpflowQNN`` on :math:`n` qubits that consists of first a feature map to insert data and second an ansatz that is trained. The default observable is :math:`Z^{\\otimes n}`, i.e., parity."
msgstr "``TwoLayerQNN` - это специальная ``OpflowQNN`` на :math:`n` кубитах, которая состоит из: (1) карты признаков для вставки данных и (2) анзаца, который обучается. Наблюдаемой по умолчанию является :math:`Z^{\\otimes n}`, т.е. четность."

#: ../../tutorials/01_neural_networks.ipynb:612
msgid "4. ``CircuitQNN``"
msgstr "4. ` ` CircuitQNN ` `"

#: ../../tutorials/01_neural_networks.ipynb:614
msgid "The ``CircuitQNN`` is based on a (parametrized) ``QuantumCircuit``. This can take input as well as weight parameters and produces samples from the measurement. The samples can either be interpreted as probabilities of measuring the integer index corresponding to a bitstring or directly as a batch of binary output. In the case of probabilities, gradients can be estimated efficiently and the ``CircuitQNN`` provides a backward pass as well. In case of samples, differentiation is not possible and the backward pass returns ``(None, None)``."
msgstr "В основе ``CircuitQNN`` лежит (параметризованная) ``QuantumCircuit``. Она может принимать как входные, так и весовые параметры и производит выборки из измерений. Выборки могут быть интерпретированы как вероятности измерения целочисленного индекса, соответствующего битовой строке, или непосредственно как пакет двоичного вывода. В случае вероятностей градиенты могут быть оценены эффективно, а ``CircuitQNN`` обеспечивает также обратный проход. В случае выборок дифференцирование невозможно, и обратный проход возвращает ``(None, None)``."

#: ../../tutorials/01_neural_networks.ipynb:617
msgid "Further, the ``CircuitQNN`` allows to specify an ``interpret`` function to post-process the samples. This is expected to take a measured integer (from a bitstring) and map it to a new index, i.e. non-negative integer. In this case, the output shape needs to be provided and the probabilities are aggregated accordingly."
msgstr ""

#: ../../tutorials/01_neural_networks.ipynb:619
msgid "A ``CircuitQNN`` can be configured to return sparse as well as dense probability vectors. If no ``interpret`` function is used, the dimension of the probability vector scales exponentially with the number of qubits and a sparse recommendation is usually recommended. In case of an ``interpret`` function it depends on the expected outcome. If, for instance, an index is mapped to the parity of the corresponding bitstring, i.e., to 0 or 1, a dense output makes sense and the result will be a probability vector of length 2."
msgstr "``CircuitQNN`` может быть настроен на возврат как разреженных, так и плотных векторов вероятности. Если не используется функция ``interpret'', размерность вектора вероятности экспоненциально растет с числом кубитов, поэтому обычно рекомендуется использовать разреженную рекомендацию. В случае ``interpret'' (интерпретирующей) функции это зависит от ожидаемого результата. Если, например, индекс сопоставляется с четностью соответствующей битовой строки, т.е. с 0 или 1, то имеет смысл плотный вывод, и результатом будет вектор вероятностей длины 2."

#: ../../tutorials/01_neural_networks.ipynb:662
msgid "4.1 Output: sparse integer probabilities"
msgstr "4.1 Выходные данные: разреженные целочисленные вероятности"

#: ../../tutorials/01_neural_networks.ipynb:761
msgid "4.2 Output: dense parity probabilities"
msgstr "4.1 Выходные данные: плотные четные вероятности"

#: ../../tutorials/01_neural_networks.ipynb:869
msgid "4.3 Output: Samples"
msgstr "4.3 Выходные данные: выборки"

#: ../../tutorials/01_neural_networks.ipynb:985
msgid "4.4 Output: Parity Samples"
msgstr "4.4 Выходные данные: четные выборки"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:9
msgid "Neural Network Classifier & Regressor"
msgstr "Классификатор нейросети и регрессор"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:11
msgid "In this tutorial we show how the ``NeuralNetworkClassifier`` and ``NeuralNetworkRegressor`` are used. Both take as an input a (Quantum) ``NeuralNetwork`` and leverage it in a specific context. In both cases we also provide a pre-configured variant for convenience, the Variational Quantum Classifier (``VQC``) and Variational Quantum Regressor (``VQR``). The tutorial is structured as follows:"
msgstr "В этом разделе мы покажем, как используются ``NeuralNetworkClassifier'' и ``NeuralNetworkRegressor''. Оба принимают на вход (квантовую) ``NeuralNetwork`` и используют ее в определенном контексте. В обоих случаях мы также предоставляем предварительно настроенные варианты для удобства, Вариационный Квантовый Классификатор (``VQC``) и Вариационный Квантовый Регрессор (``VQR``). Руководство построено следующим образом:"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:13
msgid "`Classification <#Classification>`__"
msgstr "`Классификация <#Classification>`__"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:15
msgid "Classification with an ``OpflowQNN``"
msgstr "Классификация с помощью \" OpflowQNN ` `"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:16
#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:249
msgid "Classification with a ``CircuitQNN``"
msgstr "Классификация с помощью \" CircuitQNN ` `"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:17
#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:398
msgid "Variational Quantum Classifier (``VQC``)"
msgstr "Вариационный квантовый классификатор (``VQC``)"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:19
msgid "`Regression <#Regression>`__"
msgstr "` Регрессия <#Regression> ` __"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:21
#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:539
msgid "Regression with an ``OpflowQNN``"
msgstr "Регрессия с ` ` OpflowQNN ` `"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:22
msgid "Variational Quantum Regressor (``VQR``)"
msgstr "Вариационный Квантовый Регрессор (``VQR``)"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:70
#: ../../tutorials/03_quantum_kernel.ipynb:53
msgid "Classification"
msgstr "Классификация"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:72
msgid "We prepare a simple classification dataset to illustrate the following algorithms."
msgstr "Мы создаем простой набор данных для классификации, чтобы проиллюстрировать следующие алгоритмы."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:117
msgid "Classification with the an ``OpflowQNN``"
msgstr "Классификация с помощью ``OpflowQNN``"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:119
msgid "First we show how an ``OpflowQNN`` can be used for classification within a ``NeuralNetworkClassifier``. In this context, the ``OpflowQNN`` is expected to return one-dimensional output in :math:`[-1, +1]`. This only works for binary classification and we assign the two classes to :math:`\\{-1, +1\\}`. For convenience, we use the ``TwoLayerQNN``, which is a special type of ``OpflowQNN`` defined via a feature map and an ansatz."
msgstr "Сначала мы покажем, как ``OpflowQNN`` можно использовать для классификации в рамках ``NeuralNetworkClassifier``. В этом случае ожидается, что ``OpflowQNN`` вернет одномерный результат в :math:`[-1, +1]``. Это работает только для бинарной классификации, и мы присваиваем два класса :math:`\\{-1, +1\\}`. Для удобства мы используем ``TwoLayerQNN``, который является специальным типом ``OpflowQNN``, определенным через карту признаков и анзац."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:251
msgid "Next we show how a ``CircuitQNN`` can be used for classification within a ``NeuralNetworkClassifier``. In this context, the ``CircuitQNN`` is expected to return :math:`d`-dimensional probability vector as output, where :math:`d` denotes the number of classes. Sampling from a ``QuantumCircuit`` automatically results in a probability distribution and we just need to define a mapping from the measured bitstrings to the different classes. For binary classification we use the parity mapping."
msgstr "Далее мы покажем, как ``CircuitQNN`` можно использовать для классификации в ``NeuralNetworkClassifier``. В этом случае ожидается, что ``CircuitQNN`` возвращает :math:`d`-размерный вектор вероятности в качестве вывода, где :math:`d` обозначает число классов. Выборка из ``QuantumCircuit`` автоматически приводит к распределению вероятностей, и нам просто нужно определить соответствие между измеренными битами и разными классами. Для двоичной классификации используется преобра отображение по четности."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:400
msgid "The ``VQC`` is a special variant of the ``NeuralNetworkClassifier`` with a ``CircuitQNN``. It applies a parity mapping (or extensions to multiple classes) to map from the bitstring to the classification, which results in a probability vector, which is interpreted as a one-hot encoded result. By default, it applies this the ``CrossEntropyLoss`` function that expects labels given in one-hot encoded format and will return predictions in that format too."
msgstr "``VQC`` - это специальный вариант ``NeuralNetworkClassifier`` с ``CircuitQNN``. Он использует отображение по четности (или расширения для нескольких классов) для отображения от битовой строки к классификации, в результате чего получается вектор вероятностей, который интерпретируется как результат, закодированный унитарным кодом. По умолчанию применяется функция ``CrossEntropyLoss``, которая ожидает метки в формате унитарного кода и возвращает предсказания в этом формате."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:496
msgid "Regression"
msgstr "Регрессия"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:498
msgid "We prepare a simple regression dataset to illustrate the following algorithms."
msgstr "Мы создаем простой регрессионный набор данных для иллюстрации следующих алгоритмов."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:541
msgid "Here we restrict to regression with an ``OpflowQNN`` that returns values in :math:`[-1, +1]`. More complex and also multi-dimensional models could be constructed, also based on ``CircuitQNN`` but that exceeds the scope of this tutorial."
msgstr "Здесь мы ограничимся регрессией с ``OpflowQNN``, которая возвращает значения в :math:`[-1, +1]``. Можно построить более сложные и многомерные модели, также основанные на ``CircuitQNN``, но это выходит за рамки данного руководства."

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:648
msgid "Regression with the Variational Quantum Regressor (``VQR``)"
msgstr "Регрессия с помощью Вариационной Квантовой Регрессии (``VQR``)"

#: ../../tutorials/02_neural_network_classifier_and_regressor.ipynb:650
msgid "Similar to the ``VQC`` for classification, the ``VQR`` is a special variant of the ``NeuralNetworkRegressor`` with a ``OpflowQNN``. By default it considers the ``L2Loss`` function to minimize the mean squared error between predictions and targets."
msgstr "По аналогии с ``VQC`` для классификации, ``VQR`` - это специальный вариант ``NeuralNetworkRegressor`` с ``OpflowQNN``. По умолчанию он рассматривает функцию ``L2Loss`` для минимизации средней квадратичной ошибки между предсказаниями и целями."

#: ../../tutorials/03_quantum_kernel.ipynb:9
msgid "Quantum Kernel Machine Learning"
msgstr "Квантовый ядерный метод машинного обучения"

#: ../../tutorials/03_quantum_kernel.ipynb:11
msgid "The general task of machine learning is to find and study patterns in data. For many datasets, the datapoints are better understood in a higher dimensional feature space, through the use of a kernel function: :math:`k(\\vec{x}_i, \\vec{x}_j) = \\langle f(\\vec{x}_i), f(\\vec{x}_j) \\rangle` where :math:`k` is the kernel function, :math:`\\vec{x}_i, \\vec{x}_j` are :math:`n` dimensional inputs, :math:`f` is a map from :math:`n`-dimension to :math:`m`-dimension space and :math:`\\langle a,b \\rangle` denotes the dot product. When considering finite data, a kernel function can be represented as a matrix: :math:`K_{ij} = k(\\vec{x}_i,\\vec{x}_j)`."
msgstr "Общая задача машинного обучения заключается в поиске и изучении закономерностей в данных. Для многих наборов данных элементы данных (datapoints) лучше распознаются в большем пространстве измерений, через использование ядерной функции: :math:`k(\\vec{x}_i, \\vec{x}_j) = \\langle f(\\vec{x}_i), f(\\vec{x}_j) \\rangle`, где :math:`k` - ядерная функция, :math:`\\vec{x}_i, \\vec{x}_j` - входные данные с количеством измерений равным :math:`n`, :math:`f` — это преобразование от :math:`n`-мерного до :math:`m`-мерного пространства и :math:`\\langle a,b \\rangle` обозначает точечный результат. При рассмотрении конечных данных, ядерная функция может быть представлена в виде матрицы: :math:`K_{ij} = k(\\vec{x}_i,\\vec{x}_j)`."

#: ../../tutorials/03_quantum_kernel.ipynb:14
msgid "In quantum kernel machine learning, a quantum feature map :math:`\\phi(\\vec{x})` is used to map a classical feature vector :math:`\\vec{x}` to a quantum Hilbert space, :math:`| \\phi(\\vec{x})\\rangle \\langle \\phi(\\vec{x})|`, such that :math:`K_{ij} = \\left| \\langle \\phi^\\dagger(\\vec{x}_j)| \\phi(\\vec{x}_i) \\rangle \\right|^{2}`. See `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__ for more details."
msgstr "В квантовом машинном обучении методом ядерной функции, преобразование :math:`\\phi(\\vec{x})` используется для сопоставления классической векторной функции :math:`\\vec{x} с квантовым Гильбертовым пространством, :math:`| \\phi(\\vec{x})\\rangle \\langle \\phi(\\vec{x})|`, таким, что :math:`K_{ij} = \\left| \\langle \\phi^\\dagger(\\vec{x}_j)| \\phi(\\vec{x}_i) \\rangle \\right|^{2}`. См. `Контролируемое обучение с квантово расширенными функциональными пространствами <https://arxiv.org/pdf/1804.11326.pdf>`__ для более подробной информации."

#: ../../tutorials/03_quantum_kernel.ipynb:16
msgid "In this notebook, we use ``qiskit`` to calculate a kernel matrix using a quantum feature map, then use this kernel matrix in ``scikit-learn`` classification and clustering algorithms."
msgstr "В данном блокноте мы используем ``qiskit`` для вычисления ядерной матрицы с использованием квантовой функции преобразования, а затем используем эту матрицу в алгоритмах классификации и кластеризации ``scikit-learn``."

#: ../../tutorials/03_quantum_kernel.ipynb:55
msgid "For our classification example, we will use the *ad hoc dataset* as described in `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__, and the ``scikit-learn`` `support vector machine <https://scikit-learn.org/stable/modules/svm.html>`__ classification (``svc``) algorithm."
msgstr "Для нашего примера классификации мы будем использовать набор данных *ad hoc dataset* как в статье ``Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__, и алгоритм ``scikit-learn``, `поддерживающий алгоритм векторной машинной <https://scikit-learn.org/stable/modules/svm.html>`__ классификации (``svc``)."

#: ../../tutorials/03_quantum_kernel.ipynb:111
msgid "With our training and testing datasets ready, we set up the ``QuantumKernel`` class to calculate a kernel matrix using the `ZZFeatureMap <https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZZFeatureMap.html>`__, and the ``BasicAer`` ``qasm_simulator`` using 1024 shots."
msgstr "С нашими готовыми наборами данных для обучения и тестирования, мы настроим класс ``QuantumKernel`` для вычисления с помощью `ZFeatureMap <https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZZFeatureMap.html>`__, и ``BasicAer`` ``qasm_simulator`` используя 1024 вычисления."

#: ../../tutorials/03_quantum_kernel.ipynb:138
msgid "The ``scikit-learn`` ``svc`` algorithm allows us to define a `custom kernel <https://scikit-learn.org/stable/modules/svm.html#custom-kernels>`__ in two ways: by providing the kernel as a callable function or by precomputing the kernel matrix. We can do either of these using the ``QuantumKernel`` class in ``qiskit``."
msgstr "Алгоритм ``scikit-learn`` ``svc`` позволяет нам определить `пользовательское ядро <https://scikit-learn.org/stable/modules/svm.html#custom-kernels>`__ двумя способами: путем предоставления ядра в качестве вызываемой функции или путем предварительного вычисления ядерной матрицы. Мы можем выполнить любую из двух операций, используя класс ``QuantumKernel`` в ``qiskit``."

#: ../../tutorials/03_quantum_kernel.ipynb:140
msgid "The following code gives the kernel as a callable function:"
msgstr "Следующий код предоставляет ядро в качестве вызываемой функции:"

#: ../../tutorials/03_quantum_kernel.ipynb:184
msgid "The following code precomputes and plots the training and testing kernel matrices before providing them to the ``scikit-learn`` ``svc`` algorithm:"
msgstr "Следующий код предварительно вычисляет и выводит в графическом виде данные об обучении и тестирует ядерные матрицы, прежде чем передать их алгоритму ``scikit-learn`` ``svc`:"

#: ../../tutorials/03_quantum_kernel.ipynb:250
msgid "``qiskit`` also contains the ``qsvc`` class that extends the ``sklearn svc`` class, that can be used as follows:"
msgstr "``qiskit`` также содержит класс ``qsvc``, расширяющий класс ``sklearn svc``, который может быть использован следующим образом:"

#: ../../tutorials/03_quantum_kernel.ipynb:295
msgid "Clustering"
msgstr "Кластеризация"

#: ../../tutorials/03_quantum_kernel.ipynb:297
msgid "For our clustering example, we will again use the *ad hoc dataset* as described in `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__, and the ``scikit-learn`` ``spectral`` clustering algorithm."
msgstr "Для нашего примера кластеризации мы снова будем использовать набор данных *ad hoc dataset* из статьи `Supervised learning with quantum enhanced feature spaces <https://arxiv.org/pdf/1804.11326.pdf>`__, и алгоритм кластеризации ``scikit-learn`` ``spectral``."

#: ../../tutorials/03_quantum_kernel.ipynb:299
msgid "We will regenerate the dataset with a larger gap between the two classes, and as clustering is an unsupervised machine learning task, we don’t need a test sample."
msgstr "Мы создадим набор данных с большим разрывом между двумя классами, и, поскольку кластеризация является заданием неконтролируемого (unsupervised) машинного обучения, нам не нужна тестовая выборка."

#: ../../tutorials/03_quantum_kernel.ipynb:350
msgid "We again set up the ``QuantumKernel`` class to calculate a kernel matrix using the `ZZFeatureMap <https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZZFeatureMap.html>`__, and the BasicAer ``qasm_simulator`` using 1024 shots."
msgstr "Мы снова настроим класс ``QuantumKernel`` для вычисления ядерной матрицы, используя `ZFeatureMap <https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZZFeatureMap.html>`__, и BasicAer ``qasm_simulator`` с помощью 1024 вычислений."

#: ../../tutorials/03_quantum_kernel.ipynb:377
msgid "The scikit-learn spectral clustering algorithm allows us to define a [custom kernel] in two ways: by providing the kernel as a callable function or by precomputing the kernel matrix. Using the QuantumKernel class in qiskit, we can only use the latter."
msgstr "Изучаемый спектральный алгоритм кластеризации позволяет нам определить [пользовательское ядро] двумя способами: путем предоставления ядра в качестве вызываемой функции или путем предварительного вычисления ядерной матрицы. Используя класс QuantumKernel в qiskit, мы можем использовать только второе."

#: ../../tutorials/03_quantum_kernel.ipynb:379
msgid "The following code precomputes and plots the kernel matrices before providing it to the scikit-learn spectral clustering algorithm, and scoring the labels using normalized mutual information, since we a priori know the class labels."
msgstr ""

#: ../../tutorials/03_quantum_kernel.ipynb:439
msgid "``scikit-learn`` has other algorithms that can use a precomputed kernel matrix, here are a few:"
msgstr "``scikit-learn`` имеет и другие алгоритмы, использующие предварительно рассчитанную ядерную матрицу. Вот некоторые из них:"

#: ../../tutorials/03_quantum_kernel.ipynb:441
msgid "`Agglomerative clustering <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html>`__"
msgstr "`Агломеративная кластеризация <https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html>`__"

#: ../../tutorials/03_quantum_kernel.ipynb:442
msgid "`Support vector regression <https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html>`__"
msgstr "`Регрессия опорных векторов <https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html>`__"

#: ../../tutorials/03_quantum_kernel.ipynb:443
msgid "`Ridge regression <https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html>`__"
msgstr "`Хребтовая регрессия <https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html>`__"

#: ../../tutorials/03_quantum_kernel.ipynb:444
msgid "`Gaussian process regression <https://scikit-learn.org/stable/modules/gaussian_process.html>`__"
msgstr ""

#: ../../tutorials/03_quantum_kernel.ipynb:445
msgid "`Principal component analysis <https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html>`__"
msgstr "`Анализ основных компонентов <https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html>` __"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:9
msgid "qGANs for Loading Random Distributions"
msgstr "qGANs для загрузки случайных распределений"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:11
msgid "Given :math:`k`-dimensional data samples, we employ a quantum Generative Adversarial Network (qGAN) to learn the data’s underlying random distribution and to load it directly into a quantum state:"
msgstr "Дана :math:`k`-мерная выборки данных. Мы используем квантовую генеративное-состязательную сеть (quantum generative adversarial network), чтобы узнать данные, лежащие в основе случайного распределения и загрузить их непосредственно в квантовое состояние:"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:13
msgid "\\big| g_{\\theta}\\rangle = \\sum_{j=0}^{2^n-1} \\sqrt{p_{\\theta}^{j}}\\big| j \\rangle\n\n"
msgstr "\\big| g_{\\theta}\\rangle = \\sum_{j=0}^{2^n-1} \\sqrt{p_{\\theta}^{j}}\\big| j \\rangle\n\n"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:15
msgid "where :math:`p_{\\theta}^{j}` describe the occurrence probabilities of the basis states :math:`\\big| j\\rangle`."
msgstr "где :math:`p_{\\theta}^{j}` описывает вероятности появления базовых состояний :math:`\\big| j\\rangle`."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:17
msgid "The aim of the qGAN training is to generate a state :math:`\\big| g_{\\theta}\\rangle` where :math:`p_{\\theta}^{j}`, for :math:`j\\in \\left\\{0, \\ldots, {2^n-1} \\right\\}`, describe a probability distribution that is close to the distribution underlying the training data :math:`X=\\left\\{x^0, \\ldots, x^{k-1} \\right\\}`."
msgstr "Цель обучения qGAN — создать состояние :math:`\\big| g_{\\theta}\\rangle` где :math:`p_{\\theta}^{j}`, для :math:`j\\in \\left\\{0, \\ldots, {2^n-1} \\right\\}`, и описать распределение вероятностей, которое близко к распределению, лежащему в основе данных обучения :math:`X=\\left\\{x^0, \\ldots, x^{k-1} \\right\\}`."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:19
msgid "For further details please refer to `Quantum Generative Adversarial Networks for Learning and Loading Random Distributions <https://arxiv.org/abs/1904.00043>`__ *Zoufal, Lucchi, Woerner* [2019]."
msgstr "Для получения дополнительной информации обратитесь к `Quantum Generative Adversarial Networks for Learning and Loading Random Distributions <https://arxiv.org/abs/1904.00043>`__ *Zoufal, Lucchi, Woerner* [2019]."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:21
msgid "For an example of how to use a trained qGAN in an application, the pricing of financial derivatives, please see the `Option Pricing with qGANs <https://github.com/Qiskit/qiskit-finance/tree/main/docs/tutorials/10_qgan_option_pricing.ipynb>`__ tutorial."
msgstr "Для примера того, как использовать обученные qGAN в приложении, например, в ценообразовании финансовых дериватов, ознакомьтесь с руководством `Ценообразование опционов с qGANs <https://github.com/Qiskit/qiskit-finance/tree/main/docs/tutorials/10_qgan_option_pricing.ipynb>`__."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:56
msgid "Load the Training Data"
msgstr "Загрузка данных для обучения"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:58
msgid "First, we need to load the :math:`k`-dimensional training data samples (here k=1)."
msgstr "Сначала нам нужно загрузить :math:`k`-мерные примеры данных для обучения (здесь k=1)."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:60
msgid "Next, the data resolution is set, i.e. the min/max data values and the number of qubits used to represent each data dimension."
msgstr "Далее задается разрешение данных, то есть минимальные/максимальные значения и количество кубитов, используемых для представления каждого измерения данных."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:95
msgid "Initialize the qGAN"
msgstr "Инициализация qGAN"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:97
msgid "The qGAN consists of a quantum generator :math:`G_{\\theta}`, i.e., an ansatz, and a classical discriminator :math:`D_{\\phi}`, a neural network."
msgstr "qGAN состоит из квантового генератора :math:`G_{\\theta}`, т.е. анзаца и классического дискриминатора :math:`D_{\\phi}` - нейросети."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:99
msgid "To implement the quantum generator, we choose a depth-\\ :math:`1` ansatz that implements :math:`R_Y` rotations and :math:`CZ` gates which takes a uniform distribution as an input state. Notably, for :math:`k>1` the generator’s parameters must be chosen carefully. For example, the circuit depth should be :math:`>1` because higher circuit depths enable the representation of more complex structures."
msgstr "Для реализации квантового генератора, мы выбираем анзац глубиной \\ :math:`1`, который реализует повороты :math:`R_Y` и :math:`CZ` вентилей, принимающих однородное распределение в качестве входного состояния. В частности, для :math:`k>1` параметры генератора должны быть тщательно отобраны. Например, глубина цепи должна быть :math:`>1`, потому что большая глубина цепи позволяет представить более сложные структуры."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:101
msgid "The classical discriminator used here is based on a neural network implementation using NumPy. There is also a discriminator based on PyTorch which is not installed by default when installing Qiskit - see `Optional Install <https://github.com/Qiskit/qiskit-machine-learning#optional-installs>`__ for more information."
msgstr "Используемый здесь классический дискриминатор основан на внедрении нейронных сетей с использованием NumPy. Существует также дискриминатор на основе PyTorch, не установленный по умолчанию при установке Qiskit - см. `Optional Install <https://github.com/Qiskit/qiskit-machine-learning#optional-installs>`__ для получения дополнительной информации."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:103
msgid "Here, both networks are updated with the ADAM optimization algorithm (ADAM is qGAN optimizer default)."
msgstr "Здесь обе сети обновляются алгоритмом оптимизации ADAM (по умолчанию для qGAN)."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:164
msgid "Run the qGAN Training"
msgstr "Запуск обучения qGAN"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:166
msgid "During the training the discriminator’s and the generator’s parameters are updated alternately w.r.t the following loss functions:"
msgstr "Во время обучения параметры дискриминатора и генератора обновляются поочередно в отношении функции потери:"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:168
msgid "L_G\\left(\\phi, \\theta\\right) = -\\frac{1}{m}\\sum\\limits_{l=1}^{m}\\left[\\log\\left(D_{\\phi}\\left(g^{l}\\right)\\right)\\right]\n\n"
msgstr "L_G\\left(\\phi, \\theta\\right) = -\\frac{1}{m}\\sum\\limits_{l=1}^{m}\\left[\\log\\left(D_{\\phi}\\left(g^{l}\\right)\\right)\\right]\n\n"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:170
msgid "and"
msgstr "и"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:172
msgid "L_D\\left(\\phi, \\theta\\right) =\n"
"  \\frac{1}{m}\\sum\\limits_{l=1}^{m}\\left[\\log D_{\\phi}\\left(x^{l}\\right) + \\log\\left(1-D_{\\phi}\\left(g^{l}\\right)\\right)\\right],"
msgstr "L_D\\left(\\phi, \\theta\\right) =\n"
"  \\frac{1}{m}\\sum\\limits_{l=1}^{m}\\left[\\log D_{\\phi}\\left(x^{l}\\right) + \\log\\left(1-D_{\\phi}\\left(g^{l}\\right)\\right)\\right],"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:177
msgid "with :math:`m` denoting the batch size and :math:`g^l` describing the data samples generated by the quantum generator."
msgstr "где :math:`m` означает размер пакета и :math:`g^l` описывает выборки данных, сгенерированные квантовым генератором."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:179
msgid "Please note that the training, for the purpose of this notebook, has been kept briefer by the selection of a known initial point (``init_params``). Without such prior knowledge be aware training may take some while."
msgstr "Пожалуйста, обратите внимание, что в данном блокноте обучение проводилось в кратчайшие сроки с выбором известной начальной точки (``init_params``). Без таких предварительных знаний обучение может занять большее время."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:245
msgid "Training Progress & Outcome"
msgstr "Прогресс обучения и результаты"

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:247
msgid "Now, we plot the evolution of the generator’s and the discriminator’s loss functions during the training, as well as the progress in the relative entropy between the trained and the target distribution."
msgstr "Теперь мы определили эволюцию генератора и функцию потери дискриминатора во время обучения, а также прогресс в относительной энтропии между подготовленным и целевым распределением."

#: ../../tutorials/04_qgans_for_loading_random_distributions.ipynb:249
msgid "Finally, we also compare the cumulative distribution function (CDF) of the trained distribution to the CDF of the target distribution."
msgstr "Наконец, мы также сравниваем совокупную функцию распределения (CDF) подготовленного распределения с CDF целевого распределения."

#: ../../tutorials/05_torch_connector.ipynb:9
msgid "Torch Connector and Hybrid QNNs"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:11
msgid "This tutorial introduces Qiskit’s ``TorchConnector`` class, and demonstrates how the ``TorchConnector`` allows for a natural integration of any ``NeuralNetwork`` from Qiskit Machine Learning into a PyTorch workflow. ``TorchConnector`` takes a Qiskit ``NeuralNetwork`` and makes it available as a PyTorch ``Module``. The resulting module can be seamlessly incorporated into PyTorch classical architectures and trained jointly without additional considerations, enabling the development and testing of novel **hybrid quantum-classical** machine learning architectures."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:15
msgid "Content:"
msgstr "Содержание:"

#: ../../tutorials/05_torch_connector.ipynb:17
msgid "`Part 1: Simple Classification & Regression <#Part-1:-Simple-Classification-&-Regression>`__"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:19
msgid "The first part of this tutorial shows how quantum neural networks can be trained using PyTorch’s automatic differentiation engine (``torch.autograd``, `link <https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html>`__) for simple classification and regression tasks."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:21
msgid "`Classification <#1.-Classification>`__"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:23
msgid "Classification with PyTorch and ``OpflowQNN``"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:24
msgid "Classification with PyTorch and ``CircuitQNN``"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:26
msgid "`Regression <#2.-Regression>`__"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:28
msgid "Regression with PyTorch and ``OpflowQNN``"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:30
msgid "`Part 2: MNIST Classification, Hybrid QNNs <#Part-2:-MNIST-Classification,-Hybrid-QNNs>`__"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:32
msgid "The second part of this tutorial illustrates how to embed a (Quantum) ``NeuralNetwork`` into a target PyTorch workflow (in this case, a typical CNN architecture) to classify MNIST data in a hybrid quantum-classical manner."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:74
msgid "Part 1: Simple Classification & Regression"
msgstr "Часть 1: Простая классификация и регрессия"

#: ../../tutorials/05_torch_connector.ipynb:86
msgid "1. Classification"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:88
msgid "First, we show how ``TorchConnector`` allows to train a Quantum ``NeuralNetwork`` to solve a classification tasks using PyTorch’s automatic differentiation engine. In order to illustrate this, we will perform **binary classification** on a randomly generated dataset."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:144
msgid "A. Classification with PyTorch and ``OpflowQNN``"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:146
msgid "Linking an ``OpflowQNN`` to PyTorch is relatively straightforward. Here we illustrate this using the ``TwoLayerQNN``, a sub-case of ``OpflowQNN`` introduced in previous tutorials."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:254
msgid "Optimizer"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:256
msgid "The choice of optimizer for training any machine learning model can be crucial in determining the success of our training’s outcome. When using ``TorchConnector``, we get access to all of the optimizer algorithms defined in the [``torch.optim``] package (`link <https://pytorch.org/docs/stable/optim.html>`__). Some of the most famous algorithms used in popular machine learning architectures include *Adam*, *SGD*, or *Adagrad*. However, for this tutorial we will be using the L-BFGS algorithm (``torch.optim.LBFGS``), one of the most well know second-order optimization algorithms for numerical optimization."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:260
msgid "Loss Function"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:262
msgid "As for the loss function, we can also take advantage of PyTorch’s pre-defined modules from ``torch.nn``, such as the `Cross-Entropy <https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html>`__ or `Mean Squared Error <https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html>`__ losses."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:264
msgid "**💡 Clarification :** In classical machine learning, the general rule of thumb is to apply a Cross-Entropy loss to classification tasks, and MSE loss to regression tasks. However, this recommendation is given under the assumption that the output of the classification network is a class probability value in the [0,1] range (usually this is achieved through a Softmax layer). Because the following example for ``TwoLayerQNN`` does not include such layer, and we don’t apply any mapping to the output (the following section shows an example of application of parity mapping with ``CircuitQNNs``), the QNN’s output can take any value in the range [-1,1]. In case you were wondering, this is the reason why this particular example uses MSELoss for classification despite it not being the norm (but we encourage you to experiment with different loss functions and see how they can impact training results)."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:442
#: ../../tutorials/05_torch_connector.ipynb:674
msgid "The red circles indicate wrongly classified data points."
msgstr "Красными кружками отмечены неверно классифицированные данные."

#: ../../tutorials/05_torch_connector.ipynb:454
msgid "B. Classification with PyTorch and ``CircuitQNN``"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:456
msgid "Linking an ``CircuitQNN`` to PyTorch requires a bit more attention than ``OpflowQNN``. Without the correct setup, backpropagation is not possible."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:458
msgid "In particular, we must make sure that we are returning a dense array of probabilities in the network’s forward pass (``sparse=False``). This parameter is set up to ``False`` by default, so we just have to make sure that it has not been changed."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:460
msgid "**⚠️ Attention:** If we define a custom interpret function ( in the example: ``parity``), we must remember to explicitly provide the desired output shape ( in the example: ``2``). For more info on the initial parameter setup for ``CircuitQNN``, please check out the `official qiskit documentation <https://qiskit.org/documentation/machine-learning/stubs/qiskit_machine_learning.neural_networks.CircuitQNN.html>`__."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:523
#: ../../tutorials/05_torch_connector.ipynb:815
msgid "For a reminder on optimizer and loss function choices, you can go back to `this section <#Optimizer>`__."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:686
msgid "2. Regression"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:688
msgid "We use a model based on the ``TwoLayerQNN`` to also illustrate how to perform a regression task. The chosen dataset in this case is randomly generated following a sine wave."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:730
msgid "A. Regression with PyTorch and ``OpflowQNN``"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:741
msgid "The network definition and training loop will be analogous to those of the classification task using ``TwoLayerQNN``. In this case, we define our own feature map and ansatz, instead of using the default values."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:963
msgid "Part 2: MNIST Classification, Hybrid QNNs"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:965
msgid "In this second part, we show how to leverage a hybrid quantum-classical neural network using ``TorchConnector``, to perform a more complex image classification task on the MNIST handwritten digits dataset."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:967
msgid "For a more detailed (pre-``TorchConnector``) explanation on hybrid quantum-classical neural networks, you can check out the corresponding section in the `Qiskit Textbook <https://qiskit.org/textbook/ch-machine-learning/machine-learning-qiskit-pytorch.html>`__."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:996
msgid "Step 1: Defining Data-loaders for train and test"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:1007
msgid "We take advantage of the ``torchvision`` `API <https://pytorch.org/vision/stable/datasets.html>`__ to directly load a subset of the `MNIST dataset <https://en.wikipedia.org/wiki/MNIST_database>`__ and define torch ``DataLoader``\\ s (`link <https://pytorch.org/docs/stable/data.html>`__) for train and test."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:1048
msgid "If we perform a quick visualization we can see that the train dataset consists of images of handwritten 0s and 1s."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:1120
msgid "Step 2: Defining the QNN and Hybrid Model"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:1131
msgid "This second step shows the power of the ``TorchConnector``. After defining our quantum neural network layer (in this case, a ``TwoLayerQNN``), we can embed it into a layer in our torch ``Module`` by initializing a torch connector as ``TorchConnector(qnn)``."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:1133
msgid "**⚠️ Attention:** In order to have an adequate gradient backpropagation in hybrid models, we MUST set the initial parameter ``input_gradients`` to TRUE during the qnn initialization."
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:1235
msgid "Step 3: Training"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:1337
msgid "Step 4: Evaluation"
msgstr ""

#: ../../tutorials/05_torch_connector.ipynb:1440
msgid "🎉🎉🎉🎉 **You are now able to experiment with your own hybrid datasets and architectures using Qiskit Machine Learning.** **Good Luck!**"
msgstr ""

#: ../../tutorials/index.rst:3
msgid "Machine Learning Tutorials"
msgstr "Учебные материалы по машинному обучению"

