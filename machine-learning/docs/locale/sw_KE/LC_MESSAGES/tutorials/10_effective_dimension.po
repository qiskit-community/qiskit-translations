msgid ""
msgstr ""
"Project-Id-Version: qiskit-docs\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2023-03-28 08:39+0000\n"
"PO-Revision-Date: 2023-03-28 10:54\n"
"Last-Translator: \n"
"Language: sw\n"
"Language-Team: Swahili\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.12.1\n"
"X-Crowdin-Project: qiskit-docs\n"
"X-Crowdin-Project-ID: 369271\n"
"X-Crowdin-Language: sw\n"
"X-Crowdin-File: /master/machine-learning/docs/locale/en/LC_MESSAGES/tutorials/10_effective_dimension.po\n"
"X-Crowdin-File-ID: 9730\n"

#: ../../tutorials/10_effective_dimension.ipynb:9
msgid "This page was generated from `docs/tutorials/10_effective_dimension.ipynb`__."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:9
msgid "Effective Dimension of Qiskit Neural Networks"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:11
msgid "In this tutorial, we will take advantage of the ``EffectiveDimension`` and ``LocalEffectiveDimension`` classes to evaluate the power of Quantum Neural Network models. These are metrics based on information geometry that connect to notions such as trainability, expressibility or ability to generalize."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:13
msgid "Before diving into the code example, we will briefly explain what is the difference between these two metrics, and why are they relevant to the study of Quantum Neural Networks. More information about global effective dimension can be found in `this paper <https://arxiv.org/pdf/2011.00027.pdf>`__, while the local effective dimension was introduced in a `later work <https://arxiv.org/abs/2112.04807>`__."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:25
msgid "1. Global vs. Local Effective Dimension"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:27
msgid "Both classical and quantum machine learning models share a common goal: being good at **generalizing**, i.e. learning insights from data and applying them on unseen data."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:29
msgid "Finding a good metric to assess this ability is a non-trivial matter. In `The Power of Quantum Neural Networks <https://arxiv.org/pdf/2011.00027.pdf>`__, the authors introduce the **global** effective dimension as a useful indicator of how well a particular model will be able to perform on new data. In `Effective Dimension of Machine Learning Models <https://arxiv.org/pdf/2112.04807.pdf>`__, the **local** effective dimension is proposed as a new capacity measure that bounds the generalization error of machine learning models."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:32
msgid "The key difference between global (``EffectiveDimension`` class) and **local** effective dimension (``LocalEffectiveDimension`` class) is actually not in the way they are computed, but in the nature of the parameter space that is analyzed. The global effective dimension incorporates the **full parameter space** of the model, and is calculated from a **large number of parameter (weight) sets**. On the other hand, the local effective dimension focuses on how well the **trained** model can generalize to new data, and how **expressive** it can be. Therefore, the local effective dimension is calculated from **a single** set of weight samples (training result). This difference is small in terms of practical implementation, but quite relevant at a conceptual level."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:45
msgid "2. The Effective Dimension Algorithm"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:47
msgid "Both the global and local effective dimension algorithms use the Fisher Information matrix to provide a measure of complexity. The details on how this matrix is calculated are provided in the `reference paper <https://arxiv.org/pdf/2011.00027.pdf>`__, but in general terms, this matrix captures how sensitive a neural network's output is to changes in the network's parameter space."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:49
msgid "In particular, this algorithm follows 4 main steps:"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:51
msgid "**Monte Carlo simulation:** the forward and backward passes (gradients) of the neural network are computed for each pair of input and weight samples."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:52
msgid "**Fisher Matrix Computation:** these outputs and gradients are used to compute the Fisher Information Matrix."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:53
msgid "**Fisher Matrix Normalization:** averaging over all input samples and dividing by the matrix trace"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:54
msgid "**Effective Dimension Calculation:** according to the formula from `Abbas et al. <https://arxiv.org/pdf/2011.00027.pdf>`__"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:66
msgid "3. Basic Example (SamplerQNN)"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:68
msgid "This example shows how to set up a QNN model problem and run the global effective dimension algorithm. Both Qiskit ``SamplerQNN`` (shown in this example) and ``EstimatorQNN`` (shown in a later example) can be used with the ``EffectiveDimension`` class."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:70
msgid "We start off from the required imports and a fixed seed for the random number generator for reproducibility purposes."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:108
msgid "3.1 Define QNN"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:110
msgid "The first step to create a ``SamplerQNN`` is to define a parametrized feature map and ansatz. In this toy example, we will use 3 qubits, and we will define the circuit used in the ``SamplerQNN`` class."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:150
msgid "The parametrized circuit can then be sent together with an optional interpret map (parity in this case) to the ``SamplerQNN`` constructor."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:195
msgid "3.2 Set up Effective Dimension calculation"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:197
msgid "In order to compute the effective dimension of our QNN using the ``EffectiveDimension`` class, we need a series of sets of input samples and weights, as well as the total number of data samples available in a dataset. The ``input_samples`` and ``weight_samples`` are set in the class constructor, while the number of data samples is given during the call to the effective dimension computation, to be able to test and compare how this measure changes with different dataset sizes."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:208
msgid "We can define the number of input samples and weight samples and the class will randomly sample a corresponding array from a normal (for ``input_samples``) or a uniform (for ``weight_samples``) distribution. Instead of passing a number of samples we can pass an array, sampled manually."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:235
msgid "If we want to test a specific set of input samples and weight samples, we can provide it directly to the ``EffectiveDimension`` class as shown in the following snippet:"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:260
msgid "The effective dimension algorithm also requires a dataset size. In this example, we will define an array of sizes to later see how this input affects the result."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:283
msgid "3.3 Compute Global Effective Dimension"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:285
msgid "Let's now calculate the effective dimension of our network for the previously defined set of input samples, weights, and a dataset size of 5000."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:306
msgid "The effective dimension values will range between 0 and ``d``, where ``d`` represents the dimension of the model, and it's practically obtained from the number of weights of the QNN. By dividing the result by ``d``, we can obtain the normalized effective dimension, which correlates directly with the capacity of the model."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:359
msgid "By calling the ``EffectiveDimension`` class with an array if input sizes ``n``, we can monitor how the effective dimension changes with the dataset size."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:444
msgid "4. Local Effective Dimension Example"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:446
msgid "As explained in the introduction, the local effective dimension algorithm only uses **one** set of weights, and it can be used to monitor how training affects the expressiveness of a neural network. The ``LocalEffectiveDimension`` class enforces this constraint to ensure that these calculations are conceptually separate, but the rest of the implementation is shared with ``EffectiveDimension``."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:448
msgid "This example shows how to leverage the ``LocalEffectiveDimension`` class to analyze the effect of training on QNN expressiveness."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:460
msgid "4.1 Define Dataset and QNN"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:462
msgid "We start by creating a 3D binary classification dataset using ``make_classification`` function from scikit-learn."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:495
msgid "The next step is to create a QNN, an instance of ``EstimatorQNN`` in our case in the same fashion we created an instance of ``SamplerQNN``."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:519
msgid "4.2 Train QNN"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:521
msgid "We can now proceed to train the QNN. The training step may take some time, be patient. You can pass a callback to the classifier to observe how the training process is going on. We fix ``initial_point`` for reproducibility purposes as usual."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:594
msgid "The classifier can now differentiate between classes with an accuracy of:"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:642
msgid "4.3 Compute Local Effective Dimension of trained QNN"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:644
msgid "Now that we have trained our network, let's evaluate the local effective dimension based on the trained weights. To do that we access the trained weights directly from the classifier."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:705
msgid "4.4 Compute Local Effective Dimension of untrained QNN"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:707
msgid "We can compare this result with the effective dimension of the untrained network, using the ``initial_point`` as our weight sample:"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:766
msgid "4.5 Plot and analyze results"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:768
msgid "If we plot the effective dimension values before and after training, we can see the following result:"
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:806
msgid "In general, we should expect the value of the local effective dimension to decrease after training. This can be understood by looking back into the main goal of machine learning, which is to pick a model that is expressive enough to fit your data, but not too expressive that it overfits and performs badly on new data samples."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:808
msgid "Certain optimizers help regularize the overfitting of a model by learning parameters, and this action of learning inherently reduces a model’s expressiveness, as measured by the local effective dimension. Following this logic, a randomly initialized parameter set will most likely produce a higher effective dimension that the final set of trained weights, because that model with that particular parameterization is “using more parameters” unnecessarily to fit the data. After training (with the implicit regularization), a trained model will not need to use so many parameters and thus have more “inactive parameters” and a lower effective dimension."
msgstr ""

#: ../../tutorials/10_effective_dimension.ipynb:811
msgid "We must keep in mind though that this is the general intuition, and there might be cases where a randomly selected set of weights happens to provide a lower effective dimension than the trained weights for a specific model."
msgstr ""

